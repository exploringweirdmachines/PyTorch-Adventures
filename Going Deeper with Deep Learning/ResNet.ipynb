{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eee73982-df85-43f0-ae43-caa4ea0f0223",
   "metadata": {},
   "source": [
    "# Why is it Called Deep Learning?\n",
    "\n",
    "Like we had talked about before, the word \"Deep\" comes from the fact that there are multiple layers of calculations that happen. The problem is, if our model becomes too deep, we are not able to optimize it anymore! \n",
    "\n",
    "Uptil now we have almost completely ignored the \"why\" or \"how\" backpropagation works so lets take a look now. Couple of things to keep in mind before we get into this. If this doesn't make that much sense (or even if it does beacuse its incredible) please watch the following couple of videos by **3Blue1Brown**!!\n",
    "\n",
    "- [**What is a Neural Network**](https://www.youtube.com/watch?v=aircAruvnKk)\n",
    "- [**Gradient Descent, How Neural Netoworks Learn**](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n",
    "- [**What is backpropagation really doing?**](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n",
    "\n",
    "If you watch these videos and intuitively understand whats going on, that should give you a pretty good foundation for everything else! The math you need to know to better understand this next part is really just one thing: **Chain Rule!!**\n",
    "\n",
    "### Chain Rule Recap\n",
    "\n",
    "Let $f(y) = y^2$ and $g(x) = 3x + 2$. Then $f(g(x)) = (3x+2)^2$. Now if we want to take the derivative of the function with respect to x, the problem is that x is composed in a function composed inside another function! This is exactly what the chain rule was made for :\n",
    "\n",
    "$$\\frac{df}{dx} = \\frac{df}{dg}\\frac{dg}{dx} = 2 * (3x+2)*\\frac{dg}{dx} = 2(3x+2)(3) = 6(3x+2)$$\n",
    "\n",
    "### BackPropagation on an Easy Neural Network\n",
    "Lets take this basic example to see hwo backpropagation works:\n",
    "\n",
    "![easynet](../src/visuals/easynet.png)\n",
    "\n",
    "This model has 3 inputs $[x_1, x_2, x_3]$, a single hidden layer with 2 nodes, and a single output. The loss we will use is our standard regression loss Mean squared error!\n",
    "\n",
    "\n",
    "Lets break this neural network up into all its components:\n",
    "\n",
    "$$Loss = L = \\frac{1}{N}(y_{true} - y_{pred})^2$$\n",
    "\n",
    "We can now write the expression for $y_{pred}$ based on the two previous nodes $[h_1, h_2]$ and their respective weights $[w_1^{[2]}, w_2^{[2]}]$\n",
    "\n",
    "$$y_{pred} = h_1*w_1^{[2]} + h_2*w_2^{[2]}$$\n",
    "\n",
    "We can now write a out the expression for $[h_1, h_2]$ given the inputs $[x_1, x_2, x_3]$ and the weights $[w_1^{[1]}, w_2^{[1]}, w_3^{[1]}, w_4^{[1]}, w_5^{[1]}, w_6^{[1]}]$\n",
    "\n",
    "$$h_1 = x_1*w_1^{[1]} + x_2*w_3^{[2]} + x_2*w_5^{[2]}$$\n",
    "\n",
    "$$h_2 = x_1*w_2^{[1]} + x_2*w_4^{[2]} + x_2*w_6^{[2]}$$\n",
    "\n",
    "As we can see, we have a bunch of compositions of fuctions here! Therefore we know **Chain Rule** will come into play somewhere. As we know, our goal is to minimize our Mean Squared Error Loss $L$. So lets start at the end of the network and work our way back! The variable is $y_{pred}$ and we want to take the derivative of this loss with respect to this variable\n",
    "\n",
    "$$\\frac{dL}{dy_{pred}} = -\\frac{2}{N}(y_{true} - y_{pred})$$\n",
    "\n",
    "The problem is, we can't really control $y_{pred}$ directly, and our only way to make a change is through our two weight parameters $[w_1^{[2]}, w_2^{[2]}]$. In that case, lets take the derivative of $L$ with respect to the first of these weights $w_1^{[2]}$\n",
    "\n",
    "$$\\frac{dL}{dw_1^{[2]}} = \\frac{dL}{dy_{pred}}\\frac{dy_{pred}}{dw_1^{[2]}}$$\n",
    "\n",
    "$$\\frac{dL}{dy_{pred}} \\text{ was calculated previously}$$\n",
    "\n",
    "$$\\frac{dy_{pred}}{dw_1^{[2]}} = \\frac{d(h_1*w_1^{[2]} + h_2*w_2^{[2]})}{dw_1^{[2]}} = h_1$$\n",
    "\n",
    "$$\\therefore \\frac{dL}{dw_1^{[2]}} = -\\frac{2h_1}{N}(y_{true} - y_{pred}) $$\n",
    "\n",
    "And similarly:\n",
    "$$\\frac{dL}{dw_2^{[2]}} = -\\frac{2h_2}{N}(y_{true} - y_{pred}) $$\n",
    "\n",
    "Great! We did backpropagation for just one layer... Now another one!!\n",
    "\n",
    "Lets take the derivative of $L$ with respect to $w_1^{[1]}$\n",
    "\n",
    "$$\\frac{dL}{dw_1^{[1]}} = \\frac{dL}{dy_{pred}}\\frac{dy_{pred}}{dh_1}\\frac{dh_1}{dw_1^{[1]}}$$\\\n",
    "\n",
    "$$\\frac{dL}{dy_{pred}} \\text{ was calculated previously}$$\n",
    "\n",
    "$$\\frac{dy_{pred}}{dh_1} = w_1^{[2]}$$\n",
    "\n",
    "$$\\frac{dh_1}{dw_1^{[1]}} = x_1$$\n",
    "\n",
    "$$\\therefore \\frac{dL}{dw_1^{[1]}} = -\\frac{2w_1^{[2]}x_1}{N}(y_{true} - y_{pred})$$\n",
    "\n",
    "### Lets Stop Here\n",
    "Ok we could keep going but I think we get the point... Now that we see what backpropagation looks like (and again please watch the videos to really know whats going on), lets see what the problem is.\n",
    "\n",
    "### Problems with BackPropagation\n",
    "Pretend we have 5 hidden layers and want to backpropagate to the start of the network. All the math afterwards has some forced notation but it should give you the concept of whats going on!\n",
    "\n",
    "$$\\frac{dL}{dw} = \\frac{dL}{dy_{pred}}\\frac{dy_{pred}}{dh^{[5]}}\\frac{dh^{[5]}}{dh^{[4]}}\\frac{dh^{[4]}}{dh^{[3]}}\\frac{dh^{[3]}}{dh^{[2]}}\\frac{dh^{[2]}}{dh^{[1]}}\\frac{dh^{[1]}}{dw}$$\n",
    "\n",
    "- **Vanishing Gradient:** If all our derivatives are small, then multiplying a lot of small numbers together will cause the overall gradient to be 0. If we are depending on the gradients to tell us the direction to shift all the parameters, and the gradient is 0, then the network gets stuck as that information isn't making it.\n",
    "\n",
    "- **Exploding Gradient:** If all our derivatives are larger numbers, then multiplying a bunch of larger numbers will give a very large number causing instable learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b3d5560f-cd11-4305-9296-c9a1a33f1883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 128, 128])\n",
      "torch.Size([2, 256, 128, 128])\n",
      "torch.Size([2, 256, 128, 128])\n",
      "torch.Size([2, 256, 128, 128])\n",
      "torch.Size([2, 512, 64, 64])\n",
      "torch.Size([2, 512, 64, 64])\n",
      "torch.Size([2, 512, 64, 64])\n",
      "torch.Size([2, 512, 64, 64])\n",
      "torch.Size([2, 512, 64, 64])\n",
      "torch.Size([2, 512, 64, 64])\n",
      "torch.Size([2, 512, 64, 64])\n",
      "torch.Size([2, 512, 64, 64])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "torch.Size([2, 2048, 16, 16])\n",
      "torch.Size([2, 2048, 16, 16])\n",
      "torch.Size([2, 1000])\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
