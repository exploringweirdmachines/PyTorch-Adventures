{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5e6e2bc-ebf5-41fa-9042-9b0ddf1174d6",
   "metadata": {},
   "source": [
    "# AutoEncoders: Compression is Your Friend!\n",
    "\n",
    "## How Does MP4 Compression Work?\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/priyammaz/HAL-DL-From-Scratch/blob/main/src/visuals/mp4_compression_ibm_visual.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "[Image Source](https://blog.video.ibm.com/streaming-video-tips/keyframes-interframe-video-compression/)\n",
    "\n",
    "Data Compression is a crucial aspect for Information management. Compression is everywhere! Everyone has seen .mp4 files before, but what does this compression format do? Instead of storing every frame as an image and storing all the images, MP4 will create reference frames every few frames throughout the video. Then, instead of storing all the pixel values of frames after the reference frame, it will only store what pixels changed from the reference frame. By doing this, if there is not a lot of movement in the frame, all that redundant data can be thrown out for the compression to occur. Although our filesize has now gone down considerably, when we go to play back the video, the computer needs to perform computations of taking every reference frame and then adding the changes to it to display the future frames. There is a ton more that goes into this (and you can explore some of this [here!](https://blog.video.ibm.com/streaming-video-tips/keyframes-interframe-video-compression/)) but for now just use this as an analogy for why we love AutoEnoders!\n",
    "\n",
    "## What is an AutoEncoder?\n",
    "\n",
    "AutoEncoders are kind of the same thing. In data, typically there is a lot of redundant information (correlated features). There are repeating patterns that don't need to be stored, but can be extrapolated from a compressed state. For example, if I gave you an image with random chunks of it missing, could you fill in the gaps? Of course! And we will take a look at these type of gap-filling architectures in the future, but for now lets explore the basics. More specifically, AutoEncoders fall into a category of Unsupervised Dimensionality Reduction Algorithms. \n",
    "\n",
    "### Curse of Dimensionality\n",
    "We typically describe data by the number of features (or dimensions) it has, but we fall into a problem coined by Richard Bellman, the Curse of Dimensionality. High dimensional data causes a lot of problems:\n",
    "- Lack of interpretability, we cannot visualize more than 3 dimensions at a time!\n",
    "- Sparsity, your data may have a super high dimensionality, but may only occupy small pockets of that vector space.\n",
    "- Complexity, as we increase the number of features, the complexity to train models increase\n",
    "- P > N problem, if you have more features than samples of data, your model may be underdetermined or unsolveable! Genomics has this problem typically, where there may be a few hundred samples and thousands of features.\n",
    "\n",
    "\n",
    "### PCA to the Rescue!\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/priyammaz/HAL-DL-From-Scratch/blob/main/src/visuals/pca_visual.png?raw=true\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "You have probably heared of Principal Component Analysis (PCA) before which attempts to solve this exact problem, and seems to work really well in practice. The high-level intuition of PCA is, lets pretend you have dimensions $x_1, x_2$ and some data that exists in this space. What we want to do is come up with a new set of axis called $PC_1, PC_2$ (principal component 1 and 2) where each axis represents a direction of highest variance, and all our axis remain orthogonal to each other (just like regular axis). \n",
    "\n",
    "In the visual above, we have some green and blue dots and we want to train a classifier, but the complexity of two dimensions is too high (just pretend). So we use PCA, which will draw our $PC_1$, the direction of highest spread (variance) in the dataset and $PC_2$, the second direction of highest variance that is also orthogonal to the first, and orthogonal axis is centered at the mean of our $x_1, x_2$ features. We then throw away $PC_2$ and project all the data onto $PC_1$, thus leaving us with only a single dimension (a line). In this line, we can still see a separation between our green and blue dots, therefore a classifier on this one feature will probably be pretty close to training one on the original $x_1, x_2$ features! If you then have a large number of correlated features in your dataset, you can compress them down into just a few features.\n",
    "\n",
    "This works really great in practice actually, but there are two limitations to PCA:\n",
    "\n",
    "- The princpal components are linear\n",
    "- Each principal component has to be orthogonal to each other\n",
    "\n",
    "Data can be complex, messy, and definitely non-linear, so how do Autoencoders address this?\n",
    "\n",
    "### AutoEncoders are Non-Linear PCA\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/priyammaz/HAL-DL-From-Scratch/blob/main/src/visuals/autoencoder_visual.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "Instead of performing a linear compression via PCA, we can force a Neural Network to compress data down (the encoder) and then use the compressed form to recontruct the original data (the decoder). The forces the model to create a low dimensional representation that is good enough to reconstruct the original data from. The typical form for the autoencoder is this hourglass shape, where the encoder and decoder are mirror images of each other with the most restriction of data flow in the middle (typically called the latent space of the model). It is important for the bottleneck to be sufficiently small, otherwise the model will just learn an identity function. To train this model, all we need to do is pass in data, and then compute a reconstruction loss on the other end to the input. \n",
    "\n",
    "The architecture you use for the compression and decompression is totally up to you! For images, starting with simple MNIST for now, we will explore Linear compression and then Convolution compression. What I want to look into here is how the AutoEncoder learns the latent space over time. For visualizing the embeddings, we will compress MNIST down to just 2 Number! That means we are going from a 28 x 28 = 784 pixels to just 2 numbers, a very high compression factor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66ed87d-f1f2-4b00-a65c-f104e18b912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "### Stuff to Visualize the Latent Space ###\n",
    "from celluloid import Camera\n",
    "from IPython.display import HTML\n",
    "\n",
    "### Seed Everything ###\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "### GENERATE ANIMATIONS ###\n",
    "generate_anim = False\n",
    "\n",
    "# I am resizing the 28x28 image to 32x32 just so its a power of 2! \n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((32,32)), \n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set = MNIST(\"../../data/mnist/\", train=True, transform=transform)\n",
    "test_set = MNIST(\"../../data/mnist/\", train=False, transform=transform)\n",
    "\n",
    "### SET DEVICE ###\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8029bc2-2d7d-4b34-9f8e-6ca407762b02",
   "metadata": {},
   "source": [
    "## Basic NonLinear AutoEncoder\n",
    "\n",
    "Everything about this Nonlinear AutoEncoder is arbritrary. All I care about is we have 28 x 28 inputs going in and 28 x 28 inputs coming out. All the layers are just reasonable guesses for hidden layers, and we included ReLU nonlinearity. Also, the images we are trying to predict have been scaled between 0 and 1, so we want the output of our model to be the same, so that is why we have a sigmoid placed at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff586793-62dd-4232-ae8d-1ac8a35fab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaAutoEncoder(nn.Module):\n",
    "    def __init__(self, bottleneck_size=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(32*32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, bottleneck_size)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32*32),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward_enc(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def forward_dec(self, x):\n",
    "        x = self.decoder(x)\n",
    "        x = x.reshape(-1,1,32,32)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        batch, channels, height, width = x.shape\n",
    "        \n",
    "        ### Flatten Image to Vector ###\n",
    "        x = x.flatten(1)\n",
    "\n",
    "        ### Pass Through Encoder ###\n",
    "        enc = self.forward_enc(x)\n",
    "\n",
    "        ### Pass Through Decoder ###\n",
    "        dec = self.forward_dec(enc)\n",
    "\n",
    "        return enc, dec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab6e24-104b-48a6-9cc7-46a76341129b",
   "metadata": {},
   "source": [
    "### Super Simple Training Script\n",
    "\n",
    "There isn't anything fancy going on in this training script. The onlt things we will pass in is the model we want to train, the datasets, batch size we want to use, as well as the number of training iterations and how often we want to evaluate the model with the testing data. The only extra thing added in is when we evaluate the model, we will store all the encoded images so we can plot the encodings over the duration of the training later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ffb27-0061-4f87-831c-96c07a2cabf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          train_set,\n",
    "          test_set,\n",
    "          batch_size, \n",
    "          training_iterations, \n",
    "          evaluation_iterations,\n",
    "          verbose=False):\n",
    "\n",
    "    print(\"Training Model!\")\n",
    "    print(model)\n",
    "    \n",
    "    ### Set the Device ###\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    ### Define the Model and Place on Device ###\n",
    "    model = model.to(device)\n",
    "\n",
    "    ### Set the Dataloaders ###\n",
    "    trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    testloader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "    ### Set the Optimizer ###\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "    ### Some List for logging ###\n",
    "    train_loss = []\n",
    "    evaluation_loss = []\n",
    "    train_losses = []\n",
    "    evaluation_losses = []\n",
    "\n",
    "    ### List to store Encoded Data when Evaluating ###\n",
    "    encoded_data_per_eval = []\n",
    "\n",
    "    ### Create a Progress Bar ###\n",
    "    pbar = tqdm(range(training_iterations))\n",
    "\n",
    "    train = True\n",
    "    step_counter = 0\n",
    "    while train:\n",
    "\n",
    "        for images, labels in trainloader:\n",
    "            \n",
    "            images = images.to(device)\n",
    "            encoded, reconstruction = model(images)\n",
    "\n",
    "            ### Simple MSE Loss ###\n",
    "            loss = torch.mean((images - reconstruction)**2) \n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if step_counter % evaluation_iterations == 0:\n",
    "                \n",
    "                model.eval()\n",
    "                encoded_evaluations = []\n",
    "                \n",
    "                for images, labels in testloader:\n",
    "\n",
    "                    images = images.to(device)\n",
    "                    encoded, reconstruction = model(images)\n",
    "                    loss = torch.mean((images - reconstruction)**2)\n",
    "                    evaluation_loss.append(loss.item())\n",
    "\n",
    "                    ### Store the Encoded Image with their Labels ###\n",
    "                    encoded, labels = encoded.cpu().flatten(1), labels.reshape(-1,1)\n",
    "                    encoded_evaluations.append(torch.cat((encoded, labels), axis=-1))\n",
    "            \n",
    "                ### Store All Testing Encoded Images ###\n",
    "                encoded_data_per_eval.append(torch.concatenate(encoded_evaluations).detach())\n",
    "\n",
    "                train_loss = np.mean(train_loss)\n",
    "                evaluation_loss = np.mean(evaluation_loss)\n",
    "\n",
    "                train_losses.append(train_loss)\n",
    "                evaluation_losses.append(evaluation_loss)\n",
    "\n",
    "                if verbose:\n",
    "                    print(\"Training Loss\", train_loss)\n",
    "                    print(\"Evaluation Loss\", evaluation_loss)\n",
    "\n",
    "                ### Reset For Next Evaluation ###\n",
    "                train_loss = []\n",
    "                evaluation_loss = []\n",
    "\n",
    "                model.train()\n",
    "            \n",
    "            step_counter += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "            if step_counter >= training_iterations:\n",
    "                print(\"Completed Training!\")\n",
    "                train = False\n",
    "                break\n",
    "\n",
    "    ### Store All Encoded Data as Numpy Arrays for each Eval Iteration ###\n",
    "    encoded_data_per_eval = [np.array(i) for i in encoded_data_per_eval]\n",
    "\n",
    "    print(\"Final Training Loss\", train_losses[-1])\n",
    "    print(\"Final Evaluation Loss\", evaluation_losses[-1])\n",
    "    \n",
    "    return model, train_losses, evaluation_losses, encoded_data_per_eval\n",
    "        \n",
    "vanilla_model = VanillaAutoEncoder(bottleneck_size=2)\n",
    "vanilla_model, train_losses, evaluation_losses, vanilla_encoded_data = train(vanilla_model,\n",
    "                                                                             train_set,\n",
    "                                                                             test_set,\n",
    "                                                                             batch_size=64,\n",
    "                                                                             training_iterations=25000,\n",
    "                                                                             evaluation_iterations=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d352e85-2c11-4df1-9009-0092624e2e39",
   "metadata": {},
   "source": [
    "### Plotting the Emedding Space Through Training ###\n",
    "\n",
    "We can now write a function that can take in the **encoded_data_per_eval** from our output of the training, and then create an animation of the embeddings space to see how the Neural Network Learns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc25f2a-713e-4a85-8d11-fc2af884e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_animation(encoded_data_per_eval, iterations_per_eval=100):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    camera = Camera(fig)\n",
    "    \n",
    "    for idx, encoding in enumerate(encoded_data_per_eval):\n",
    "        \n",
    "        encoding = pd.DataFrame(encoding, columns=[\"x\", \"y\", \"class\"])\n",
    "        encoding = encoding.sort_values(by=\"class\")\n",
    "        encoding[\"class\"] = encoding[\"class\"].astype(int).astype(str)\n",
    "    \n",
    "        for grouper, group in encoding.groupby(\"class\"):\n",
    "            plt.scatter(x=group[\"x\"], y=group[\"y\"], label=grouper, alpha=0.8, s=5)\n",
    "    \n",
    "        ax.text(0.4, 1.01, f\"Step {idx*iterations_per_eval}\", transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_xlabel(\"x\")\n",
    "        ax.set_ylabel(\"y\")\n",
    "        \n",
    "        camera.snap()\n",
    "        \n",
    "    plt.close()\n",
    "    anim = camera.animate(blit=True)\n",
    "\n",
    "    return anim\n",
    "\n",
    "def build_embedding_plot(encoding, title):\n",
    "    encoding = pd.DataFrame(encoding, columns=[\"x\", \"y\", \"class\"])\n",
    "    encoding = encoding.sort_values(by=\"class\")\n",
    "    encoding[\"class\"] = encoding[\"class\"].astype(int).astype(str)\n",
    "\n",
    "    for grouper, group in encoding.groupby(\"class\"):\n",
    "        plt.scatter(x=group[\"x\"], y=group[\"y\"], label=grouper, alpha=0.8, s=5)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if generate_anim:\n",
    "    anim = build_embedding_visual(vanilla_encoded_data)\n",
    "    HTML(anim.to_jshtml())\n",
    "\n",
    "else:\n",
    "    build_embedding_plot(vanilla_encoded_data[-1], \"Vanilla Autoencoder Latents\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0c5ef5-8cb5-4550-81c2-98c9ed2b9931",
   "metadata": {},
   "source": [
    "### Just for Fun, a Linear Autoencoder!\n",
    "\n",
    "Although this isnt exactly PCA (because we have no orthogonality restrictions on the components), this should look somewhat similar to what PCA produces!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d98477a-f371-4430-93dc-a4b1aec3777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAutoEncoder(nn.Module):\n",
    "    def __init__(self, bottleneck_size=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(32*32, bottleneck_size),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_size, 32*32),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch, channels, height, width = x.shape\n",
    "        \n",
    "        ### Flatten Image to Vector ###\n",
    "        x = x.flatten(1)\n",
    "\n",
    "        ### Pass Through Encoder ###\n",
    "        enc = self.encoder(x)\n",
    "\n",
    "        ### Pass Through Decoder ###\n",
    "        dec = self.decoder(enc)\n",
    "\n",
    "        ### Put Decoded Image Back to Original Shape ###\n",
    "        dec = dec.reshape(batch, channels, height, width)\n",
    "\n",
    "        return enc, dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9d8d95-c227-4816-a939-146789541fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearAutoEncoder(bottleneck_size=2)\n",
    "model, train_losses, evaluation_losses, linear_encoded_data = train(linear_model,\n",
    "                                                                    train_set,\n",
    "                                                                    test_set,\n",
    "                                                                    batch_size=64,\n",
    "                                                                    training_iterations=25000,\n",
    "                                                                    evaluation_iterations=250)\n",
    "if generate_anim:\n",
    "    anim = build_embedding_visual(linear_encoded_data)\n",
    "    HTML(anim.to_jshtml())\n",
    "\n",
    "else:\n",
    "    build_embedding_plot(linear_encoded_data[-1], \"Linear Autoencoder\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732b5045-ae7f-4e2f-8339-05b07c6829e5",
   "metadata": {},
   "source": [
    "### Lets Compare to PCA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e6f9cb-b5ee-41d2-a902-f4802647b791",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(torch.cat([d[0].flatten(1) for d in test_set], dim=0))\n",
    "labels = np.array([d[1] for d in test_set]).reshape(-1,1)\n",
    "\n",
    "### PCA Encodings ###\n",
    "pca = PCA(n_components=2)\n",
    "pca_encoding = pca.fit_transform(data)\n",
    "pca_encoding = np.concatenate((pca_encoding, labels), axis=-1)\n",
    "pca_encoding = pd.DataFrame(pca_encoding, columns=[\"x\", \"y\", \"class\"])\n",
    "pca_encoding = pca_encoding.sort_values(by=\"class\")\n",
    "pca_encoding[\"class\"] = pca_encoding[\"class\"].astype(int).astype(str)\n",
    "\n",
    "\n",
    "### Grab Last Linear Encodings ###\n",
    "linauto_encoding = linear_encoded_data[-1]\n",
    "linauto_encoding = pd.DataFrame(linauto_encoding, columns=[\"x\", \"y\", \"class\"])\n",
    "linauto_encoding = linauto_encoding.sort_values(by=\"class\")\n",
    "linauto_encoding[\"class\"] = linauto_encoding[\"class\"].astype(int).astype(str)\n",
    "\n",
    "\n",
    "### Lets Plot it All Now! ###\n",
    "f, (ax1,ax2) = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "### Plot PCA Plot ###\n",
    "for grouper, group in pca_encoding.groupby(\"class\"):\n",
    "    ax1.scatter(x=group[\"x\"], y=group[\"y\"], label=grouper, alpha=0.8, s=5)\n",
    "ax1.legend()\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"y\")\n",
    "ax1.set_title(\"PCA Embeddings\")\n",
    "\n",
    "for grouper, group in linauto_encoding.groupby(\"class\"):\n",
    "    ax2.scatter(x=group[\"x\"], y=group[\"y\"], label=grouper, alpha=0.8, s=5)\n",
    "ax2.legend()\n",
    "ax2.set_xlabel(\"x\")\n",
    "ax2.set_ylabel(\"y\")\n",
    "ax2.set_title(\"Linear AutoEncoder Embeddings\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b739074-36a5-4cc2-8749-3033389ec5d6",
   "metadata": {},
   "source": [
    "These actually look pretty similar! Disregarding the scale, the Linear Autoencoder is a seems like a rotated version of the PCA embeddings. (The Linear AutoEncoder can also change every time you run it just due to randomness of training). If they look similar, then what do the weight vectors of each principal component look like? Are they orthogonal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6983525e-48cd-4339-ad93-5c772fd27008",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Principal Component Vectors ###\n",
    "pc_1, pc_2 = torch.tensor(pca.components_[0]), torch.tensor(pca.components_[1])\n",
    "print(\"Angle Between PCA Principal Components\", torch.rad2deg(torch.acos(torch.dot(pc_1, pc_2))).item())\n",
    "\n",
    "### Linear AutoEncoder Projection Vectors ###\n",
    "model_weights = model.encoder[0].weight\n",
    "\n",
    "weight_vector_1, weight_vector_2 = model_weights[0].cpu(), model_weights[1].cpu()\n",
    "weight_vector_1 = weight_vector_1 / torch.norm(weight_vector_1)\n",
    "weight_vector_2 = weight_vector_2 / torch.norm(weight_vector_2)\n",
    "\n",
    "print(\"Angle Between AutoEncoder Vectors\", torch.rad2deg(torch.acos(torch.dot(weight_vector_1, weight_vector_2))).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2d575f-fa72-48e7-acce-80e947e78787",
   "metadata": {},
   "source": [
    "It looks like the best projection vectors the model could create were almost Orthogonal! This shows that AutoEncoders can closely approximate PCA by using Gradient Descent rather than expensive eigenvalue decompositions on large datasets. \n",
    "\n",
    "## Convolutional AutoEncoder\n",
    "\n",
    "There are two ways you can do Convolutional Autoencoders. The first way is have some convolutional compression (decrease image size, increase channels), then flatten with a few linear layers down to the bottleneck size you want, and then have some decoder linear layers followed by some Transpose Convolutions to upsample back to the image shape. The second is to forgo the linear layers and only have convolutions compression and transpose convolutional decompression. We will be doing the second method as this type of architecture will repeat again in the future! Remember, Convolutions work in the channel space and we never will compress down to linear (pixel) space. \n",
    "\n",
    "Again, there is absolutely no reason for any of the architecture decisions I made, i just picked stuff randomly so it worked. But, there is one thing to talk about and that is compression ratio!\n",
    "\n",
    "In our Vanilla Autoencoder we used 2 numbers to represent all 1024 pixels (there are 1024 pixels instead of 784 bceause we resized MNIST to 32x32). In this example, you will see our AutoEncoder will comress our image down to a 4 Channels x 4 Height x 4 Width, so effectively 64 numbers, giving the model a lot more room to express the images! So although our compression ratio isnt as high, we should see better reconstructions atleast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1446d618-5887-46dd-b497-a88f336de7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAutoEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, channels_bottleneck=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bottleneck = channels_bottleneck\n",
    "        self.in_channels = in_channels \n",
    "        \n",
    "        self.encoder_conv = nn.Sequential(\n",
    "\n",
    "            ### Convolutional Encoding ###\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=5, stride=2, padding=1, bias=False), \n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(), \n",
    "\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=1, bias=False), \n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(), \n",
    "\n",
    "            nn.Conv2d(in_channels=16, out_channels=self.bottleneck, kernel_size=3, stride=2, padding=1, bias=False), \n",
    "            nn.BatchNorm2d(self.bottleneck),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=self.bottleneck, out_channels=16, kernel_size=3, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(), \n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=8, out_channels=in_channels, kernel_size=2, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward_enc(self, x):\n",
    "        batch_size, num_channels, height, width = x.shape\n",
    "        conv_enc = self.encoder_conv(x)\n",
    "        return conv_enc\n",
    "\n",
    "    def forward_dec(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape((batch_size, self.bottleneck, 4, 4))\n",
    "        conv_dec = self.decoder_conv(x)\n",
    "        return conv_dec\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, num_channels, height, width = x.shape\n",
    "        enc = self.forward_enc(x)\n",
    "        dec = self.forward_dec(enc)\n",
    "        return enc, dec\n",
    "\n",
    "    \n",
    "conv_model = ConvolutionalAutoEncoder()\n",
    "conv_model, train_losses, evaluation_losses, conv_encoded_data_per_eval = train(conv_model,\n",
    "                                                                                train_set,\n",
    "                                                                                test_set,\n",
    "                                                                                batch_size=64,\n",
    "                                                                                training_iterations=25000,\n",
    "                                                                                evaluation_iterations=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0f69a7-7897-449f-8933-b4669f44588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_index = 0\n",
    "image, label = test_set[generated_index]\n",
    "\n",
    "_, vanilla_reconstructed = vanilla_model(image.unsqueeze(0).to(device))\n",
    "_, conv_reconstructed = conv_model(image.unsqueeze(0).to(device))\n",
    "\n",
    "vanilla_reconstructed = vanilla_reconstructed.to(\"cpu\").detach().numpy()\n",
    "conv_reconstructed = conv_reconstructed.to(\"cpu\").detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1,3)\n",
    "\n",
    "ax[0].imshow(image.squeeze(), cmap=\"gray\")\n",
    "ax[0].set_title(\"Original Image\")\n",
    "ax[0].set_xticklabels([])\n",
    "ax[0].set_yticklabels([])\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "ax[1].imshow(vanilla_reconstructed.squeeze(), cmap=\"gray\")\n",
    "ax[1].set_title(\"Vanilla Autoencoder\")\n",
    "ax[1].set_xticklabels([])\n",
    "ax[1].set_yticklabels([])\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "ax[2].imshow(conv_reconstructed.squeeze(), cmap=\"gray\")\n",
    "ax[2].set_title(\"Conv AutoEncoder\")\n",
    "ax[2].set_xticklabels([])\n",
    "ax[2].set_yticklabels([])\n",
    "ax[2].axis(\"off\")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cf015c-a2d4-4b8e-8f8f-ead7b430b2b1",
   "metadata": {},
   "source": [
    "## What about Generating New Images!!\n",
    "\n",
    "Until now we have only been looking at our Encoder, which takes an image and compresses it down. What about our Decoder? Lets take our Vanilla Autoencoder for example. If we gave it our own two numbers, will it generate any images? Of course! Lets try this exercise, we have our encoded testing data, lets find the median 2 number encodings for each digit and pass them through our decoder to see what it makes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a41256-483b-4091-b18b-fc1b3f136788",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embeddings = vanilla_encoded_data[-1]\n",
    "\n",
    "avg_digit_embeddings = []\n",
    "\n",
    "for i in range(10):\n",
    "    avg_embeddings = np.median(final_embeddings[final_embeddings[:, 2] == i][:, :2], axis=0)\n",
    "    avg_digit_embeddings.append(avg_embeddings)\n",
    "\n",
    "avg_digit_embeddings = torch.tensor(np.array(avg_digit_embeddings))\n",
    "pred_images = vanilla_model.forward_dec(avg_digit_embeddings.to(\"cuda\"))\n",
    "\n",
    "fig, axes = plt.subplots(1,10, figsize=(15,5))\n",
    "for idx, img in enumerate(pred_images):\n",
    "    img = img.squeeze().detach().cpu().numpy()\n",
    "    axes[idx].imshow(img, cmap=\"gray\")\n",
    "    axes[idx].set_xticklabels([])\n",
    "    axes[idx].set_yticklabels([])\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444fd4f8-f7d6-4f18-9a79-3c88ffe73ee4",
   "metadata": {},
   "source": [
    "### Interpolating the Space\n",
    "\n",
    "So our generations are alright, not the best but maybe model tuning would improve it. But the bigger question is, why don't we use AutoEncoders more for generation? You give it some numbers and it can create what you want! The problem is, the generation space is not compact. Here is again the embedded space from the Vanilla Autoencoder:\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/priyammaz/HAL-DL-From-Scratch/blob/main/src/visuals/uncompact_vanilla_autoencoder_latents.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "*Note: might look a bit different from the actual Embeddings above, I just saved this during a one of the training runs*\n",
    "\n",
    "Notice how the class are totally broken with large amounts of whitespace in between. Many of them also look like they have been stretched out into strands of vector space from the center. Lets try to interpolate the vector space then. What this means is, we will create a grid of values over a range of X and Y values, and the plot what the image generated from a point taken from that space looks like. If we sample points from where the actual encodings are, we should have pretty good results, but if we sample from where there is whitespace between the encodings, the generations will be pretty bad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2622d6d6-d10d-465e-9c83-a7ae302b146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_space(model, x_range=(-3,3), y_range=(-3,3), num_steps=20):\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    x_space = np.linspace(x_range[0], x_range[1], num_steps)\n",
    "    y_space = np.linspace(y_range[0], y_range[1], num_steps)\n",
    "\n",
    "    points = []\n",
    "    for x in x_space:\n",
    "        for y in y_space:\n",
    "            points.append([x,y])\n",
    "\n",
    "    points = torch.tensor(points, dtype=torch.float32).to(device)\n",
    "\n",
    "    ### Pass Through Model Decoder and Reshape ###\n",
    "    dec = model.forward_dec(points).detach().cpu()\n",
    "    dec = dec.reshape((num_steps,num_steps, *dec.shape[1:]))\n",
    "\n",
    "    fig, ax = plt.subplots(num_steps,num_steps, figsize=(12,12))\n",
    "\n",
    "    for x in range(num_steps):\n",
    "        for y in range(num_steps):\n",
    "            img = np.array(dec[x,y].permute(1,2,0))\n",
    "            ax[x,y].imshow(img, cmap=\"gray\")\n",
    "            ax[x,y].set_xticklabels([])\n",
    "            ax[x,y].set_yticklabels([])\n",
    "            ax[x,y].axis(\"off\")\n",
    "\n",
    "    fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "interpolate_space(vanilla_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc093c-8106-45e0-962d-3aa729b6c541",
   "metadata": {},
   "source": [
    "### Moving to Variational Autoencoders!\n",
    "\n",
    "**This is Super Important:** Why can't we generate anything from our Convolutional AutoEncoder? \n",
    "\n",
    "This is because, our convolutional autoencoder compresses our data down to 64 numbers (as a 4 x 4 x 4 cube). I have no idea what the vector space for that 64 values looks like. We dont know where the numbers are, or where we can get good generations. We could probe the model and find them but this isnt practical as the model becomes more complex, and it will be hard to find consistent generations. But what if instead of mapping to some arbritrary latent space like we are doing, if we mapped to a specific one with a known distribution?\n",
    "\n",
    "This is exactly what Variational AutoEncoders do! Not only do they need to perform the reconstruction loss like we have already, but the latent distribution is forced to follow some known probability distribution (typically a Normal distribution). If we know the latent is a Standard Normal distribution, it is very easy to sample from, because we know exactly where the bulk of the data is! In the case of our Convolutional model, we could just pass in gaussian noise to our decoder (in the shape 4 x 4 x 4) and generate away!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
