{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ff1568-12af-4353-954c-33747e4f1fe0",
   "metadata": {},
   "source": [
    "![banner](https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/banner.png?raw=true)\n",
    "\n",
    "# Residual Vector Quantization\n",
    "\n",
    "Residual Vector Quantization attempts to improve on the traditional Vector Quantization method that we learned about [here](https://github.com/priyammaz/PyTorch-Adventures/blob/main/PyTorch%20for%20Generation/AutoEncoders/Intro%20to%20AutoEncoders/Vector_Quantized_Variational_AutoEncoders.ipynb)! This notebook is very quick and is just an extension of the previous VQVAE tutorial.\n",
    "\n",
    "## Review: Vector Quantization\n",
    "\n",
    "Vector Quantization is the process of taking some continuous signal (like audio, images, etc...) and mapping them to discrete codes. \n",
    "\n",
    "Lets look at KMeans again, as KMeans is a quantization method:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/kmeans.png?raw=true\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "[Image Source](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html)\n",
    "\n",
    "As we can see, in this trained KMeans model, all the points in each color coded location (the continuous point) is represented by the cluster centroid marked by the X. This has a a pro and a con:\n",
    "\n",
    "- Pro: We have heavily reduced our data. Instead of having to store a large number of continuous values, we just store an index of which cluster centroid best represents it\n",
    "- Con: We have lost our precision. Basically, all points in a region, is represented by the single cluster centroid. This means for every point we have some loss our our precision $\\epsilon = x - q$ where $x$ was the original data and $q$ was its quantized cluster center. \n",
    "\n",
    "## Learning the Residuals\n",
    "RVQs were originally a signal processing technique used for data compression (mainly audio) to solve the loss of precision that comes from quantizing. They do this by progressively building finer approximations by estimating residuals. \n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/rvq.jpeg?raw=true\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "[Image Source](https://www.assemblyai.com/blog/what-is-residual-vector-quantization#:~:text=The%20encoder%20converts%20each%20fixed,originating%20in%20digital%20signal%20processing.)\n",
    "\n",
    "In our VQVAE implementation, we had a single codebook in our model. The Encoder would compress data down to the compressed space, we would select the code from our codebook that was closest to it, and then that code was sent to the decoder! The loss of precision was whatever is the difference between the output of the Encoder and the selected code.\n",
    "\n",
    "In Residual Vector Quantization, we instead have multiple codebooks in sequence. As you can see above, we take our vector and quantize it like normal. But the key step is, instead of passing this quantized vector onto the decoder, we instead compute the residual, the difference between our quantized code and the output of the encoder. The residual is then passed to the next codebook which does the same thing. So we end up with something like this:\n",
    "\n",
    "- Input: $X$ -> Codebook 1 -> $Q_1$\n",
    "    - Residual 1 -> $R_1 = X - Q_1$\n",
    "- Input: $R_1$ -> Codebook 2 -> $Q_2$\n",
    "    - Residual 2 -> $R_2 = R_1 - Q_2$\n",
    "- Input: $R_2$ -> Codebook 3 -> $Q_3$\n",
    "    - Residual 1 -> $R_3 = R_2 - Q_3$\n",
    " \n",
    "In the end our final \"quantized\" vector will just then be the sum of our codes, as each successive code of learning the residual.\n",
    "- Final Quantized: $Q_1 + Q_2 + Q_3$\n",
    "\n",
    "By stacking together our codebooks and learning the residuals in each successive codebook, we end up with more precision in our final code representation than would have been possible with just a single code. \n",
    "\n",
    "### Lets Implement It!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5db8a6e6-ff11-4dc9-98a1-858cb49ce8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "### Prep Dataset ###\n",
    "tensor_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set = MNIST(\"../../../data/mnist/\", train=True, transform=tensor_transforms)\n",
    "test_set = MNIST(\"../../../data/mnist/\", train=False, transform=tensor_transforms)\n",
    "\n",
    "### Set Device ###\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55d90d5-62f6-4d3f-b5c0-636271b23ada",
   "metadata": {},
   "source": [
    "### Standard Vector Quantizer\n",
    "\n",
    "The Vector Quantization process hasn't changed, we are just doing it multiple times instead of once! So lets just grab our vector quantizer from before and use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45a4a4e2-9cc6-4dd6-9c06-3d7540d172f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.8444e-04, -1.5788e-03,  4.8431e-04,  ..., -3.1403e-04,\n",
       "         -1.9469e-03,  1.6878e-03],\n",
       "        [-1.8885e-03,  1.2530e-03,  5.7340e-04,  ...,  1.9445e-03,\n",
       "          2.5548e-04, -6.6695e-05],\n",
       "        [-1.7849e-03,  1.4674e-03, -1.6144e-03,  ..., -1.6780e-03,\n",
       "         -1.4555e-03,  1.0619e-03],\n",
       "        ...,\n",
       "        [ 1.9005e-03, -1.4409e-03,  1.2105e-03,  ...,  1.4559e-03,\n",
       "          4.6668e-04,  1.5669e-03],\n",
       "        [-1.9158e-03, -9.9820e-04, -7.9142e-04,  ...,  1.6708e-03,\n",
       "         -1.1253e-03, -1.1863e-03],\n",
       "        [-1.1107e-03, -1.0723e-03,  1.8683e-03,  ..., -1.8745e-03,\n",
       "          1.6517e-05, -1.8587e-03]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, codebook_size=1024, latent_dim=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(codebook_size, latent_dim)\n",
    "        self.embedding.weight.data.uniform_(-1/codebook_size, 1/codebook_size)\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.codebook_size = codebook_size\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        ### Distance btwn every Latent and Code: (L-C)**2 = (L**2 - 2LC + C**2 ) ###\n",
    "\n",
    "        ### L2: [B, L] -> [B, 1]\n",
    "        L2 = torch.sum(x**2, dim=1, keepdim=True)\n",
    "\n",
    "        ### C2: [C, L] -> [C]\n",
    "        C2 = torch.sum(self.embedding.weight**2, dim=1).unsqueeze(0)\n",
    "\n",
    "        ### CL: [B,L]@[L,C] -> [B, C]\n",
    "        CL = x@self.embedding.weight.t()\n",
    "\n",
    "        ### [B, 1] - 2 * [B, C] + [C] -> [B, C]\n",
    "        distances = L2 - 2*CL + C2\n",
    "        \n",
    "        ### Grab Closest Indexes, create matrix of corresponding vectors ###\n",
    "        ### Closest: [B, 1]\n",
    "        closest = torch.argmin(distances, dim=-1)\n",
    "\n",
    "        ### Create Empty Quantized Latents Embedding ###\n",
    "        # latents_idx: [B, C]\n",
    "        quantized_latents_idx = torch.zeros(batch_size, self.codebook_size, device=x.device)\n",
    "\n",
    "        ### Place a 1 at the Indexes for each sample for the codebook we want ###\n",
    "        batch_idx = torch.arange(batch_size)\n",
    "        quantized_latents_idx[batch_idx,closest] = 1\n",
    "\n",
    "        ### Matrix Multiplication to Grab Indexed Latents from Embeddings ###\n",
    "\n",
    "        # quantized_latents: [B, C] @ [C, L] -> [B, L]\n",
    "        quantized_latents = quantized_latents_idx @ self.embedding.weight\n",
    "\n",
    "        return quantized_latents \n",
    "\n",
    "vq = VectorQuantizer(codebook_size=512,latent_dim=8)\n",
    "rand = torch.randn(1024,8)\n",
    "vq(rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f22d0-e314-48db-9fb4-6efcfb163775",
   "metadata": {},
   "source": [
    "### Lets Implement a Linear RVQ Now!\n",
    "\n",
    "Nothing is really changing here from our VQVAE, but there are two key ideas:\n",
    "\n",
    "1) We have multiple VQ codebooks now\n",
    "2) When we compute the residual between our encoder output $z$ and our codes $q$, we will do $z - q$. The problem? WE CANT!! The entire purpose of the VQVAE loss (codebook and commitment) with our passthrough gradients was that getting from $z$ to $q$ was not a differentiable process. Therefore, we cant just subtract $q$ from $z$ to compute our residual as we can't backprop through that operation. So instead, we will do our residual computed as $z - sg(q)$, treating our quantized as a constant, because we are afterall sending the residual forward to be quantized again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5ee64ad-27f6-4ad8-bfd2-3b9ad7095f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearResidualVectorQuantizedVAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       "  (rvq): ModuleList(\n",
       "    (0-3): 4 x VectorQuantizer(\n",
       "      (embedding): Embedding(64, 2)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearResidualVectorQuantizedVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=2, codebook_size=64, num_codebooks=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(32*32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, latent_dim)\n",
    "        )\n",
    "        \n",
    "        #############################################################\n",
    "        ###  Stack of Codebooks for Residual Vector Quantization  ###\n",
    "        self.rvq = nn.ModuleList(\n",
    "            [\n",
    "                VectorQuantizer(codebook_size, latent_dim) for _ in range(num_codebooks)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        #############################################################\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32*32),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward_enc(self, x):\n",
    "\n",
    "        x = self.encoder(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "    def quantize(self, z, return_individual_codes=False):\n",
    "        \n",
    "        #############################################\n",
    "        ## Quantize the Latent Space Representation #\n",
    "\n",
    "        codebook_losses = 0\n",
    "        commitment_losses = 0\n",
    "        quantized_codes = []\n",
    "        final_quantized = torch.zeros_like(z)\n",
    "        \n",
    "        for quantizer in self.rvq:\n",
    "\n",
    "            ### Get Closest Codes ###\n",
    "            codes = quantizer(z)\n",
    "\n",
    "            ### Compute VQ Loss ###\n",
    "            codebook_loss = torch.mean((codes - z.detach())**2)\n",
    "            commitment_loss = torch.mean((codes.detach() - z)**2)\n",
    "\n",
    "            codebook_losses += codebook_loss\n",
    "            commitment_losses += commitment_loss\n",
    "\n",
    "            ### Straight Through Gradients ###\n",
    "            codes = z + (codes - z).detach()\n",
    "\n",
    "            ### Accumulate Codes for Final Quantized ###\n",
    "            final_quantized = final_quantized + codes\n",
    "\n",
    "            ### Store Quantized ###\n",
    "            quantized_codes.append(codes)\n",
    "            \n",
    "            ### Update Z to be the Residual Error (But dont compute gradients) ###\n",
    "            z = z - codes.detach()\n",
    "            \n",
    "        #############################################\n",
    "\n",
    "        if return_individual_codes:\n",
    "            return final_quantized, quantized_codes, codebook_losses, commitment_losses\n",
    "        else:\n",
    "            return final_quantized, codebook_losses, commitment_losses\n",
    "\n",
    "    def forward_dec(self, x):\n",
    "        codes, codebook_loss, commitment_loss = self.quantize(x)\n",
    "        decoded = self.decoder(codes)\n",
    "        \n",
    "        return codes, decoded, codebook_loss, commitment_loss\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch, channels, height, width = x.shape\n",
    "        \n",
    "        ### Flatten Image to Vector ###\n",
    "        x = x.flatten(1)\n",
    "\n",
    "        ### Pass Through Encoder ###\n",
    "        latents = self.forward_enc(x)\n",
    "        \n",
    "        ### Pass Sampled Data Through Decoder ###\n",
    "        quantized_latents, decoded, codebook_loss, commitment_loss = self.forward_dec(latents)\n",
    "\n",
    "        ### Put Decoded Image Back to Original Shape ###\n",
    "        decoded = decoded.reshape(batch, channels, height, width)\n",
    "\n",
    "        return latents, quantized_latents, decoded, codebook_loss, commitment_loss\n",
    "\n",
    "model = LinearResidualVectorQuantizedVAE()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f601040f-1024-446f-9cf7-3e2e9d913344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          train_set,\n",
    "          test_set,\n",
    "          batch_size, \n",
    "          training_iterations, \n",
    "          evaluation_iterations):\n",
    "        \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    testloader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "    train_loss = []\n",
    "    evaluation_loss = []\n",
    "    train_losses = []\n",
    "    evaluation_losses = []\n",
    "\n",
    "    pbar = tqdm(range(training_iterations))\n",
    "    \n",
    "    train = True\n",
    "    \n",
    "    step_counter = 0\n",
    "    while train:\n",
    "\n",
    "        for images, labels in trainloader:\n",
    "            \n",
    "            images = images.to(device)\n",
    "\n",
    "            encoded, quantized_encoded, decoded, codebook_loss, commitment_loss = model(images)\n",
    "            reconstruction_loss = torch.mean((images-decoded)**2)\n",
    "            loss = reconstruction_loss + codebook_loss +  0.25*commitment_loss\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if step_counter % evaluation_iterations == 0:\n",
    "                \n",
    "                model.eval()\n",
    "\n",
    "                for images, labels in testloader:\n",
    "\n",
    "                    images = images.to(device)\n",
    "\n",
    "                    encoded, quantized_encoded, decoded, codebook_loss, commitment_loss = model(images)\n",
    "                    reconstruction_loss = torch.mean((images-decoded)**2) \n",
    "                    loss = reconstruction_loss + codebook_loss +  0.25*commitment_loss\n",
    "                    \n",
    "                    evaluation_loss.append(loss.item())\n",
    "                    \n",
    "                train_loss = np.mean(train_loss)\n",
    "                evaluation_loss = np.mean(evaluation_loss)\n",
    "\n",
    "                train_losses.append(train_loss)\n",
    "                evaluation_losses.append(evaluation_loss)\n",
    "                \n",
    "                train_loss = []\n",
    "                evaluation_loss = []\n",
    "\n",
    "                model.train()\n",
    "\n",
    "            step_counter += 1\n",
    "            pbar.update(1)\n",
    "           \n",
    "            \n",
    "            if step_counter >= training_iterations:\n",
    "                print(\"Completed Training!\")\n",
    "                train = False\n",
    "                break\n",
    "\n",
    "    print(\"Final Training Loss\", train_losses[-1])\n",
    "    print(\"Final Evaluation Loss\", evaluation_losses[-1])\n",
    "    \n",
    "    return model, train_losses, evaluation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca982fbb-cb44-47eb-8bf5-06195cb647ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35f0fcfeaf84e0c8e634c909f1fa52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Training!\n",
      "Final Training Loss 0.032834265537559984\n",
      "Final Evaluation Loss 0.03267755837529708\n"
     ]
    }
   ],
   "source": [
    "linear_rvq = LinearResidualVectorQuantizedVAE()\n",
    "linear_ae, train_losses, evaluation_losses = train(linear_rvq,\n",
    "                                                   train_set=train_set,\n",
    "                                                   test_set=test_set,\n",
    "                                                   batch_size=64,\n",
    "                                                   training_iterations=25000,\n",
    "                                                   evaluation_iterations=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b283861-3009-403e-a416-2ee7837acfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAATSklEQVR4nO3dW2zXd/348de3pS2UchgwGIhOZkh0mxjd9GIGFKaiWQT1YuEU4yHx0mTGZBeLpxijMdmNiXfGxCGFiVsyp4vL4iCSOBM1S8iyRVxwTsExZhmUng/f/4Xy8ocF83ntb6GQxyPpTfvqm0+/PTz76drXWu12ux0AEBEdV/sCAJg7RAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJArMWdu2bYve3t4YHBy87Mzu3buju7s7/vGPf0RExNDQUHzzm9+MDRs2RG9vbyxZsiQ2btwYe/fujf+70eXRRx+NVqsVP/jBDy579lNPPRWtViu+973vXfT8e++9N1qtVtx///2XfL3Dhw9Hq9W67NOBAwcqDwNcUS27j5irHn744dixY0f86Ec/ik9/+tMzXj48PBwrV66MLVu2xM9+9rM4depU3H333fHCCy/Ejh074gMf+ECMjo7GI488Er/+9a9j165dsXfv3ujo6IixsbFYtWpVvOc974mnn376kv/+Zz/72di7d2+cPHkyVq5cGRER586di1WrVsVNN90UU1NT8Ze//CVardZFr3f48OHYvHlzfPGLX4z3vve9M87duHFj3Hzzzf+DRwhmQRvmqOHh4faiRYvaW7duveTL+/v72xHRPnDgQLvdbre3bt3a7ujoaD/22GMzZr/85S+3I6L93e9+N5/3+c9/vt3R0dE+ceLEjPmRkZH2kiVL2h/96Ecvev4Pf/jDdldXV/vpp59uR0T78OHDM1730KFD7YhoHzx4sPT2wlzgx0fMWQsWLIhPfepT8atf/SpeffXVGS/v7++PRYsWxbZt2+K3v/1tPPnkk/GZz3wmtm3bNmP229/+dqxfvz6+853vxMjISERE7NmzJ6anpy/545xf/OIXcfbs2di9e/dFz9+3b198+MMfjs2bN8c73vGO2Ldv3//orYW5QRSY03bv3h2Tk5Pxk5/85KLnDwwMxJNPPhmf/OQnY8GCBfH4449HRFzyx0wREfPmzYtdu3bFwMBA/OY3v4mIiE2bNsXatWujv79/xnx/f3/09vbGJz7xiXzeyZMn49ChQ7Fz586IiNi5c2f89Kc/jfHx8Uv+m4ODg/Haa6/NeGr7iS1zmCgwp23ZsiVWr1494wv3wYMHY2JiIr+Tf/755yMi4l3vetdlz7rwsguzHR0dsXPnzvjDH/4Qx44dy7lz587FE088Edu3b4++vr58/v79+6Onpye2b98eERE7duyIM2fOxBNPPHHJf+9zn/tc3HjjjTOeTp06VX0Y4IoRBea0zs7O2LFjRzzzzDPx0ksv5fP7+/tj1apVcffdd0dE5G8oLVq06LJnXXjZ//1tpj179uR5FzzyyCMxOjp6yR8d3XPPPXnO+vXr44477rjsj5C++tWvxlNPPTXjadmyZU3ffLjiRIE578IX5wtfuP/2t7/FkSNHYseOHdHZ2RkRl/6C/58uvOzCbxJFRGzYsCFuv/322L9/fz6vv78/VqxYEVu3bs3nvfDCC/Hss8/G+9///njxxRfz6YMf/GD8/Oc/j3Pnzs349975znfGhz70oRlP3d3db/ShgFknCsx5d9xxR7z97W/PL9z79++Pdrt90Xfyt956a0REHD169LLnXHjZLbfcctHz9+zZE8eOHYvf//738corr8ShQ4fi3nvvjXnz5uXMj3/844iIuO+++2L9+vX59OCDD+avvcL1QBS4JuzevTuee+65OHr0aPT398f69esv+huAj3/84xER8dBDD13y9aempvJHTps2bbroZTt37oxWqxX9/f3x8MMPx9TU1EXBabfb0d/fH5s3b46DBw/OeNqwYYPfQuL6cZV/JRYaOX78eDsi2tu3b29HRPvrX//6jJmPfOQj7Y6Ojvbjjz8+42X3339/OyLaDz744CXP37RpU3vNmjXtO++8s71u3bqLXnbkyJF2RLQfeuihS77ut771rYv+3sHfKXAtm/dfiwFzxLp16+Kuu+6Kxx57LCJixn8EjvjnXcKWLVti+/btsWvXrti4cWOMjY3Fo48+GocPH449e/bEfffdd8nz9+zZE1/4whfi5MmT8cADD1z0sn379kVnZ2fcc889l3zdbdu2xQMPPBAHDhyIL33pS/n8I0eOxOjo6Iz5DRs2xIYNGxq/7XBFXe0qQVPf//732xHRft/73nfZmcHBwfY3vvGN9m233daeP39+OyLaEdH+yle+8l/PHhgYaPf09LQjov3888/n88fHx9vLly9vb9y48b++/rp169rvfve72+32v+8ULvf0ta99rfkbDVeY3Udc106cOBF33XVXTE5OxjPPPBNvectbrvYlwZzmPzRzXXvTm94Uv/zlL2N0dDQ+9rGPxZkzZ672JcGc5k4BgOROAYAkCgAkUQAgiQIAqfEfr/3n/3IQgGtLk98rcqcAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYA072pfAJfWarVmdb67u7vxbE9Pz6ydXb3u0dHRxrOTk5OlsycmJkrz09PTpfmKjo7m3691dnaWzp6ammo82263S2dXHpPq2bNpLl3L1eZOAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAg2X30Hyq7eCr7aarz8+bV3jXV/UTLli1rPLtixYrS2YsWLWo8W9mTFDG7O4Eqe5Uiatde/VgZHBxsPDswMFA6+/Tp07NyHRERY2NjjWcrO5gi5tZ+orl0Lf9r7hQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQLru11zM5hqF6iqKrq6uxrO9vb2ls6urKN761rc2nr3llltKZ1dWaFQfw8oakomJidLZVZV1HtWVG2fOnGk8++c//7l09tGjRxvPTk5Ols6uvH8qKzEi6qslpqenZ2X2jVzLtcSdAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAuiZ3H1X2GfX09JTOruyoqe5VquwzquwPioi47bbbSvN33nln49m1a9eWzq7sMxodHS2dPTg42Hh2eHi4dPbU1FRpfunSpY1nV65cWTq7srdpyZIlpbMrb+fQ0FDp7IGBgcaz1f1B1T1MlT1m4+PjpbOvZ+4UAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQDSnNh9VNlRMtsqu48qO34iIvr6+hrPrlq1qnT27bffPmvz1T1M58+fbzx74sSJ0tlnzpxpPFvZwxNRe99H1HZZdXV1lc6uPOaV64iIOHv2bOPZ48ePl86u7Kaq7huanp4uzVd2PFX3MF3P5s5XYwCuOlEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIM2JNRdVlT93n5ycLJ1dWbkxf/780tmVNQrLly8vnf22t72tNH/jjTc2nq2sC4iIOH36dOPZP/3pT6WzX3755cazr732WunshQsXztr8mjVrSmffdNNNjWe7u7tLZ69du7bx7A033FA6+6WXXirNzxWtVqs0fz2vxXCnAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQ5sTuo+oekcruo4mJiVm7lq6urtLZY2NjjWf7+vpKZy9ZsqQ0X9kJVd0hdPz48cazf/zjH0tnV3brnD9/vnT24sWLS/OLFi1qPFvZN1TV29tbml+6dGnj2eo+qMrHVeXzIaK+g6syfz3vMqpypwBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUA0jW55qKishIjovZn+iMjI6WzR0dHG892dnaWzh4fHy/NDw4ONp79+9//Xjr7r3/966zMRkS8+uqrjWerj0nV0NBQ49nh4eHS2ZUVDfPnzy+dXVnPMW9e7UtE5TGfzbUVb2Sef3KnAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQ5sTuo9lU3atU2ZU0MTFROruy+6h69tjYWGm+p6en8ey5c+dKZ7/++uuNZ6vX3Wq1Gs9W90dVHpPqtVR3CHV1dTWere4+qqjuD6p8/lQ/N2dzRxr/5k4BgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKAKRrcs3FXPlz9+oKgMnJycaz1fUPs7nOo7JyISKit7e38ezy5ctLZ1dWOlTXPyxcuLA0v3r16sazK1asKJ1deQwXL15cOruynqO64mS2riNi7nzeX+/cKQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoApGty99FsquxXqewPipjd3UcTExOl+QULFjSeXbNmTens4eHhxrM9PT2lsyv7cubNq314V+ff/OY3N55dtmxZ6ezKHqbqdXd3dzeere73qnxOVD9/uDLcKQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJLuPrqDK7qOBgYHS2SdOnCjNL168uPHs/PnzS2evXr268WxfX1/p7Mounupuner88uXLG89WHu+I2mNe3X1U2R/V2dk5a2dX9oy9kWupvD+r11Kdv5a4UwAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAyZqLK2hiYqLx7Msvv1w6+9ixY6X5ymqEZcuWlc6urACormiozI+MjJTO7uiofY80Pj7eeHZsbKx0dnWlQ0XlWqrXXVlDUlVZoVGdv57XVlS5UwAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASHYfXUGVvTCvvPJK6ezf/e53pfmhoaHGs8uXLy+d3dfX13i2us+msp+ourdnenq6NL906dLGszfddFPp7MrHSnd3d+nss2fPNp49f/586ezKY1h931f3E1Wuxe6jf3OnAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSNRf/H6p/Gl9ZXVBdL3Ds2LHS/OnTpxvPVtdc3HDDDY1ne3t7S2dXHsOqycnJ0nxlzcWtt95avJrmqusiKitOqh+Hlc+J6lqR6jxvjDsFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYBk99EVVNkLU93DMzw8XJofGRlpPHvq1KnS2V1dXbMyGxGxYMGCxrPVXTkdHbXvkW6++ebGs6Ojo6WzK/uMOjs7S2ePjY01nq3u95rN3UdcGe4UAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAECy5uI6MZsrA6ampkrz4+PjjWerqyWGhoYaz1ZWRbyRa1m8eHHj2e7u7tLZFRMTE6X5ygqV6vve6oprnzsFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYBk9xFXVbvdLs1Xd/FUdHZ2luYru5Wqe5gq+6Mqu4wiIkZHR0vzFdX3J3OPOwUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkKy54Kq6ltci9PT0NJ6trtCorLmYmJgonV1Zi1FdiXEtvz/5J3cKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgDJ7iOuKXNpt87ChQsbz86bN3ufat3d3aX56h6mirn0/uGNcacAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJDsPoJ/qe7tmZycbDzb0VH7/mt6enrWzu7r65u1s1utVmmeucedAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABI1lzAv1TXXIyNjTWePXnyZOnstWvXNp6trqKoXEtllQfXB3cKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgDJ7iN4g86ePdt49rnnniudvWDBgsaz4+PjpbNffPHFxrOV/U4R9f1RzD3uFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAsuYC/mV6ero0//rrrzeeffbZZ0tnnz9/flZmIyKOHTvWeHZkZKR0tjUX1z53CgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAqdVuuKyk1WrN9rXAVVX9GO/p6Wk8u3LlytLZvb29jWeHhoZKZw8ODjaere5VmpqaajxrT9KV1+Qxd6cAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJI1F/AvHR2175Gmp6cbz3Z3d5fOrny+jY2Nlc6uvJ2Vt5G5z5oLAEpEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgCkeU0HG65IAuAa5k4BgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgPT/AKVLPkNGcpMIAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generated_index = 150\n",
    "image, label = test_set[generated_index]\n",
    "image = image.unsqueeze(0).to(device)\n",
    "\n",
    "_,_, rvqvae_reconstructed, *_ = linear_rvq(image)\n",
    "\n",
    "image = image.to(\"cpu\")\n",
    "rvqvae_reconstructed = rvqvae_reconstructed.to(\"cpu\").detach().numpy()\n",
    "\n",
    "plt.imshow(rvqvae_reconstructed.squeeze(), cmap=\"gray\")\n",
    "plt.title(\"VQVAE\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8616c2-e0ea-49a4-b5cb-0e5c06d51c35",
   "metadata": {},
   "source": [
    "### Thats It!\n",
    "\n",
    "So this was very quick, and just a minor extension of the VQVAE. On a dataset like MNIST we don't see any major gains, but this is a crucial technique used on things like Audio Codecs like EnCodec, we will come back to this later when we implement that!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
