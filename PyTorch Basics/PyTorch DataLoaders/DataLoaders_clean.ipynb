{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7961237e-b8f0-4ff5-884b-e30d3ae03e83",
   "metadata": {},
   "source": [
    "![banner](../src/visuals/banner.png)\n",
    "\n",
    "# PyTorch DataLoaders\n",
    "In our [Intro to PyTorch](https://github.com/priyammaz/HAL-DL-From-Scratch/blob/main/Intro%20to%20PyTorch/Intro%20to%20PyTorch.ipynb) we got a basic view of the general PyTorch mechanics. What we didn't discuss in much detail is how to actually pass data to the model efficiently. Lets quickly pull up what we had done before for the MNIST dataset.\n",
    "\n",
    "\n",
    "The MNIST dataset was offered by default by PyTorch as one of their testing datasets. Our goal for this notebook will be to learn how to build this **Dataset** class from scratch so we can use it on our own custom dataset!!\n",
    "```\n",
    "train = torchvision.datasets.MNIST('../data', train=True, download=True,\n",
    "                      transform=transforms.Compose([ ### CONVERT ARRAY TO TENSOR\n",
    "                          transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "test = torchvision.datasets.MNIST('../data', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "```\n",
    "\n",
    "These datasets above can then be passed to the **Dataloader** so we can grab random minibatches. We will also explore the Dataloader specifically and see what other functionality it has!\n",
    "\n",
    "```\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de9990db-dfaa-4de4-9a9f-acc2eb28e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39760e1f-92ff-40aa-bce0-052cf8bbec06",
   "metadata": {},
   "source": [
    "## Dogs Vs Cats Dataset\n",
    "\n",
    "The first dataloader we will build will be the Dogs vs Cats dataset. Incase you haven't done this already run the following to download all the dataset to the */data* folder\n",
    "\n",
    "```\n",
    "bash download_data.sh\n",
    "```\n",
    "\n",
    "### Important Ideas to Keep in Mind\n",
    "- **Memory Constraints:** When we build a Dataset class we have to think about our hardware constraints. If we have a smaller CSV file, we can use Pandas to read it in and then index the dataframe to grab samples. The issue with this approach is that, the entire Pandas Dataframe is **read to memory**. This means if you have massive tabular datasets, Pandas is not the way to go. You would have to use a system that only loads samples of your tabular data to memory only when accessed. We will cover one such system that has been incredible called [Deep Lake](https://github.com/activeloopai/deeplake) in a future lesson. \n",
    "\n",
    "In our Cats vs Dogs dataset, this is what our file directory looks like:\n",
    "\n",
    "```\n",
    ".\n",
    "└── data/\n",
    "    └── Petimages/\n",
    "        ├── Dogs/\n",
    "        │   ├── xxx.jpg\n",
    "        │   ├── yyy.jpg\n",
    "        │   └── ...\n",
    "        └── Cats/\n",
    "            ├── xxx.jpg\n",
    "            ├── yyy.jpg\n",
    "            └── ...\n",
    "```\n",
    "\n",
    "In our folder Petimages, we have two more folders called Dogs and Cats, and each folder contains images of Dogs and Cats respectively. So we have two options:\n",
    "- Load all our images into Numpy Arrays (and tensors later) up front along with their class label 0 or 1 **### PLEASE DONT DO THIS!!!**\n",
    "- Store only a list of strings that indicate the path to each Image and then load the image only when accessed\n",
    "\n",
    "\n",
    "### Components of a Dataset\n",
    "\n",
    "The dataset class will have three components:\n",
    "- **init**: Initialize the model class with everything you want to store as class variables\n",
    "- **len**: We have to tell the Dataset how many samples there are. When we go to grab a sample, it can grab all the samples from index 0 to index len\n",
    "- **getitem**: This is the block that does most of the heavy lifting. Basically, given some index between 0 and the length of data defined before, it will grab some sample.\n",
    "```\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "```\n",
    "\n",
    "### Lets be a bit more specific about whats going on!\n",
    "\n",
    "```\n",
    ".\n",
    "└── data/\n",
    "    └── Petimages/\n",
    "        ├── Dogs/\n",
    "        │   ├── 111.jpg\n",
    "        │   ├── 222.jpg\n",
    "        └── Cats/\n",
    "            ├── 333.jpg\n",
    "            ├── 444.jpg\n",
    "```\n",
    "\n",
    "Lets say in our directories we have only 4 images in total (2 Cat and 2 Dog Images). Our **len** would then be 4 as we only have 4 images in total! The **getitem** will then use the indexes [0,1,2,3] to access these samples, so our goal in the getitem is to give the Dataset a way to load the image based on the sample number. \n",
    "\n",
    "The easiest way to do this is to store a list of filepaths to all of the images, and then when we access the filepath, we can load that image in! So we would first have to build this list of paths and store it to some class variable we can access later! \n",
    "\n",
    "**Note**: This function should not be in the **getitem**. You want it to happen when the Dataset is being Initialized, otherwise we will build the list every time we grab a sample. The code in **getitem** runs **EVERY TIME WE GRAB A SAMPLE**, but **init** will only run once.\n",
    "\n",
    "```\n",
    "path_to_dogs = [data/Petimages/Dogs/111.jpg, data/Petimages/Dogs/222.jpg] # Make a list of the path to all the Dog files\n",
    "path_to_cats = [data/Petimages/Cats/333.jpg, data/Petimages/Cats/444.jpg] # Make a list of the path to all the Cat files\n",
    "\n",
    "training_files = path_to_dogs + path_to_cats = [data/Petimages/Dogs/111.jpg, data/Petimages/Dogs/222.jpg, \n",
    "                                                data/Petimages/Cats/333.jpg, data/Petimages/Cats/444.jpg]\n",
    "                                                \n",
    "```\n",
    "\n",
    "Now that we have a list of files, we have to set the **len**, which in this case would just be the length of training_files, or 4.\n",
    "\n",
    "Lastly, when we start doing a forloop through our Dataset, the **getitem** function is run with an input of **idx**. More specifically, as we loop from 0 to 3 (our total samples of 4 as indicated from before), the idx returned for us to access in **getitem** can be utilized. For example, the first loop will return the index 0, and we can then take that and index our training files.\n",
    "\n",
    "- In the first iteration at the index 0 we have the filepath *data/Petimages/Dogs/111.jpg*.\n",
    "- In the second iteration at the index 1 we have the filepath *data/Petimages/Dogs/222.jpg*.\n",
    "- In the third iteration at the index 2 we have the filepath *data/Petimages/Cats/333.jpg*.\n",
    "- In the fourth iteration at the index 3 we have the filepath *data/Petimages/Cats/444.jpg*.\n",
    "\n",
    "Once we do the fourth iteration, we have done the entire length as indicated in **len** and the loop will end. Therefore in **getitem**, if we can index the filepath to each sample in this way, we can then load the image from the file and return the image as an array of numbers. Also, we can easily find the label of the image (Is it a Cat or Dog?) because the filepath includes \"Dogs\" and \"Cats\" in the name. Therefore we can also return from **getitem** some interger values like 0, if Dog is in the path, or 1 if Cat is in the path.\n",
    "\n",
    "#### Arrays vs Tensors\n",
    "When we load an image we will use the PIL Image module that we imported above. All this module does is take a filepath and loads the image in the PIL format. We can then convert this to a numpy array, but numpy arrays dont work with PyTorch, so we need to convert to a tensor. From PyTorch Torchvision module, we have imported transform which has a ton of cool image transformations we can do (and will look at a bit later). The one we need right now is the **ToTensor()** function that can accept a PIL image (or Numpy Array) and convert to a Tensor that PyTorch models can work with!\n",
    "\n",
    "\n",
    "#### 8Bit Images \n",
    "Most images are stored in what is known as an 8bit format. Essentially each pixel in the image can take integer values in the range of [0,255]. Now the problem with this is, Deep Learning tends to prefer numbers scaled between [0,1], so we just need to scale our 8bit images down. One way is to just divide everything by 255, but the **ToTensor()** function will already handle this for us in the PIL -> Tensor transformation.\n",
    "\n",
    "### Lets Put all the ideas Together!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f67b0800-642a-4b8a-99a6-72823a26251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogsvCats(Dataset):\n",
    "    def __init__(self, path_to_root):\n",
    "        path_to_cat = os.path.join(path_to_root, 'Cat')\n",
    "        path_to_cat_imgs = [os.path.join(path_to_cat, file) for file in os.listdir(path_to_cat)]\n",
    "        \n",
    "        path_to_dog = os.path.join(path_to_root, 'Dog')\n",
    "        path_to_dog_imgs = [os.path.join(path_to_dog, file) for file in os.listdir(path_to_dog)]\n",
    "        \n",
    "        self.training_files = path_to_cat_imgs + path_to_dog_imgs\n",
    "        \n",
    "        self.transforms = transforms.ToTensor()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.training_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path_to_image = self.training_files[idx]\n",
    "        \n",
    "        if \"Cat\" in path_to_image:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "            \n",
    "        image = Image.open(path_to_image)\n",
    "        image = self.transforms(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "path_to_root = \"../data/PetImages/\"\n",
    "dogvcat = DogsvCats(path_to_root=path_to_root)\n",
    "\n",
    "counter = 0\n",
    "for data in dogvcat:\n",
    "    if counter == 5:\n",
    "        break\n",
    "    counter += 1\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f5827-d8bb-4b47-ad76-e7d1d141b00d",
   "metadata": {},
   "source": [
    "### We have a Dataset class built and now lets try loading it to the DataLoader!\n",
    "\n",
    "To reiterate, we have built a dataloader that can access a single image, but in Deep Learning, we want to sample minibatches, so we need to grab a **BATCH_SIZE** amount of images. We can do that pretty easily using the DataLoader!\n",
    "\n",
    "The Dataloader has some more functions we will look at later but the most basic things we need to include are:\n",
    "\n",
    "```\n",
    "DataLoader(dataset=DATASET, # We place here the dataset we have defined previously\n",
    "           batch_size=16,   # How many samples do you want to put together in each batch?\n",
    "           shuffle=True)    # Do you want to shuffle the data?\n",
    "```\n",
    "\n",
    "Lets then go ahead and instantiate the dataloder and try to do a for loop. Again we are expecting 16 images at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0ac84fe-60fc-4508-8ffe-b3feed1dc4ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 500, 495] at entry 0 and [3, 375, 500] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m dogvcatloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m=\u001b[39mdogvcat, \n\u001b[1;32m      2\u001b[0m                            batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, \n\u001b[1;32m      3\u001b[0m                            shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m dogvcatloader:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(labels)\n",
      "File \u001b[0;32m/mnt/sdc2/analysis/environments/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/mnt/sdc2/analysis/environments/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/mnt/sdc2/analysis/environments/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/sdc2/analysis/environments/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/sdc2/analysis/environments/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/sdc2/analysis/environments/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:141\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    139\u001b[0m         storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    140\u001b[0m         out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr_\u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring_\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndarray\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 500, 495] at entry 0 and [3, 375, 500] at entry 1"
     ]
    }
   ],
   "source": [
    "dogvcatloader = DataLoader(dataset=dogvcat, \n",
    "                           batch_size=16, \n",
    "                           shuffle=True)\n",
    "\n",
    "for images, labels in dogvcatloader:\n",
    "    print(images.shape)\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc08e40-b6ea-40d9-b855-c5f3efcb0bff",
   "metadata": {},
   "source": [
    "## Oh No! What Happened?\n",
    "\n",
    "Lets take a look at a key assumption of the dataloader before we keep going. Take a closer look at the error:\n",
    "\n",
    "```\n",
    "RuntimeError: stack expects each tensor to be equal size, but got [3, 375, 500] at entry 0 and [3, 500, 327] at entry 1\n",
    "```\n",
    "\n",
    "What this is trying to say is the first image had dimension [3, 371, 500] and the second image had dimension [3, 416, 500] and because the shapes are different it cannot stack them together. Therefore the only way we can stack these images together is to somehow resize them all into the same shape.\n",
    "\n",
    "\n",
    "## Torchvision Transforms\n",
    "We will be quickly taking a look at the different transformations available from [PyTorch Torchvision Transforms](https://pytorch.org/vision/stable/transforms.html). Now there are a ton of these and we can't talk about all of them but I want to cover some common ones we use. \n",
    "\n",
    "```\n",
    "ToTensor(): We already saw this one, but it converts a PIL Image or Numpy Array to a PyTorch Tensor.\n",
    "Resize(): This will resize an image to the wanted size and is what we need to handle the images in our dataset being different sizes\n",
    "Normalize(): Allows us to feed in the Means and Standard Deviations for each channel (RGB) and normalize our images.\n",
    "RandomHorizontalFlip(): Randomly flips an image horizontally with some probability (default 0.5)\n",
    "RandomVerticalFlip(): Randomly flips an image Vertically with some probability (default 0.5)\n",
    "\n",
    "Compose(): Allows us to stick together multiple image transformations in a single sequence!\n",
    "```\n",
    "\n",
    "**Note**: Why do random flipping? Well an image of a dog flipped around is still a picture of a dog? By including this, it will help further generalize the model and how it interprets different classes. We will also talk about some issues such as overfitting later and this is a great technique to help curtail that!\n",
    "\n",
    "\n",
    "## Lets put together a Composition of Transfomations and Add it to our Dataset Class!\n",
    "\n",
    "Compose expects a list of transformations we want to do in the sequence we want it in! Once it is done, lets try to For loop through it to see if we get any errors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b5e276e-93a1-451e-992b-87aa807cab35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([16, 3, 224, 224])\n",
      "tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "img_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "class DogsvCats(Dataset):\n",
    "    def __init__(self, path_to_root, transforms):\n",
    "        path_to_cat = os.path.join(path_to_root, 'Cat')\n",
    "        path_to_cat_imgs = [os.path.join(path_to_cat, file) for file in os.listdir(path_to_cat)]\n",
    "        \n",
    "        path_to_dog = os.path.join(path_to_root, 'Dog')\n",
    "        path_to_dog_imgs = [os.path.join(path_to_dog, file) for file in os.listdir(path_to_dog)]\n",
    "        \n",
    "        self.training_files = path_to_cat_imgs + path_to_dog_imgs\n",
    "        \n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.training_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path_to_image = self.training_files[idx]\n",
    "        \n",
    "        if \"Cat\" in path_to_image:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "            \n",
    "        image = Image.open(path_to_image)\n",
    "        image = self.transforms(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "\n",
    "path_to_root = \"../data/PetImages/\"\n",
    "dogvcat = DogsvCats(path_to_root=path_to_root,\n",
    "                   transforms=img_transforms)\n",
    "\n",
    "counter = 0\n",
    "for image, label in dogvcat:\n",
    "    if counter == 5:\n",
    "        break\n",
    "    print(image.shape)\n",
    "    counter += 1\n",
    "    \n",
    "    \n",
    "dogvcatloader = DataLoader(dataset=dogvcat, \n",
    "                           batch_size=16, \n",
    "                           shuffle=True)\n",
    "\n",
    "for images, labels in dogvcatloader:\n",
    "    print(images.shape)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf337b1-f088-4e14-9e9b-e1102e0a760d",
   "metadata": {},
   "source": [
    "## Voila! It Finally Works!!!\n",
    "We are now grabbing 16 samples of images, each image has 3 channels, and each channel is of size 224 x 224. We also are getting the 16 binary class labels of 0 or 1 depending on if its a Dog or Cat. This is the common Tensor format that most Image related Deep Learning tasks happen in:\n",
    "\n",
    "```\n",
    "[Batch Size x Num Channels x Image Height x Image Width]\n",
    "```\n",
    "\n",
    "We are mainly focusing on this notebook in how to use the DataLoader and not really train anything, but we will use this DataLoader in a future lesson!\n",
    "\n",
    "## The Next Problem: Deep Learning Consists of Highly Expressive Models\n",
    "Deep Learning models are extremly powerful because of their expressive nature and ability to model very complex, high dimensional relationships. This means that as we train on a model, how it is performing on the training data is deceptive. What I mean by this is, after training a model, you may get an astonishing 100% Accuracy! You think this model is incredible, but the second you start using it for real world prediction problems with examples your model has never seen, you see that it doesn't perform accurately at all.\n",
    "\n",
    "We breifly talked about this before in the Intro to PyTorch, the problem of **Overfitting** on your data. To have a fair comparison, we have to train our data on some of the labeled data we have, but keep a small part of it for validation. The model will never optimize on the Validation but just inference to see how well it performs on data it has never seen. We will get into that in a future lesson, but for now, how do we setup our DataLoader to be able to do this?\n",
    "\n",
    "\n",
    "### Random Split\n",
    "Luckily for us, PyTorch has a function in **torch.utils.data** called **random_split**. In this function we will essentially let PyTorch know how many samples we want in our training set and testing set, and it will randomly split the dataset for us! We saw previously that our datset has about 25000 samples, so what we want to do is give 90 percent of our samples for training and the remaining 10 percent for validation. We will then load each of these datasets into our dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c3ab5c1-c04a-4573-a071-60a573017162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Samples: 22437 Number of Test Samples: 2494\n",
      "torch.Size([16, 3, 224, 224])\n",
      "tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0])\n",
      "torch.Size([16, 3, 224, 224])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "train_samples = int(0.9 * len(dogvcat))\n",
    "test_samples = len(dogvcat) - train_samples\n",
    "print(\"Number of Training Samples:\", train_samples, \"Number of Test Samples:\", test_samples)\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dogvcat, lengths=[train_samples, test_samples])\n",
    "\n",
    "### Load Datasets into two DataLoaders ###\n",
    "trainloader = DataLoader(train_dataset, \n",
    "                         batch_size=16, \n",
    "                         shuffle=True)\n",
    "\n",
    "testloader = DataLoader(test_dataset, \n",
    "                        batch_size=16, \n",
    "                        shuffle=False)\n",
    "\n",
    "\n",
    "### Test Loaders ###\n",
    "for images, labels in trainloader:\n",
    "    print(images.shape)\n",
    "    print(labels)\n",
    "    break\n",
    "    \n",
    "for images, labels in testloader:\n",
    "    print(images.shape)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56deef9c-f336-453b-b7ab-cf6792f9ad89",
   "metadata": {},
   "source": [
    "## Quick Dataset Building\n",
    "\n",
    "If your image files are stored in the format as shown at the beginning (seperate folders for each class with images in the folder) we can then user the PyTorch **ImageFolder** available in the Datasets module in Torchvision. This will functionally do the same thing, but save you a few lines of code so it comes in handy! It will, by default, make each folder for each class a different class label. In our case if we print our the classes attribute of our ImageFolder dataset, it will return a list:\n",
    "```\n",
    "['Cat', 'Dog']\n",
    "```\n",
    "\n",
    "This tells us that Cat will have the label 0 and Dog will have the label 1 (as per their indexes).\n",
    "\n",
    "The main input to **ImageFolder** is the path to the root directory (the folder that has all the class folders in it) and then the transformations we want to apply (same as the img_transforms from before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "930d4f8f-992e-4c43-980c-88271f7b3d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1])\n",
      "torch.Size([16, 3, 224, 224])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "dogvcat = ImageFolder(root=\"../data/PetImages/\", transform=img_transforms)\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dogvcat, lengths=[train_samples, test_samples])\n",
    "\n",
    "\n",
    "### Load Datasets into two DataLoaders ###\n",
    "trainloader = DataLoader(train_dataset, \n",
    "                         batch_size=16, \n",
    "                         shuffle=True)\n",
    "\n",
    "testloader = DataLoader(test_dataset, \n",
    "                        batch_size=16, \n",
    "                        shuffle=False)\n",
    "\n",
    "\n",
    "### Test Loaders ###\n",
    "for images, labels in trainloader:\n",
    "    print(images.shape)\n",
    "    print(labels)\n",
    "    break\n",
    "    \n",
    "for images, labels in testloader:\n",
    "    print(images.shape)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a160014-cfb2-4d75-9119-f534cfbe9a2b",
   "metadata": {},
   "source": [
    "## DataLoaders for NLP \n",
    "We will now repeat what we had done previously for images and just repeat it for NLP. There are two difficulties we will face with NLP:\n",
    "- We are now working with words, not numbers, so how do we convert it?\n",
    "- Sentences have different lengths, but PyTorch cannot stack different length tensors as we saw previously with the image example. Sentences aren't also something we resize so what do we do?\n",
    "\n",
    "### Words to Numbers: Tokenizers\n",
    "There is a ton of methods that we will explore when we dig into NLP more, but for now lets just look at the most basic one!\n",
    "\n",
    "**Character Level Tokenizers**\n",
    "We will essentially take a sentence and just split it by characters. Lets pretend we only have lowercase alphabet letters and spaces in our dataset, in which case [a-z, \" \"] can be represented with the numbers [0-26] for our 26 letters and one space character.\n",
    "```\n",
    "Tokenize Sentence\n",
    "\"hi my name is priyam\" -> ['h', 'i', ' ', 'm','y',' ','n','a','m','e',' ','i','s',' ','p','r','i','y','a','m'] \n",
    "\n",
    "Convert to Numbers\n",
    "['h', 'i', ' ', 'm','y',' ','n','a','m','e',' ','i','s',' ','p','r','i','y','a','m'] -> [7, 8, 26, 12, 24, 26, 13, 0, 12, 4, 26, 8, 18, 26, 15, 17, 8, 24, 0, 12]\n",
    "```\n",
    "\n",
    "We will have to build this tokenizer by iterating through all of our data, finding all the unique characters available, and make a dictionary that allows us to swap between characters and their numerical values.\n",
    "\n",
    "### IMDB Dataset \n",
    "For this example we will be looking at the IMDB dataset. This dataset is a binary classification problem that expects us to take in some text and then predict if it is a positive or negative review.\n",
    "```\n",
    ".\n",
    "└── alcimdb/\n",
    "    ├── train/\n",
    "    │   ├── neg/\n",
    "    │   │   ├── txt1.txt\n",
    "    │   │   ├── txt2.txt\n",
    "    │   │   └── ...\n",
    "    │   └── pos/\n",
    "    │       ├── txt3.txt\n",
    "    │       ├── txt4.txt\n",
    "    │       └── ...\n",
    "    └── test/\n",
    "        ├── neg/\n",
    "        │   ├── txt5.txt\n",
    "        │   ├── txt6.txt\n",
    "        │   └── ...\n",
    "        └── pos/\n",
    "            ├── txt7.txt\n",
    "            ├── txt8.txt\n",
    "            └── ...\n",
    "```\n",
    "\n",
    "As you can see they already split the training and testing data up for us, and each train and test has two classes, positive and negative! Inside each of those folders we have a series of text files that we need to read in. The majority of the Dataset principles we have covered are identical, so we will just do it that way again. The main difference is, instead of making a single dataset and using **random_split**, they already did the split for us so we can just make two datasets with two different filepaths.\n",
    "\n",
    "### Build Tokenizer Dictionary\n",
    "\n",
    "We will build a tokenizer off of our training data, and include an Unkown token incase there are characters in our testing data that are not found in the training. We will also just include a threshold that a character has to exist 1500 times for it to be included in our list of available characters that we care about. This 1500 is arbritrary though, and you should pick a threshold that makes sense for your problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "af8ef5c6-fb11-4764-85c2-57c671a7eb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '!': 1, '\"': 2, '&': 3, \"'\": 4, '(': 5, ')': 6, '*': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '<': 24, '>': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'R': 43, 'S': 44, 'T': 45, 'U': 46, 'V': 47, 'W': 48, 'Y': 49, 'Z': 50, 'a': 51, 'b': 52, 'c': 53, 'd': 54, 'e': 55, 'f': 56, 'g': 57, 'h': 58, 'i': 59, 'j': 60, 'k': 61, 'l': 62, 'm': 63, 'n': 64, 'o': 65, 'p': 66, 'q': 67, 'r': 68, 's': 69, 't': 70, 'u': 71, 'v': 72, 'w': 73, 'x': 74, 'y': 75, 'z': 76, 'é': 77, '<UNK>': 78, '<PAD>': 79}\n"
     ]
    }
   ],
   "source": [
    "path_to_data = \"../data/aclImdb/train/\"\n",
    "\n",
    "path_to_pos_fld = os.path.join(path_to_data, \"pos\")\n",
    "path_to_neg_fld = os.path.join(path_to_data, \"neg\")\n",
    "\n",
    "path_to_pos_txt = [os.path.join(path_to_pos_fld, file) for file in os.listdir(path_to_pos_fld)]\n",
    "path_to_neg_txt = [os.path.join(path_to_neg_fld, file) for file in os.listdir(path_to_neg_fld)]\n",
    "\n",
    "training_files = path_to_pos_txt + path_to_neg_txt\n",
    "\n",
    "alltxt = \"\"\n",
    "for file in training_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        text = f.readlines()[0]\n",
    "        alltxt += text\n",
    "        \n",
    "unique_counts = Counter(alltxt)\n",
    "\n",
    "characters = sorted([key for (key, value) in unique_counts.items() if value > 1500])\n",
    "characters.append(\"<UNK>\")\n",
    "characters.append(\"<PAD>\")\n",
    "\n",
    "char2idx = {c:i for i,c in enumerate(characters)}\n",
    "idx2char = {i:c for i,c in enumerate(characters)}\n",
    "\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "094ab30c-d211-46c0-a9c6-8fc93645a521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([806])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, path_to_data, char2idx):\n",
    "        path_to_pos_fld = os.path.join(path_to_data, \"pos\")\n",
    "        path_to_neg_fld = os.path.join(path_to_data, \"neg\")\n",
    "        \n",
    "        path_to_pos_txt = [os.path.join(path_to_pos_fld, file) for file in os.listdir(path_to_pos_fld)]\n",
    "        path_to_neg_txt = [os.path.join(path_to_neg_fld, file) for file in os.listdir(path_to_neg_fld)]\n",
    "        \n",
    "        self.training_files = path_to_pos_txt + path_to_neg_txt\n",
    "        self.tokenizer = char2idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.training_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path_to_txt = self.training_files[idx]\n",
    "        \n",
    "        with open(path_to_txt, \"r\") as f:\n",
    "            txt = f.readlines()[0]\n",
    "        \n",
    "        tokenized = []\n",
    "        for char in txt:\n",
    "            if char in self.tokenizer.keys():\n",
    "                tokenized.append(self.tokenizer[char])\n",
    "            else:\n",
    "                tokenized.append(self.tokenizer[\"<UNK>\"])\n",
    "                \n",
    "        tokenized = torch.tensor(tokenized)\n",
    "        \n",
    "        if \"neg\" in path_to_txt:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "            \n",
    "        return tokenized, label\n",
    "        \n",
    "        \n",
    "imdbdataset = IMDBDataset(path_to_data, char2idx)\n",
    "                \n",
    "for token, label in dataset:  \n",
    "    print(token.shape)\n",
    "    print(label)\n",
    "    break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84da99ac-fee1-4946-9c84-1fd8e025d372",
   "metadata": {},
   "source": [
    "## Data Collator\n",
    "\n",
    "As we can see above, we are able to return tokenized forms for each sentence, but they are all different lengths! This makes sense as different sentences are different lengths so we will append  the **<PAD>** token to each of our samples to make sure a batch is the same length. We could have, in the Dataset Class, made it such that we set a default length of 3000 tokens, and sentences that are longer will be cut to that size. The problem with this is, if we have samples that are much shorter than 3000, then we will have unecessary amounts of padding. Instead, we should pad every batch, so the maximum amount of padding would only go to the longest sample in a specific batch. \n",
    "\n",
    "    \n",
    "To do this we will take a look at the **collate_fn** option in the DataLoader. By default, the **collate_fn** expects everything to be of the same size and will stack them together, but that wont work. We will instead write our own function that will first pad all the samples to the longest sample in a batch and then stack them!\n",
    "    \n",
    "To do this, we wil use the pad_sequence fucntion give in PyTorch found in the utils for their RNN function. Here is a sample from the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html) that explains how it works! As you can see we give three different tensors of different sizes, and it padds sequences along the first dimension. \n",
    "    \n",
    "Also, when it comes to PyTorch we can have our data tensors in two formats:\n",
    "    \n",
    "```\n",
    "Sequence First [sequence length x batch x ...]\n",
    "Batch First [batch x sequence length x ...]\n",
    "```\n",
    "I personally prefer doing batch first as it makes more sense to me so we will just have to let everything in PyTorch know we will do batch first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "de423cd7-55ce-477a-9013-38fc8f04d316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.],\n",
      "        [  1.,   1.,   1.,   1.,   1.,   1.,   1.,   1., 999., 999.],\n",
      "        [  1.,   1., 999., 999., 999., 999., 999., 999., 999., 999.]])\n",
      "torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(10)\n",
    "b = torch.ones(8)\n",
    "c = torch.ones(2)\n",
    "\n",
    "padded = nn.utils.rnn.pad_sequence([a,b,c], batch_first=True, padding_value=999)\n",
    "print(padded)\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a470f63-b894-418d-9b3b-bf9375be8973",
   "metadata": {},
   "source": [
    "Notice that we have padded our batches shorter than the longest batch with the token 999. We will use this ability in our collate function now!\n",
    "\n",
    "The input to the collate function will be the batch of samples grabbed from the Dataset as a list of tuples like this:\n",
    "\n",
    "```\n",
    "[(text1, label1), (text2, label2), ...]\n",
    "```\n",
    "\n",
    "We just have to iterate through these tuples, stack our labels together, and then pad/stack our text together! Once we have the Collator we can define the DataLoader as usual with this extra function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1f703df4-3a5f-4c4b-996a-6ec4e86a1294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(batch):\n",
    "    texts, labels = [], []\n",
    "    for text, label in batch:\n",
    "        labels.append(label)\n",
    "        texts.append(text)\n",
    "        \n",
    "    label = torch.tensor(label)\n",
    "    texts = nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=char2idx[\"<PAD>\"])\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4765dcc9-32cd-445e-beaa-d5c9d952b0da",
   "metadata": {},
   "source": [
    "### Run Without Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1165e568-823f-4a52-8eb3-473858a279d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [789] at entry 0 and [1856] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m imdbloader \u001b[38;5;241m=\u001b[39m DataLoader(imdbdataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m texts, labels \u001b[38;5;129;01min\u001b[39;00m imdbloader:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(texts\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/sdc2/analysis/environments/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/mnt/sdc2/analysis/environments/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/mnt/sdc2/analysis/environments/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/sdc2/analysis/environments/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/sdc2/analysis/environments/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/sdc2/analysis/environments/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:141\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    139\u001b[0m         storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    140\u001b[0m         out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr_\u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring_\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndarray\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [789] at entry 0 and [1856] at entry 1"
     ]
    }
   ],
   "source": [
    "imdbloader = DataLoader(imdbdataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for texts, labels in imdbloader:\n",
    "    print(texts.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f80ce6d-c530-441b-825a-a2d0d47f6398",
   "metadata": {},
   "source": [
    "### Run Dataloader With Collator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2c6c90c0-0132-49da-9061-8438ce8cde74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[35, 70,  4,  ..., 61, 69, 10],\n",
      "        [28, 59, 62,  ..., 79, 79, 79],\n",
      "        [45, 58, 59,  ..., 79, 79, 79],\n",
      "        ...,\n",
      "        [38, 51, 70,  ..., 79, 79, 79],\n",
      "        [45, 58, 59,  ..., 79, 79, 79],\n",
      "        [59,  0, 73,  ..., 79, 79, 79]])\n"
     ]
    }
   ],
   "source": [
    "imdbloader = DataLoader(imdbdataset, batch_size=16, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "for texts, labels in imdbloader:\n",
    "    print(texts)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcf1ee4-784a-47b6-baa0-aef392aa9df7",
   "metadata": {},
   "source": [
    "## It Works!! \n",
    "That is basically it! We built dataloaders for Images and Text and as you can see they are almost similar with just a few differences. We will be using versions of these DataLoaders for training later on, and there are a few more complexities that come with that, but this is essentially everything you need to build datasets for PyTorch\n",
    "\n",
    "## Increase Performance of DataLoaders\n",
    "There are a couple of things we can also do with our DataLoaders that can make them faster. If we have a lot of preprocessing on our data, this can definitely help a bit!\n",
    "\n",
    "- Num Workers: DataLoading happens on the CPU and prepared tensors are then passed to the GPU. We can increase the number of workers that do these tasks\n",
    "- Pin Memory: Pre-Loads Tensors to GPU as Training is happening "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
