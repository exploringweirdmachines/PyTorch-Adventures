{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc0ce33-c0d6-4342-9386-61ad50fd8ff4",
   "metadata": {},
   "source": [
    "# Transfer Learning and PreTrained Models\n",
    "\n",
    "Before we get into building a ton of models, I want to first talk about one of the coolest aspects of Deep Learning: **Transfer Learning**. As we have seen in the [Intro to PyTorch](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/Intro%20to%20PyTorch), a Deep Learning model is really just a structured bag of parameters. In the Linear Regression example, we had only two parameters, a weight $W_1$ and a Bias $W_0$. The only models we can create by multiplying and adding numbers together is a linear model. The Dense MNIST model we then made after was a deeper model with more parameters and connections, but we were still only multiplying and adding numbers together, therefore we can only represent a linear relationship. To fix this we then added in some ReLU (Rectified Linear Unit) activation functions that would allow us to introduce non-linearity to our model! All these weights that are randomly initialized when we define the model are then optimized and tuned so we have the least error between our predictions and true value. \n",
    "\n",
    "If we took our model and saved the parameters that define it, this would be known as a **pre-trained** model. Transfer learning is then the idea that, if we already have a model that can predict handwritten digits very well (0 through 9), and we have a new dataset of the lowercase alphabet (a through z), do we really need to train a new model from scratch? Or can we just grab our pre-trained model and its already optimized weights and finetune it to the new dataset?\n",
    "\n",
    "Intuitively you can think about it this way. Lets say we show a kid some pictures of dogs and cats (like the dataset we are working on). After we show them some images of these dogs and cats, they will be able to classify them pretty well all on their own! Now we want to give them a new task, classify between tigers and wolves. Obviously tigers are not the same as cats and wolves are not the same as dogs, but they do share a bunch of similarities right? So the kid would then use their previous knowledge (The **pre-trained** model) and exapand it to be able to do this new task.\n",
    "\n",
    "## AlexNet and Convolutions\n",
    "We will be skipping ahead a bit here. In the next lesson, PyTorch for Vision, we will be covering in detail the ideas for convolutions and why they are much better that Dense Linear layers for Images. For now, pretend the model we will use is some black box that takes in an image, and outputs some probabilities of belonging to different classes. The specific model we will look at is AlexNet, which was probably the first real evidence of the power of deep learning in 2012. It was trained on the ImageNet task (predict across 1000 classes of images) and beat all other methods by a significant margin. We will be loading a PyTorch defined version of this model, but don't worry, we will implement this entire model from scratch in the next lesson! I am just trying to offer some intuition for Deep Learning before we get into the weeds!\n",
    "\n",
    "\n",
    "## More Intution about Deep Learning \n",
    "\n",
    "As we have seen, the reason it is called \"Deep\" Learning is because the model physically has depth and many layers of computation. Because of this we actually get some interesting properties!\n",
    "\n",
    "![Image](https://anhvnn.files.wordpress.com/2018/02/1i75ghqla1cbkeyqz3j-yga.jpeg)\n",
    "\n",
    "[credit](https://anhvnn.wordpress.com/2018/02/01/deep-learning-computer-vision-and-convolutional-neural-networks/)\n",
    "\n",
    "This is a common image you will see when exploring Deep Learning. You can see that earlier features that are learned are focused on simple things to find in an image, like edges, lines, etc... Regardless of the Image problem at hand, we always have to detect these features, so the underlying weights in a pretrained model that encode this can easily be used. As we move forward through the model, the extracted features in the image become much more specific to the dataset, and in this case there is some type of face detection. It is really cool though that convolutions (Again a black box that does some type of image processing) can extract abstract features at such a high level!\n",
    "\n",
    "Now lets say we want to use this pretrained model and now detect Dogs instead. Well on its own, this model would do poorly as it is optimized to find human faces! But also the intial layers that do low level image feautures are probably fine to leave alone. Therefore, the typical strategy is to keep the earlier parts of the model static (don't allow gradient updates) and then finetune the later layers.\n",
    "\n",
    "There are some benefits for this:\n",
    "- We can pretrain a model on a giant dataset, and then finetune it to a similar problem that we have limited data for\n",
    "- We can control and avoid problems like overfitting better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1719367b-a3ad-4307-aea8-5a474abc9b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import AlexNet, AlexNet_Weights\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69827bd6-3504-4251-baac-f174c36104ef",
   "metadata": {},
   "source": [
    "### Recap From Before\n",
    "\n",
    "We will be skipping the details of the setup for the DataLoader and everything here. If you have any confusion, go back to my [PyTorch DataLoader](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20DataLoaders) tutorial where I go in depth about how this works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb734155-29fe-4b8d-904b-0688c70edaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build Cats vs Dogs Dataset ###\n",
    "PATH_TO_DATA = \"../data/PetImages/\"\n",
    "\n",
    "### DEFINE TRANSFORMATIONS ###\n",
    "normalizer = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]) ### IMAGENET MEAN/STD ###\n",
    "train_transforms = transforms.Compose([\n",
    "                                        transforms.Resize((224,224)),\n",
    "                                        transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        normalizer\n",
    "                                      ])\n",
    "\n",
    "\n",
    "dataset = ImageFolder(PATH_TO_DATA, transform=train_transforms)\n",
    "\n",
    "train_samples, test_samples = int(0.9 * len(dataset)), len(dataset) - int(0.9 * len(dataset))\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, lengths=[train_samples, test_samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69da814a-d037-45b7-b6d3-2418e5e7da8d",
   "metadata": {},
   "source": [
    "### Lets Now Load the Model!!\n",
    "We will now load the AlexNet model from PyTorch and then poke around a bit to see how these models are typically defined! Below you will see a ton of thigns you haven't seen before but its ok! We will go into detail again in the next tutorial. For now lets just take a rought look at whats going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c90a54d-e183-4f14-8f3f-b2c11cba86f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AlexNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d047f-67cc-41cf-8b26-f25cddd10631",
   "metadata": {},
   "source": [
    "All PyTorch models are saved like this, where every layer has a name and we can access them individually. For example, if we want to look at the 1st linear layer in the classifier we can do it this way:\n",
    "\n",
    "```\n",
    "model.classifier[1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ab509d8-2c11-49bf-828e-d232ca059865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=9216, out_features=4096, bias=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9ca553-fe2f-4c0a-a15a-e23a4f9906ba",
   "metadata": {},
   "source": [
    "### What is the Output of the Model?\n",
    "\n",
    "Notice the very last linear layer looks like this:\n",
    "\n",
    "```\n",
    "Linear(in_features=4096, out_features=1000, bias=True)\n",
    "```\n",
    "\n",
    "We can clearly see here that the model is outputing 1000 values because the ImageNet task requires us to predict across 1000 classes. As we start thinking about how we want to use this model, if we have a binary classification problem (Cats vs Dogs), then we will need to change this last linear layer so its output only 2 values.\n",
    "\n",
    "**Lets Update the Model!**\n",
    "We know that we can access the last layer of the model by doing *model.classifier[6]* and we want to change this to a linear layer. We saw in the [Intro to PyTorch](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/Intro%20to%20PyTorch) that we can define a linear layer as:\n",
    "\n",
    "```\n",
    "Linear(in_features, out_features)\n",
    "```\n",
    "\n",
    "but we have to make sure that the input features of the new layer matches the output features of the previous Linear layer. We can see in the 4th Linear layer that the output is 4096 so we will ensure to keep that as our input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "097fa271-9d84-4e67-9ef6-694ea6554907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AlexNet()\n",
    "model.classifier[6] = nn.Linear(4096, 2)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6d459-aab0-469f-9a25-f88285e41732",
   "metadata": {},
   "source": [
    "Now we can clearly see that our model is outputting 2 features rather than 1000! This then gives us everything we need, so lets pass a test tesor through it to make sure its all functional, and then we can do a bit more exploration. As we said before, the common shape for Image data is:\n",
    "\n",
    "```\n",
    "[Batch Size x Channels x Image Height x Image Width]\n",
    "```\n",
    "\n",
    "We will just make a dummy tensor that matches the CatsVsDogs dataset we made that will have the shape [16 x 3 224 x 224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80f6b2b8-1215-47b6-95d5-c1113a7b82ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_data = torch.rand(16,3,224,224)\n",
    "model_output = model(rand_data)\n",
    "model_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0a8758-5055-4542-a8b3-9e6e7b7f4269",
   "metadata": {},
   "source": [
    "We can see that we passed in 16 images and the output is 2 classes per image, so the model is fully functional!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c555a8-8f90-4299-9401-4fbe116b9de2",
   "metadata": {},
   "source": [
    "### Checking out the Model Parameters \n",
    "The attribute **.named_parameters()** that a model has allows us to iterate through all the names and parameters of the model! You will notice that the parameters are also stored as Tensors and these are the number that are updated through optimization! We can add up the number of parameters in the model and see that after we reduced the final output dimension to 2, we have roughly 57 Million parameters! That may sound like a lot but most Deep Learning models today have Billions of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74922661-f908-4a7c-9804-6a842a4ee1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight : torch.Size([64, 3, 11, 11]) Num Parameters: 23232\n",
      "features.0.bias : torch.Size([64]) Num Parameters: 64\n",
      "features.3.weight : torch.Size([192, 64, 5, 5]) Num Parameters: 307200\n",
      "features.3.bias : torch.Size([192]) Num Parameters: 192\n",
      "features.6.weight : torch.Size([384, 192, 3, 3]) Num Parameters: 663552\n",
      "features.6.bias : torch.Size([384]) Num Parameters: 384\n",
      "features.8.weight : torch.Size([256, 384, 3, 3]) Num Parameters: 884736\n",
      "features.8.bias : torch.Size([256]) Num Parameters: 256\n",
      "features.10.weight : torch.Size([256, 256, 3, 3]) Num Parameters: 589824\n",
      "features.10.bias : torch.Size([256]) Num Parameters: 256\n",
      "classifier.1.weight : torch.Size([4096, 9216]) Num Parameters: 37748736\n",
      "classifier.1.bias : torch.Size([4096]) Num Parameters: 4096\n",
      "classifier.4.weight : torch.Size([4096, 4096]) Num Parameters: 16777216\n",
      "classifier.4.bias : torch.Size([4096]) Num Parameters: 4096\n",
      "classifier.6.weight : torch.Size([2, 4096]) Num Parameters: 8192\n",
      "classifier.6.bias : torch.Size([2]) Num Parameters: 2\n",
      "------------------------\n",
      "Total Parameters in Model 57012034\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for name, params in model.named_parameters():\n",
    "    num_params = int(torch.prod(torch.tensor(params.shape)))\n",
    "    print(name,\":\", params.shape, \"Num Parameters:\", num_params)\n",
    "    total_parameters += num_params\n",
    "    \n",
    "print(\"------------------------\")\n",
    "print(\"Total Parameters in Model\", total_parameters)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ea60c3-3c32-4a9f-9071-1aca7b425d79",
   "metadata": {},
   "source": [
    "### Lets Train This Model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
