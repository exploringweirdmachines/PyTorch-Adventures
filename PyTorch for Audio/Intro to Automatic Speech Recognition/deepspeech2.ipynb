{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d0dd99b-8d0d-4ffd-90dc-6498d6b74f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priyam/anaconda3/envs/torch/lib/python3.12/site-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Wav2Vec2CTCTokenizer, get_cosine_schedule_with_warmup\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"facebook/wav2vec2-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc143b7-ad9b-434c-83f3-4c08b01370bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriSpeechDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    LibriSpeechDataset downloaded from OpenSLR: https://www.openslr.org/12\n",
    "\n",
    "    There are 5 splits downloaded, 3 which are for training and 3 for testing:\n",
    "\n",
    "        Training: [\"train-clean-100\", \"train-clean-360\", \"train-other-500\"]\n",
    "        Validation: [\"dev-clean\", \"test-clean\"]\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 path_to_data_root, \n",
    "                 include_splits=[\"train-clean-100\", \"train-clean-360\", \"train-other-500\"],\n",
    "                 sampling_rate=16000,\n",
    "                 num_audio_channels=1):\n",
    "        \n",
    "        if isinstance(include_splits, str):\n",
    "            include_splits = [include_splits]\n",
    "\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.num_audio_channels = num_audio_channels\n",
    "\n",
    "        ### GET PATH TO ALL AUDIO/TEXT FILES ###\n",
    "        self.librispeech_data = []\n",
    "        for split in include_splits:\n",
    "            path_to_split = os.path.join(path_to_data_root, split)\n",
    "            \n",
    "            for speaker in os.listdir(path_to_split):\n",
    "                path_to_speaker = os.path.join(path_to_split, speaker)\n",
    "\n",
    "                for section in os.listdir(path_to_speaker):\n",
    "                    path_to_section = os.path.join(path_to_speaker, section)\n",
    "\n",
    "                    ### Grab Files and Split FLAC Audios and Text Transcripts ###\n",
    "                    files = os.listdir(path_to_section)\n",
    "                    transcript_file = [path for path in files if \".txt\" in path][0]\n",
    "\n",
    "                    ### Load Transcripts ###\n",
    "                    with open(os.path.join(path_to_section, transcript_file), \"r\") as f:\n",
    "                        transcripts = f.readlines()\n",
    "\n",
    "                    ### Split Transcripts by Audio Filename and Transcript ###\n",
    "                    for line in transcripts:\n",
    "                        split_line = line.split()\n",
    "                        audio_root = split_line[0]\n",
    "                        audio_file = audio_root + \".flac\"\n",
    "                        full_path_to_audio_file = os.path.join(path_to_section, audio_file)\n",
    "                        transcript = \" \".join(split_line[1:]).strip()\n",
    "\n",
    "                        self.librispeech_data.append((full_path_to_audio_file, transcript))\n",
    "   \n",
    "        self.audio2mels =  T.MelSpectrogram(\n",
    "            sample_rate=sampling_rate,\n",
    "            n_mels=80\n",
    "        )\n",
    "\n",
    "        self.amp2db = T.AmplitudeToDB(\n",
    "            top_db=80.0\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.librispeech_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        ### Grab Path to Audio and Transcript ###\n",
    "        path_to_audio, transcript = self.librispeech_data[idx]\n",
    "\n",
    "        ### Load Audio ###\n",
    "        audio, orig_sr = torchaudio.load(path_to_audio, normalize=True)\n",
    "\n",
    "        if orig_sr != self.sampling_rate:\n",
    "            audio = torchaudio.functional.resample(audio, orig_freq=orig_sr, new_freq=self.sampling_rate)\n",
    "        \n",
    "        ### Create Mel Spectrogram ###\n",
    "        mel = self.audio2mels(audio)\n",
    "\n",
    "        ### Convert to Decibels ###\n",
    "        mel = self.amp2db(mel)\n",
    "\n",
    "        ### Normalize Spectrogram ###\n",
    "        mel = (mel - mel.mean()) / (mel.std() + 1e-6)\n",
    "\n",
    "        ### Tokenize Text ###\n",
    "        tokenized_transcript = torch.tensor(tokenizer.encode(transcript))\n",
    "\n",
    "        batch = {\"input_values\": mel[0].T, \n",
    "                 \"labels\": tokenized_transcript}\n",
    "        \n",
    "        return batch\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    \"\"\"\n",
    "    This collate function is basically the heart of our implementation! It includes everything we need for training\n",
    "    such as attention masks, sub_attention_masks, span_masks and our sampled negatives!\n",
    "    \"\"\"\n",
    "\n",
    "    ### Sort Batch from Longest to Shortest (for future packed padding) ###\n",
    "    batch = sorted(batch, key=lambda x: x[\"input_values\"].shape[0], reverse=True)\n",
    "    \n",
    "    ### Grab Audios from our Batch Dictionary ###\n",
    "    batch_mels = [sample[\"input_values\"] for sample in batch]\n",
    "    batch_transcripts = [sample[\"labels\"] for sample in batch]\n",
    "\n",
    "    ### Get Length of Audios ###\n",
    "    seq_lens = torch.tensor([b.shape[0] for b in batch_mels], dtype=torch.long)\n",
    "\n",
    "    ### Pad and Stack Spectrograms ###\n",
    "    spectrograms = torch.nn.utils.rnn.pad_sequence(batch_mels, batch_first=True, padding_value=0)\n",
    "\n",
    "    ### Convert to Shape Convolution Is Happy With (B x C x H x W) ###\n",
    "    spectrograms = spectrograms.unsqueeze(1).transpose(-1,-2)\n",
    "\n",
    "    ### Get Target Lengths ###\n",
    "    target_lengths = torch.tensor([len(t) for t in batch_transcripts], dtype=torch.long)\n",
    "\n",
    "    ### Pack Transcripts (CTC Loss Can Take Packed Targets) ###\n",
    "    packed_transcripts = torch.cat(batch_transcripts)\n",
    "\n",
    "    ### Create Batch ###\n",
    "    batch = {\"input_values\": spectrograms, \n",
    "             \"seq_lens\": seq_lens, \n",
    "             \"labels\": packed_transcripts, \n",
    "             \"target_lengths\": target_lengths}\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e1b3547-0649-4c1d-af53-8805c675c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "\n",
    "    \"\"\"\n",
    "    Our spectrograms are padded, so different spectrograms will have \n",
    "    a different length. We need to make sure we dont include any padding information\n",
    "    in our convolution, and update padding masks for the next convolution in the stack!\n",
    "\n",
    "    Args:\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_channels,\n",
    "                 out_channels, \n",
    "                 kernel_size,\n",
    "                 stride, \n",
    "                 padding=0,\n",
    "                 bias=True,\n",
    "                 **kwargs):\n",
    "        \n",
    "        super(MaskedConv2d, self).__init__(in_channels=in_channels, \n",
    "                                           out_channels=out_channels, \n",
    "                                           kernel_size=kernel_size, \n",
    "                                           stride=stride, \n",
    "                                           padding=padding, \n",
    "                                           bias=bias, \n",
    "                                           **kwargs)\n",
    "\n",
    "    def forward(self, x, seq_lens):\n",
    "\n",
    "        \"\"\"\n",
    "        Updates convolution forward to zero out padding regions after convolution \n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        \n",
    "        ### Compute Output Seq Lengths of Each Sample After Convolution ###\n",
    "        output_seq_lens = self._compute_output_seq_len(seq_lens)\n",
    "\n",
    "        ### Pass Data Through Convolution ###\n",
    "        conv_out = super().forward(x)\n",
    "\n",
    "        ### Zero Out Any Values In The Padding Region (After Convolution) So they Dont Contribute ###\n",
    "        mask = torch.zeros(batch_size, output_seq_lens.max(), device=x.device)\n",
    "        for i, length in enumerate(output_seq_lens):\n",
    "            mask[i, :length] = 1\n",
    "\n",
    "        ### Unsqueeze mask to match image shape ###\n",
    "        mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "        ### Apply Mask ###\n",
    "        conv_out = conv_out * mask\n",
    "\n",
    "        return conv_out, output_seq_lens\n",
    "\n",
    "    def _compute_output_seq_len(self, seq_lens):\n",
    "\n",
    "        \"\"\"\n",
    "        To perform masking AFTER the encoding 2D Convolutions, we need to \n",
    "        compute what the shape of the output tensor is after each successive convolutions\n",
    "        is applied.\n",
    "    \n",
    "        Convolution formula can be found in PyTorch Docs: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        return torch.floor((seq_lens + (2 * self.padding[1]) - (self.kernel_size[1] - 1) - 1) // self.stride[1]) + 1\n",
    "\n",
    "class ConvolutionFeatureExtractor(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_channels=1, \n",
    "                 out_channels=32):\n",
    "\n",
    "        super(ConvolutionFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels, \n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.conv1 = MaskedConv2d(in_channels, out_channels, kernel_size=(11, 41), stride=(2,2), padding=(5,20), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv2 = MaskedConv2d(out_channels, out_channels, kernel_size=(11, 21), stride=(2,1), padding=(5,10), bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        ### I just preset this, I know after I apply my two convolutions above with a kernel size of \n",
    "        ### 11, that my output feature vectors will go from 80 down to 20! I could also dynamically\n",
    "        ### compute this but its probably fine, I wont be changing the data after all!\n",
    "        self.output_feature_dim = 20\n",
    "\n",
    "        ### Compute Final Output Features ###\n",
    "        self.conv_output_features = self.output_feature_dim * self.out_channels\n",
    "        \n",
    "    def forward(self, x, seq_lens):\n",
    "\n",
    "        x, seq_lens = self.conv1(x, seq_lens)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.nn.functional.hardtanh(x)\n",
    "        \n",
    "        x, seq_lens = self.conv2(x, seq_lens)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.nn.functional.hardtanh(x)\n",
    " \n",
    "        x = x.permute(0,3,1,2).flatten(2)\n",
    "\n",
    "        return x, seq_lens \n",
    "\n",
    "class RNNLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 hidden_size = 512, \n",
    "                 dropout_p=0.1):\n",
    "\n",
    "        super(RNNLayer, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            batch_first=True, \n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(2 * hidden_size)\n",
    "\n",
    "    def forward(self, x, seq_lens):\n",
    "\n",
    "        batch, seq_len, embed_dim = x.shape \n",
    "        \n",
    "        ### Pack Sequence (For efficient computation that ignores padding) ###\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(x, seq_lens, batch_first=True)\n",
    "\n",
    "        ### Pass Packed Sequence through RNN ###\n",
    "        out, _ = self.rnn(packed_x)\n",
    "\n",
    "        ### Unpack (and repad) sequence ###\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(out, total_length=seq_len, batch_first=True)\n",
    "\n",
    "        ### Normalize ###\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "class DeepSpeech2(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 conv_in_channels=1, \n",
    "                 conv_out_channels=32, \n",
    "                 rnn_hidden_size=512,\n",
    "                 rnn_dropout_p=0.1):\n",
    "\n",
    "        super(DeepSpeech2, self).__init__()\n",
    "\n",
    "        self.feature_extractor = ConvolutionFeatureExtractor(\n",
    "            conv_in_channels, conv_out_channels\n",
    "        )\n",
    "\n",
    "        self.output_hidden_features = self.feature_extractor.conv_output_features\n",
    "\n",
    "        ### Stack Together RNN Layers ###\n",
    "        ### First Layer has 640 inputs, everything after has 2 * 512 inputs ###\n",
    "        self.rnns = nn.ModuleList(\n",
    "            [\n",
    "                RNNLayer(self.output_hidden_features if i==0 else 2 * rnn_hidden_size,\n",
    "                         hidden_size=rnn_hidden_size,\n",
    "                         dropout_p=rnn_dropout_p)\n",
    "                for i in range(6)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        ### Classification Head ###\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(2 * rnn_hidden_size, rnn_hidden_size), \n",
    "            nn.Hardtanh(), \n",
    "            nn.Linear(rnn_hidden_size, tokenizer.vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, seq_lens):\n",
    "\n",
    "        ### Extract Features ###\n",
    "        x, final_seq_lens = self.feature_extractor(x, seq_lens)\n",
    "\n",
    "        ### Pass To RNN Layers ###\n",
    "        for rnn in self.rnns:\n",
    "            x = rnn(x, final_seq_lens)\n",
    "\n",
    "        ### Classification Head ###\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x, final_seq_lens    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cafa9ebf-4a70-4974-9f14-86eff76a0237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Parameters: 27973376\n"
     ]
    }
   ],
   "source": [
    "### TRAINING ARGUMENTS ###\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 25\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_WORKERS = 32\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "### DATALOADERS ###\n",
    "DATASET_ROOT = \"/mnt/datadrive/data/LibriSpeech\"\n",
    "trainset = LibriSpeechDataset(path_to_data_root=DATASET_ROOT, include_splits=[\"train-clean-100\"])\n",
    "testset =  LibriSpeechDataset(path_to_data_root=DATASET_ROOT, include_splits=[\"test-clean\"])\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS)\n",
    "\n",
    "### DEFINE MODEL ###\n",
    "model = DeepSpeech2(conv_in_channels=1, \n",
    "                    conv_out_channels=32, \n",
    "                    rnn_hidden_size=512,\n",
    "                    rnn_dropout_p=0.1).to(DEVICE)\n",
    "\n",
    "params = sum([p.numel() for p in model.parameters()])\n",
    "print(\"Total Training Parameters:\", params)\n",
    "\n",
    "### OPTIMIZER/SCHEDULER ###\n",
    "optimizer = optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=250, \n",
    "                                            num_training_steps=EPOCHS*len(trainloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfcf39c-1981-4686-beca-9554d00be272",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e12efd842894e7dae74895b211bc847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414fd1d1f1504fbd8a4064a3c2026e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Saving Model---\n",
      "Training Loss: 1.1310985825616493\n",
      "Validation Loss: 0.73495315017591\n",
      "Starting Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998d2eac51bf4dfd80acc7623bc69e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### HOUSEKEEPING ###\n",
    "best_val_loss = np.inf \n",
    "train_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "### TRAINING LOOP ###\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(\"Starting Epoch:\", epoch)\n",
    "\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    model.train()\n",
    "    for batch in tqdm(trainloader):\n",
    "        \n",
    "        ### Pass Through Model and get input_lengths (post convolutions) and logits ###\n",
    "        logits, input_lengths = model(x=batch[\"input_values\"].to(DEVICE), \n",
    "                                      seq_lens=batch[\"seq_lens\"])\n",
    "\n",
    "        ### CTC Expects Log Probabilities ###\n",
    "        log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "        ### CTC Also Expects (T x B x C), we have (B x T x C) ###\n",
    "        log_probs = log_probs.transpose(0,1)\n",
    "\n",
    "        ### Compute CTC Loss ###\n",
    "        loss = nn.functional.ctc_loss(\n",
    "            log_probs=log_probs,\n",
    "            targets=batch[\"labels\"].to(DEVICE),\n",
    "            input_lengths=input_lengths, \n",
    "            target_lengths=batch[\"target_lengths\"], \n",
    "            blank=tokenizer.pad_token_id, \n",
    "            reduction=\"mean\", \n",
    "        )\n",
    "\n",
    "        ### Update Model ###\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        ### Store Loss ###\n",
    "        training_losses.append(loss.item())\n",
    "        \n",
    "    model.eval()\n",
    "    for batch in tqdm(testloader):\n",
    "        \n",
    "        ### Pass Through Model and get input_lengths (post convolutions) and logits ###\n",
    "        with torch.no_grad():\n",
    "            logits, input_lengths = model(x=batch[\"input_values\"].to(DEVICE), \n",
    "                                          seq_lens=batch[\"seq_lens\"])\n",
    "\n",
    "        ### CTC Expects Log Probabilities ###\n",
    "        log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "        ### CTC Also Expects (T x B x C), we have (B x T x C) ###\n",
    "        log_probs = log_probs.transpose(0,1)\n",
    "\n",
    "        ### Compute CTC Loss ###\n",
    "        loss = nn.functional.ctc_loss(\n",
    "            log_probs=log_probs,\n",
    "            targets=batch[\"labels\"].to(DEVICE),\n",
    "            input_lengths=input_lengths, \n",
    "            target_lengths=batch[\"target_lengths\"], \n",
    "            blank=tokenizer.pad_token_id, \n",
    "            reduction=\"mean\", \n",
    "        )\n",
    "\n",
    "        ### Store Loss ###\n",
    "        validation_losses.append(loss.item())\n",
    "        \n",
    "    training_loss_mean = np.mean(training_losses)\n",
    "    valid_loss_mean = np.mean(validation_losses)\n",
    "\n",
    "    train_loss.append(training_loss_mean)\n",
    "    validation_loss.append(valid_loss_mean)\n",
    "\n",
    "    ### Save Model If Val Loss Decreases ###\n",
    "    if valid_loss_mean < best_val_loss:\n",
    "        print(\"---Saving Model---\")\n",
    "        torch.save(model.state_dict(), \"best_weights.pt\")\n",
    "        best_val_loss = valid_loss_mean\n",
    "        \n",
    "    print(\"Training Loss:\", training_loss_mean)\n",
    "    print(\"Validation Loss:\", valid_loss_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82038c-689c-4bc3-bc81-f979efb02bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, train_loss, label=\"Train Loss\", marker='o')\n",
    "plt.plot(epochs, validation_loss, label=\"Validation Loss\", marker='s')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c4b33-a081-42e3-bf9e-b2f1c4f14b47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
