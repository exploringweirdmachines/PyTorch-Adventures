{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bb2598d-ec72-4e40-a653-57ac855f14dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import Wav2Vec2ForCTC\n",
    "from utils import Wav2Vec2Config\n",
    "from safetensors.torch import load_file\n",
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "import torchaudio\n",
    "import jiwer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b81bd2-c804-44ee-a2c8-669c7511d528",
   "metadata": {},
   "source": [
    "### Load Weights\n",
    "\n",
    "We first need to load our weights from our model we trained! The only thing that really matters is we need to let the config know if we are loading pretrained weights from our own model, or if we are loading the huggingface backbone. This is mainly because the pretrained backbone from Huggingface have a slightly different structure than ours (i.e. groupnorm vs layernorm and some other stuff) Once the skeleton of the model is loaded, we can then load in our own weights from our checkpoint and then load that model in! We will also be loading in the Wav2Vec2 Tokenizer from Huggingface that we used for our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2362df16-a528-424f-9a50-7bf7e8967751",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the backbone type (Huggingface Backbone is slightly different from ours ###\n",
    "huggingface_backbone = False\n",
    "path_to_backbone = \"work_dir/finetune_my_backbone/model.safetensors\"\n",
    "\n",
    "### Load Config for Model (random backbone is for our own implementation, otherwise we will load huggingface backbone) ###\n",
    "### We will then replace the weights of the model with our own weights ###\n",
    "config = Wav2Vec2Config(pretrained_backbone=\"pretrained_huggingface\" if huggingface_backbone else \"random\")\n",
    "\n",
    "### Provide Path to Model Weights ###\n",
    "model_weights = load_file(path_to_backbone)\n",
    "\n",
    "### Load Weights to Model ###\n",
    "model = Wav2Vec2ForCTC(config)\n",
    "model.load_state_dict(model_weights)\n",
    "model.eval()\n",
    "\n",
    "### Load Tokenizer with Huggingface Model Name ###\n",
    "hf_model_name = config.hf_model_name\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(hf_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb10f56-4e97-41c7-9ad5-051cd29aebca",
   "metadata": {},
   "source": [
    "### Inference and Audio File\n",
    "\n",
    "I provide a sample audio file that I just grabbed from the LibriSpeech dataset, and I also provide the corresponding transcript. You can try any audio you want here though! We just need to make sure the model is resampled to 16000Hz and is normalized before passing to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e55974cc-d5e7-4496-bc16-5401a0b7961a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Transcript\n",
      "CHAPTER SIXTEEN I MIGHT HAVE TOLD YOU OF THE BEGINNING OF THIS LAAISON IN A FEW LINES BUT I WANTED YOU TO SEE EVERY STEP BY WHICH WE CAME I TO AGREE TO WHATEVER MARGUERITE WISHED\n"
     ]
    }
   ],
   "source": [
    "path_to_audio = \"sample_audio/sample_audio.wav\"\n",
    "true_transcript = \"CHAPTER SIXTEEN I MIGHT HAVE TOLD YOU OF THE BEGINNING OF THIS LIAISON IN A FEW LINES BUT I WANTED YOU TO SEE EVERY STEP BY WHICH WE CAME I TO AGREE TO WHATEVER MARGUERITE WISHED\"\n",
    "\n",
    "def transcript_audio(path_to_audio, model):\n",
    "    ### Load Audio ###\n",
    "    audio, sr = torchaudio.load(path_to_audio)\n",
    "    \n",
    "    ### Resample Audio to 16000Hz ###\n",
    "    resample = torchaudio.transforms.Resample(sr, 16000)\n",
    "    audio = resample(audio)\n",
    "    \n",
    "    ### Normalize Audio ###\n",
    "    normed_audio = ((audio - audio.mean()) / torch.sqrt(audio.var() + 1e-7))\n",
    "\n",
    "    ### Add Batch Dimension ###\n",
    "    normed_audio = normed_audio.unsqueeze(0)\n",
    "\n",
    "    ### Inference Audio through Model ###\n",
    "    with torch.inference_mode():\n",
    "        loss, logits = model(normed_audio)\n",
    "\n",
    "    ### Grab Predicted Characters at each token ###\n",
    "    pred_ids = torch.argmax(logits, axis=-1).squeeze().tolist()\n",
    "\n",
    "    ### Decode Predicted Characters with Tokenizer ###\n",
    "    pred_transcription = tokenizer.decode(pred_ids)\n",
    "\n",
    "    return pred_transcription\n",
    "\n",
    "\n",
    "pred_transcript = transcript_audio(path_to_audio, model)\n",
    "\n",
    "print(\"Predicted Transcript\")\n",
    "print(pred_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b7d113-75e0-4546-98ad-cd24e80dd7a1",
   "metadata": {},
   "source": [
    "### Compute WER\n",
    "\n",
    "We can use ```jiwer``` to compute the Word Error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "535d52d5-c0d8-442d-81ea-49b542ba7f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate: 0.028\n"
     ]
    }
   ],
   "source": [
    "word_error_rate = jiwer.wer(reference=true_transcript, hypothesis=pred_transcript)\n",
    "print(\"Word Error Rate:\", round(word_error_rate, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
