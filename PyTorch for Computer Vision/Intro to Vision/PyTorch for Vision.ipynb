{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62de470c-aeff-449d-bf7a-8811d8c2ed61",
   "metadata": {},
   "source": [
    "![banner](../../src/visuals/banner.png)\n",
    "\n",
    "# Computer Vision\n",
    "\n",
    "Deep Learning has powered Computer Vision for more than a decade now and has enabled us to do tasks that were previously impossible. Before we dig into it, I want to quickly talk about what Computer Vision is and some of areas that exist in the field. \n",
    "\n",
    "### Biological Vision\n",
    "Humans, and most animals, have the ability to understand our environment through our vision. Biologically, our eyes have photoreceptors (rods and cones) that are tasked with taking in light and converting them to electrical signals. These signals are then passed to our brain and high level features are parsed. Lets highlight some things you can do with your eyes!\n",
    "\n",
    "- **Recognition**: We can look at something and tell you what it is or even desribe in detail\n",
    "- **Location**: We can understand where the location of an object is with respect to ourselves\n",
    "- **Depth Perception**: We can guess the relative distance to an object\n",
    "\n",
    "If we want to build a self-driving car we need to be able to replicate atleast this much with technology! So lets talk about how cameras work then and what ideas are similar. \n",
    "\n",
    "### Machine Vision\n",
    "Cameras typrically house a lens and a sensor. The purpose of the lens is to focus an area of light and project it on the sensor. The sensor contains light sensitive elements that can take this light and convert it to an electrical signals for storage. One of the typical processes is to numerically encode the pixel values we need to be able to represent it in a way computer understand, so we quantize the signal and give every pixel value an intensity from [0-255] for the typical 8bit image. \n",
    "\n",
    "### Limitations of a Dense Linear Neural Network\n",
    "When we trained the MNIST model in the [Intro to PyTorch](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/Intro%20to%20PyTorch) section, notice somethign specific about this model itself:\n",
    "\n",
    "```\n",
    "class FCNet(nn.Module): # Inherit in nn.Module Class\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64) # input flattened 28*28 into 64 node layer\n",
    "        self.fc2 = nn.Linear(64,64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Activation Function: ReLU\n",
    "        \"\"\"\n",
    "        x = x.view(-1, 28*28)    ### THE CULPRIT!!!\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "Remember that the MNIST images are small pictures of handwritted digits in the shape 28 pixels x 28 pixels. Before we even passed this data to the model, we did the follwing:\n",
    "\n",
    "```\n",
    "x = x.view(-1, 28*28) \n",
    "```\n",
    "\n",
    "This line will take an image and flatten it like this:\n",
    "\n",
    "```\n",
    "[1 Channel x 28 Pixels x 28 Pixles] -> [1 Channel x 784 Pixels]\n",
    "```\n",
    "\n",
    "But we had to do this? Our linear layer is expecting a single dimension vector of values for every sample, not a matrix? But if we do this, we inherently cause a different issue.\n",
    "\n",
    "\n",
    "![image](https://testerstories.com/files/ai_and_ml/ml-mnist-2d-to-1d.png)\n",
    "\n",
    "[credit](https://testerstories.com/2018/09/demystifying-machine-learning-part-5/)\n",
    "\n",
    "Notice that when we have an image as a 2d matrix, it visually makes sense to us and indicates that this image is 1. If we flatten it to the 1D vector, we essentially just a list of values with intensities, but it is not obviously clear what image it is representing. Therefore we have lost all the **Spatial Features** that define the relationship and our model has no opportunity to learn how a pixel is related to all of its neighbors.\n",
    "\n",
    "\n",
    "## Convolutions to the rescue!\n",
    "\n",
    "Convolutions attempt to solve this problem by creating something known as a filter (a matrix of weights) and iteratively sliding it across the image. At every iteration, it performs some local computation and uses that to update the weights of the filter matrix. This sounds like a lot so lets visualize it!\n",
    "\n",
    "## How Convolutions Slide \n",
    "![image](https://miro.medium.com/max/790/1*1okwhewf5KCtIPaFib4XaA.gif)\n",
    "\n",
    "[credit](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)\n",
    "\n",
    "\n",
    "In this image, the Blue matrix below is our original image and the green matrix above that it is mapping to is the output of the convolution calculation. In this case, the filter is the 3x3 matrix that is sliding along the blue image. The computation being done is not much different from a weighted average. Essentially our filter has some weights in each square (if it is a 3x3 matrix it has 9 weights in total). We multiply each of these weights with the value it is overlapping on the blue image and then add all of them together. This summed value will then be the new value projected to the green matrix above as our output! The border you see around the blue image is known as padding and it allows the convolution mechanic to go over the edge a bit.\n",
    "\n",
    "**Note**: This is just for extra credit but notice the setup here. You have a filter that is a matrix of values, and you are multiplying it with an input set of values. This sounds kind of like a Linear layer right? Exactly! A convolution isn't all that different from a linear layer, its just that rather than flattening an entire image and passing through a single linear layer, we grab small parts of an image and iteratively pass it through the linear layer so we can focus on more local features!\n",
    "\n",
    "## Parameters for Convolutions\n",
    "Luckily for us, PyTorch has a convolution function built for us but there are a few of them depending on the dimension we want. Conv1d would be for a timeseries type of problem that we will explore later. Conv2d is what we need as our images are all 2D (X and Y dimension). There is also higher order ones for data that has more dimensions. MRI Images for example have X, Y and Z dimensions as the brain is a 3D structure, so you would have to use a 3d convolution for that.\n",
    "\n",
    "```\n",
    "torch.nn.Conv2d(in_channels,  # Number of Input Channels (RGB images have 3 Input Channels, Grayscale has just 1)\n",
    "                out_channels, # Number of channels we want to expand out to \n",
    "                kernel_size,  # The size of the filter we want\n",
    "                stride=1,     # How much the filter should move in every iteration\n",
    "                padding=0     # How much border to put around the image\n",
    "                )\n",
    "```\n",
    "\n",
    "## What is a filter? Lets take a look at the Sobel Filter\n",
    "![image](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Bikesgray.jpg/400px-Bikesgray.jpg) ![image](https://upload.wikimedia.org/wikipedia/commons/thumb/1/17/Bikesgraysobel.jpg/400px-Bikesgraysobel.jpg)\n",
    "\n",
    "Convolution filters are not a new concept and have existed for decades. The sobel filter for example can perform tasks like edge detection by performing the following:\n",
    "\n",
    "\n",
    "$$G_x = \\begin{bmatrix}1 & 0 & -1 \\\\ 2 & 0 & -2 \\\\ 1 & 0 & -1\\end{bmatrix} \\ast Image$$\n",
    "\n",
    "\n",
    "$$G_y = \\begin{bmatrix}1 & 2 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1\\end{bmatrix} \\ast Image$$\n",
    "\n",
    "Where $\\ast$ is the Convolution operator visualized by the convolution above! If we did the convolutions and multiplied our images by these matricies we could detect horizontal and vertical lines. What a convolutional Neural Network does is, instead of having some predefined filter like the sobel filter, why dont we let the model learn the best filter for that specific task!\n",
    "\n",
    "## The Typical CNN Structure \n",
    "![image.png](https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png)\n",
    "\n",
    "This is the typical way we structure Convolutional Neural Networks. The key features to note here is, as we we progress through the model, the size of the outputs become smaller and smaller, but we increase significantly the number of channels. We also see here something called subsampling between convolutions, or known as Pooling. We will discuss that in just a bit! Once we get to the end, we simply flatten our features and pass to a Linear layer that will then try to map the image features to our output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d32694fa-9439-4661-a887-cc88bb0cfda1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61394008-402e-4c78-bca1-f1aa55132010",
   "metadata": {},
   "source": [
    "## PyTorch Convolutions\n",
    "Lets try using the PyTorch Convolution mechanic and just see what happens on a dummy image. We will create an image with 3 channels and have dimensions 128 pixels x 128 pixels. We will then create a few different convolutions to see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21594df4-f4de-44bd-b79c-d81fe0e1debe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Output: torch.Size([3, 126, 126])\n",
      "Larger Kernel Output: torch.Size([3, 122, 122])\n",
      "Larger Stride Output: torch.Size([3, 42, 42])\n",
      "Larger Padding Output: torch.Size([3, 130, 130])\n",
      "More Ouput Channels Output: torch.Size([64, 126, 126])\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Image\n",
    "rand = torch.rand(3, 128, 128) \n",
    "\n",
    "### Baseline Convolution ###\n",
    "conv1 = nn.Conv2d(in_channels=3,\n",
    "                  out_channels=3,\n",
    "                  kernel_size=3,\n",
    "                  stride=1,\n",
    "                  padding=0)\n",
    "\n",
    "### Convolution With a Larger Kernel ###\n",
    "conv2 = nn.Conv2d(in_channels=3,\n",
    "                  out_channels=3,\n",
    "                  kernel_size=7,\n",
    "                  stride=1,\n",
    "                  padding=0)\n",
    "\n",
    "### Convolution With a Larger Stride ###\n",
    "conv3 = nn.Conv2d(in_channels=3,\n",
    "                  out_channels=3,\n",
    "                  kernel_size=3,\n",
    "                  stride=3,\n",
    "                  padding=0)\n",
    "\n",
    "### Convolutions with a Larger Padding ###\n",
    "conv4 = nn.Conv2d(in_channels=3,\n",
    "                  out_channels=3,\n",
    "                  kernel_size=3,\n",
    "                  stride=1,\n",
    "                  padding=2)\n",
    "\n",
    "### Convolutions with More Output Channels ###\n",
    "conv5 = nn.Conv2d(in_channels=3,\n",
    "                  out_channels=64,\n",
    "                  kernel_size=3,\n",
    "                  stride=1,\n",
    "                  padding=0)\n",
    "\n",
    "\n",
    "conv1_out = conv1(rand)\n",
    "conv2_out = conv2(rand)\n",
    "conv3_out = conv3(rand)\n",
    "conv4_out = conv4(rand)\n",
    "conv5_out = conv5(rand)\n",
    "\n",
    "print(\"Baseline Output:\", conv1_out.shape)\n",
    "print(\"Larger Kernel Output:\", conv2_out.shape)\n",
    "print(\"Larger Stride Output:\", conv3_out.shape)\n",
    "print(\"Larger Padding Output:\", conv4_out.shape)\n",
    "print(\"More Ouput Channels Output:\", conv5_out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b543b0aa-d8d8-407a-96ea-e0fcb1596b17",
   "metadata": {},
   "source": [
    "Lets note some of the properties we see here. First the default settings from PyTorch (a stride of 1 and padding of 0) keeps the same number of output channels as 3 but we have had some decrease in the image size now from 128 to 127. Lets list out how the other things impact it:\n",
    "\n",
    "- **Larger Kernel Size:** Further decreases the output image size from 128 to 122\n",
    "- **Larger Stride:** Significantly decreases the output image size from 128 to 42\n",
    "- **Larger Padding:** Increases the output image size from 128 to 30\n",
    "- **More Output Channels:** Keeps the same 126 shape output as our baseline but we now have 64 channels instead of 3\n",
    "\n",
    "We can actually calculate the size of an image after a convolution is applied to it with a simple formula!\n",
    "\n",
    "$$\\frac{Width - KernelSize + 2*Padding}{Stride} + 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8247843d-ebc8-411a-87df-5b140bfc58c0",
   "metadata": {},
   "source": [
    "How about we try to stack some convolutions like we have to do in a Neural Network? Now there is one thing to keep in mind here. The first convolution will output however many channels you set it to, so the second convolution has to allow their in_channels to accept that amount!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c1e2dc5-a7f6-4044-8206-1b2393f0fda4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Convolution Shape: torch.Size([64, 60, 60])\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Image\n",
    "rand = torch.rand(3, 128, 128) \n",
    "\n",
    "conv1 = nn.Conv2d(in_channels=3,\n",
    "                  out_channels=32, ### 32 Output Channels\n",
    "                  kernel_size=7,\n",
    "                  stride=2,\n",
    "                  padding=1)\n",
    "\n",
    "conv2 = nn.Conv2d(in_channels=32,\n",
    "                  out_channels=64, ### 32 Input Channels\n",
    "                  kernel_size=3,\n",
    "                  stride=1,\n",
    "                  padding=0)\n",
    "\n",
    "output = conv2(conv1(rand))\n",
    "\n",
    "print(\"Stacked Convolution Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2299eee1-708b-478d-b3e2-76a5f90a64dd",
   "metadata": {},
   "source": [
    "We were able to stack convolutions together! Notice that we have a lot of channels now and have considerable downsized the images from 128 down to 60 here. But there is a few other things we need to talk about before we make our first Convolutional Neural Network.\n",
    "\n",
    "\n",
    "## Pooling\n",
    "Convolutions are expensive computations, so downscaling of our images or outputs of convolutions would greatly help reduce the number of dimensions! To do this we will use Pooling layers. The function identically to Convolutions except there is no parameters to learn. Instead of doing the sum of the product of weights of the filter and pixel values like we saw previously, we can do two other things:\n",
    "\n",
    "- **Average Pooling:** This is just a simple average of the pixels covered by the filter as our output for that specific location\n",
    "- **Max Pooling:** Even less computation as we just take the maximum value of the pixels covered by the filter\n",
    "\n",
    "The inputs are very similar to the Convolution function, but there is no channels anymore. The reason is, we will take the average/max at every iteration for every channel. Therefore we will maintain the same number of channels in our output, but will downsize the overall dimension of the image. Lets take a quick look at the functions!\n",
    "\n",
    "```\n",
    "torch.nn.AvgPool2d(kernel_size, stride=None, padding=0)\n",
    "torch.nn.MaxPool2d(kernel_size, stride=None, padding=0)\n",
    "\n",
    "```\n",
    "Another thing to keep in mind! As you can see above the stride is None. This is because, by default, the stride will be the same as the kernel size. This will essentially do pooling with non-overlapping intervals. If your filter size is 3, then you will do an average/max at that [3x3] patch of pixels, and then move over 3 pixels, therefore having no overlap. We can make the stride less than our kernel size to reduce the amount of downsampling we are doing. \n",
    "\n",
    "\n",
    "Lets take a look at the pooling function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf09df0c-be4a-4380-8391-5a963da44789",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Size == Stride torch.Size([3, 42, 42])\n",
      "Reduced Stride to 2 torch.Size([3, 63, 63])\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Image\n",
    "rand = torch.rand(3, 128, 128) \n",
    "\n",
    "### Average Pool Where Stride == Kernel_Size ###\n",
    "avgpool = nn.AvgPool2d(kernel_size=3)\n",
    "\n",
    "### Average Pool Where We Decrease Stride ###\n",
    "avgpool_2 = nn.AvgPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "\n",
    "out_1 = avgpool(rand)\n",
    "out_2 = avgpool_2(rand)\n",
    "\n",
    "print(\"Kernel Size == Stride\", out_1.shape)\n",
    "print(\"Reduced Stride to 2\", out_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ea44ea-f08b-4bb7-88c2-27baec7e475f",
   "metadata": {},
   "source": [
    "Again, just as we saw before, the pooling with a smaller stride had less downsampling! \n",
    "\n",
    "### Adaptive Pooling\n",
    "\n",
    "There is one other function that is very helpful and this is **Adaptive Pooling**. In regular pooling, we set ahead of time the kernel size and stride. In Adaptive Pooling, we set the output size we want and it will figure out the kernel size and stride for us! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2ed5da5-1606-4175-a15e-fa82ea191acb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Pool 1: torch.Size([3, 64, 64])\n",
      "Avg Pool 2: torch.Size([3, 1, 1])\n",
      "Avg Pool 3: torch.Size([3, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Image\n",
    "rand = torch.rand(3, 128, 128) \n",
    "\n",
    "adaptiveavgpool_1 = nn.AdaptiveAvgPool2d((64,64)) # Output with size 64 x 64\n",
    "adaptiveavgpool_2 = nn.AdaptiveAvgPool2d((1,1))   # Output with size 1 x 1 \n",
    "adaptiveavgpool_3 = nn.AdaptiveAvgPool2d((32,64)) # Output with size 32 x 64\n",
    "\n",
    "out_1 = adaptiveavgpool_1(rand)\n",
    "out_2 = adaptiveavgpool_2(rand)\n",
    "out_3 = adaptiveavgpool_3(rand)\n",
    "\n",
    "print(\"Avg Pool 1:\", out_1.shape)\n",
    "print(\"Avg Pool 2:\", out_2.shape)\n",
    "print(\"Avg Pool 3:\", out_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffbca4f-0289-44ca-98af-f6f710405732",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "Batch Normalization is also an extremely important aspect of Deep Learning that was introduced after AlexNet. Although this was not a part of the official implementation of AlexNet we will add it in anyway!\n",
    "\n",
    "The idea of Batch Norm is very simple! We talked about this previously and you should have seen these values in our [DataLoader Tutorial](https://github.com/priyammaz/PyTorch-Adventures/tree/main/PyTorch%20Basics/PyTorch%20DataLoaders) that we breifly explained:\n",
    "\n",
    "```\n",
    "transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "```\n",
    "\n",
    "When we process the data in our transforms, we will pre-normalize them. What these values above describe are the mean and standard deviation across pixel values in ImageNet for each channel (RGB), therefore are probably good values to use for tasks similar to ImageNet. If you have a completely different dataset then you can calculate this for your usecase. Normalization is important as it improves our ability to optimize these high dimensional spaces efficiently. \n",
    "\n",
    "But we have a problem! Sure the input data was normalized, but the second we do any nonlinear calculation on it (Linear layers and convolutions with activation functions) there is no guarantee that the output remains normalized. Therefore, before we pass the tensor to the next layer, we will do Batch Normalization to normalize our data throughout!\n",
    "\n",
    "![Batchnorm](https://production-media.paperswithcode.com/methods/batchnorm.png)\n",
    "\n",
    "What Batch normalization does is it calculates the Mean and Standard Deviation across all the samples (Dimension N) and then uses that to normalize the current batch. During training BatchNorm keeps a running tally of the mean and standard deviations is calculates over many random batches and aggregates them so it can estimate the true Mean and Standard deviation to use during prediction.\n",
    "\n",
    "```\n",
    "nn.BatchNorm2d(num_features)\n",
    "```\n",
    "\n",
    "The only input BatchNorm needs is, for computer vision type tensors, the number of channels it should expect!\n",
    "\n",
    "\n",
    "## Dropout\n",
    "\n",
    "Again, overfitting is a strong concern for Deep Learning as these models have very large expressive capacities. There are a few things we can do though to try to help called **Dropout**. Dropout is essentially randomly turning off different nodes in a neural network. \n",
    "\n",
    "![dropout](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*iWQzxhVlvadk6VAJjsgXgg.png)\n",
    "\n",
    "[credit](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5)\n",
    "\n",
    "In every forward/backward pass, a neuron can be turned off with some probability, meaning the model, during training, has less parameters to help it learn. By reducing this, we are also making the flexiblity of any single forward pass less while pushing the overall generalization of the model\n",
    "\n",
    "```\n",
    "nn.Dropout(p=0.5)\n",
    "```\n",
    "\n",
    "## AlexNet\n",
    "In the previous lesson on Transfer Learning we directly used the AlexNet model from the PyTorch implementation, but we will now put it together ourselves!\n",
    "\n",
    "![AlexNet](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_6.35.45_PM.png)\n",
    "\n",
    "In the above image, you will notice there is two paths of neural networks. This was because in 2012, a model of this size would never fit on a single GPU, so we had to train across two of them. Lucky for us, this model is quite small compared to todays standard and can easily fit here, so we can ignore one of the branches all together! \n",
    "\n",
    "For reference, this model will follow the [PyTorch Torchvision.models](https://github.com/pytorch/vision/blob/main/torchvision/models/alexnet.py) implementation pretty closely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "118ba453-007b-4464-9965-72d47adf250e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, classes=2, dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        self.classes = classes\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=3, out_channels=64, kernel_size=11, stride=4, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),  \n",
    "                nn.BatchNorm2d(num_features=64), # ADDED IN BATCHNORM\n",
    "                \n",
    "                nn.Conv2d(in_channels=64, out_channels=192, kernel_size=5, stride=1, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2), \n",
    "                nn.BatchNorm2d(num_features=192), # ADDED IN BATCHNORM \n",
    "                \n",
    "                nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                nn.BatchNorm2d(num_features=384), # ADDED IN BATCHNORM \n",
    "                \n",
    "                \n",
    "                nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm2d(num_features=256), # ADDED IN BATCHNORM \n",
    "                \n",
    "                nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                nn.BatchNorm2d(num_features=256), # ADDED IN BATCHNORM \n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6,6))\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "                nn.Dropout(dropout_p),\n",
    "                nn.Linear(256*6*6, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_p),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(batch_size, -1)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290da586-eaa1-4a94-95b4-20d7925242b4",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45375378-cf43-4927-a10b-d59c59356615",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Build Cats vs Dogs Dataset ###\n",
    "PATH_TO_DATA = \"../../data/PetImages/\"\n",
    "\n",
    "### DEFINE TRANSFORMATIONS ###\n",
    "normalizer = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]) ### IMAGENET MEAN/STD ###\n",
    "train_transforms = transforms.Compose([\n",
    "                                        transforms.Resize((224,224)),\n",
    "                                        transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        normalizer\n",
    "                                      ])\n",
    "\n",
    "\n",
    "dataset = ImageFolder(PATH_TO_DATA, transform=train_transforms)\n",
    "\n",
    "train_samples, test_samples = int(0.9 * len(dataset)), len(dataset) - int(0.9 * len(dataset))\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, lengths=[train_samples, test_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a121850f-922f-4fc3-93af-9732d9af8f17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Device cuda\n",
      "Starting Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:24<00:00,  7.27it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5161231678317894\n",
      "Training Acc: 0.7418191896920855\n",
      "Validation Loss: 0.4387053146958351\n",
      "Validation Acc: 0.7840221762657166\n",
      "Starting Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:23<00:00,  7.56it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3718796923587268\n",
      "Training Acc: 0.8310450897975401\n",
      "Validation Loss: 0.3525701269507408\n",
      "Validation Acc: 0.8399319529533387\n",
      "Starting Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:23<00:00,  7.50it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2938833811574362\n",
      "Training Acc: 0.8699672242457216\n",
      "Validation Loss: 0.3450699120759964\n",
      "Validation Acc: 0.8492817521095276\n",
      "Starting Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:23<00:00,  7.50it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.24598711356520653\n",
      "Training Acc: 0.8952078856527805\n",
      "Validation Loss: 0.31320265755057336\n",
      "Validation Acc: 0.860710683465004\n",
      "Starting Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:23<00:00,  7.56it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2053669745271856\n",
      "Training Acc: 0.9153125960041176\n",
      "Validation Loss: 0.2710632346570492\n",
      "Validation Acc: 0.8777973771095275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### SELECT DEVICE ###\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on Device {DEVICE}\")\n",
    "\n",
    "### LOAD IN and Modify AlexNet Model ###\n",
    "model = AlexNet()\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "### MODEL TRAINING INPUTS ###\n",
    "epochs = 5\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "batch_size = 128\n",
    "\n",
    "### BUILD DATALOADERS ###\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "def train(model, device, epochs, optimizer, loss_fn, batch_size, trainloader, valloader):\n",
    "    log_training = {\"epoch\": [],\n",
    "                    \"training_loss\": [],\n",
    "                    \"training_acc\": [],\n",
    "                    \"validation_loss\": [],\n",
    "                    \"validation_acc\": []}\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Starting Epoch {epoch}\")\n",
    "        training_losses, training_accuracies = [], []\n",
    "        validation_losses, validation_accuracies = [], []\n",
    "        \n",
    "        model.train() # Turn On BatchNorm and Dropout\n",
    "        for image, label in tqdm(trainloader):\n",
    "            image, label = image.to(DEVICE), label.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            out = model.forward(image)\n",
    "        \n",
    "            ### CALCULATE LOSS ##\n",
    "            loss = loss_fn(out, label)\n",
    "            training_losses.append(loss.item())\n",
    "\n",
    "            ### CALCULATE ACCURACY ###\n",
    "            predictions = torch.argmax(out, axis=1)\n",
    "            accuracy = (predictions == label).sum() / len(predictions)\n",
    "            training_accuracies.append(accuracy.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval() # Turn Off Batchnorm \n",
    "        for image, label in tqdm(valloader):\n",
    "            image, label = image.to(DEVICE), label.to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                out = model.forward(image)\n",
    "\n",
    "                ### CALCULATE LOSS ##\n",
    "                loss = loss_fn(out, label)\n",
    "                validation_losses.append(loss.item())\n",
    "\n",
    "                ### CALCULATE ACCURACY ###\n",
    "                predictions = torch.argmax(out, axis=1)\n",
    "                accuracy = (predictions == label).sum() / len(predictions)\n",
    "                validation_accuracies.append(accuracy.item())\n",
    "\n",
    "        training_loss_mean, training_acc_mean = np.mean(training_losses), np.mean(training_accuracies)\n",
    "        valid_loss_mean, valid_acc_mean = np.mean(validation_losses), np.mean(validation_accuracies)\n",
    "\n",
    "        log_training[\"epoch\"].append(epoch)\n",
    "        log_training[\"training_loss\"].append(training_loss_mean)\n",
    "        log_training[\"training_acc\"].append(training_acc_mean)\n",
    "        log_training[\"validation_loss\"].append(valid_loss_mean)\n",
    "        log_training[\"validation_acc\"].append(valid_acc_mean)\n",
    "\n",
    "        print(\"Training Loss:\", training_loss_mean) \n",
    "        print(\"Training Acc:\", training_acc_mean)\n",
    "        print(\"Validation Loss:\", valid_loss_mean)\n",
    "        print(\"Validation Acc:\", valid_acc_mean)\n",
    "        \n",
    "    return log_training, model\n",
    "\n",
    "\n",
    "random_init_logs, model = train(model=model,\n",
    "                                device=DEVICE,\n",
    "                                epochs=epochs,\n",
    "                                optimizer=optimizer,\n",
    "                                loss_fn=loss_fn,\n",
    "                                batch_size=batch_size,\n",
    "                                trainloader=trainloader,\n",
    "                                valloader=valloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa20c08-5882-4dcd-ac8c-a509fe97b8b0",
   "metadata": {},
   "source": [
    "And AlexNet is done! We have officially implemented one of the earliest examples of Deep Learning for Computer Vision. Although this was the state of the art model in 2012, a lot has happened since then. The main limitation that we will explore next is the **Deep** part of deep learning. This model only has a couple of layers and they were able to show that once the model got too deep we were no longer able to optimize it anymore. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
