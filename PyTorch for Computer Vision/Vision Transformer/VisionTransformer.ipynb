{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326e4e9b-1b0e-4704-aab5-c5115224bd19",
   "metadata": {
    "tags": []
   },
   "source": [
    "![banner](https://raw.githubusercontent.com/priyammaz/HAL-DL-From-Scratch/main/src/visuals/banner.png)\n",
    "\n",
    "# Vision Transformer From Scratch\n",
    "\n",
    "**Code Heavily Inspired by the Following Repositories!!!**\n",
    "\n",
    "[Huggingface Pytorch Image Models](https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py)\n",
    "\n",
    "\n",
    "\n",
    "[Lucidrains ViT Pytorch](https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/local_vit.py)\n",
    "\n",
    "[Jankrepl Github Adventures](https://github.com/jankrepl/mildlyoverfitted/blob/master/github_adventures/vision_transformer/custom.py)\n",
    "\n",
    "[Karpathy minGPT](https://github.com/karpathy/minGPT/blob/master/mingpt/model.py)\n",
    "\n",
    "### Some Pre-Reqs:\n",
    "- You should be comfortable in PyTorch at this point and atleast trained one Convolutional Neural Network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1d26fa3-d942-46af-830d-155c4d5e19ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Packages ###\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Normalize, RandomHorizontalFlip, Resize, ToTensor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d667857d-3df3-46eb-bbfb-f7d2cdc37df1",
   "metadata": {},
   "source": [
    "### Patch Embedding (The Main Addition Over The Vanilla Transformer)\n",
    "\n",
    "The purpose of patch embeddings is to convert an Image into a \"sequence\" type data. To do so we will split an image into patches of size **patch_size** and encode each patch with a vector of length **embed_dim**\n",
    "\n",
    "![PatchEmbedding](imgs/patchembedding.png)\n",
    "\n",
    "\n",
    "Assume you have an **image of size 3x224x224** and we want to use a **patch_size of 16** and **embed_dim of 768**.\n",
    "\n",
    "The number of patches can be calculated as $(\\dfrac{224}{16})^2 = 196$. Therefore we want to get to the tensor of shape **(196 x 768)** where each patch is being represented with this embedding dimension.\n",
    "\n",
    "We have two ways to do this:\n",
    "1) Write a patching funciton to split our image of **(3 x 224 x 224)** to a **(196 x 3 x 14 x 14)**. We can then flatten the last three dimensions and get a tensor of  **(196 x 3\\*14\\*14)** and then use a linear layer to scale the **3\\*14\\*14** to our **embed_dim**\n",
    "2) The prefered method will be to utilize the convolution mechanic in PyTorch to patch the image for us. \n",
    "    - The convolution gives us the following iputs and we will fill them as such:\n",
    "        - in_channels = 3 (We have RGB Images)\n",
    "        - out_channels = 768 (What is the embed dim we want?)\n",
    "        - kernel_size = 16 (How large do we want each patch?)\n",
    "        - stride = 16 (If we dont want any overlapping patches our kernel size and stride have to be the same)\n",
    "        \n",
    "    - By doing this, we can essentially have a filter of size 16 traversing over our image with 16 pixel shifts, which will create 196 non-overlapping patches and return the tensor of shape **(768 x 14 x 14)**. We will then flatten on the last dimension to get **(768 x 196)** and then transpose the tensor to get our final output of **(196 x 768)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a97ed23-2959-461f-be6e-c131cefb0d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchEmbed module will take an input image in the shape (C, H, W), patch the image into\n",
    "    patches of size patch_size and embed each patch into embedding dim of embed_dim\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=786):\n",
    "        super(PatchEmbed, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        ### Calculate Number of Patches ###\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        ### Use Convolution to Patch the Images ###\n",
    "        self.proj = nn.Conv2d(in_channels=in_chans,\n",
    "                              out_channels=embed_dim,\n",
    "                              kernel_size=patch_size,\n",
    "                              stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x) # (batch , embed_dim , sqrt(n_patches) , sqrt(n_patches))\n",
    "        x = x.flatten(2) # (batch , embed_dim , n_patches)\n",
    "        x = x.transpose(1,2) # (batch, n_patches, embed_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be6e849-f0cc-46ff-b8b2-ade91efc0be1",
   "metadata": {},
   "source": [
    "### Class Tokens and Positional Embeddings\n",
    "Unfortunately Transformers have an issue of Permuation Invariance. What this means is, if we were doing an NLP task, the Transformer wouldn't pay any attention to the order or words (which is obviously a problem for sequence tasks). Also, we need to be able to aggregate all the information across all the patches into a single vector that can then be used for classification at the end. We will codify these two things later when defining the final VisionTransformer but for now lets review the concept.\n",
    "\n",
    "![cls_embed](imgs/embed_cls.png)\n",
    "\n",
    "What we will do first is concatenate onto our original tensor of shape **(196 x 768)** and concatenate on some learnable parameters **CLS_TOKEN** of shape **(1 x 768)** to create our final data shape of **(197 x 768)**. We will then add to this new tensor additional learnable parameters called the **Positional Embeddings** of the shape **(197 x 768)** that will learn to encode spatial (or temporal in sequence tasks) relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37812e6a-f5a9-4892-89d0-4103a960ed76",
   "metadata": {},
   "source": [
    "### Single Headed Attention\n",
    "\n",
    "To calculate attention we will have to generate three distinct projections of our data, known as the **Queries**, **Keys** and **Values**. The purpose of this is to use the Queries and Keys to create score weights between different patches of the image and then use that new weight matrix to perform something analogous to a weighted average of the Values matrix.\n",
    "\n",
    "#### **Step 1: Projection**\n",
    "\n",
    "We will take our Input Data **(197 x 768)** that includes that **CLS TOKEN** and has the **Positional Embeddings** added to it and project it to three seperate tensors of the same shape. Essentially we are doing three seperate matrix multiplications on weight matricies $W_Q$, $W_K$ , $W_V$ which is the same as doing a simple Linear layer. Because we want the shape to be the same, if we have an input embedding of 768, we will output the same 768, therefore using a Linear layer $nn.Linear(768,768)$\n",
    "![projection](imgs/projection.png)\n",
    "\n",
    "#### **Step 2: Build the Attention Matrix**\n",
    "We will multiply together our Queries and the Transpose of the Keys to generate a tensor of shape **(197 x 197)**. More specifically, we have a matrix of **(num_patches + 1) x (num_patches + 1)** which will encode the relationship between each pair of patches.\n",
    "![attn](imgs/att_mat.png)\n",
    "\n",
    "#### **Step 3: Normalize and Softmax**\n",
    "Next we divide the resulting Attention matrix by the square root of our embedding length. It was shown that the variance of the attention matrix scaled higher with larger embedding lenghts so this is a simple normalization procedue to make sure the variance of our Q, K and Attention Matrix are all about the same.\n",
    "\n",
    "More important is our softmax, to ensure that each row of our attention matrix adds to 1. This way, it turns the next calcualation into a simple weighted average of the values. \n",
    "![norm_softmax](imgs/norm_softmax.png)\n",
    "\n",
    "#### **Step 4: Multiply with Values for Output**\n",
    "We then take our Attention matrix of shape **(197 x 197)** and multiply it with our third projection, the Values with shape **(197 x 768)** giving us the final Output of shape **(197 x 768)**. Notice that the output tensor shape of this entire Attention calculation is identical to the input shape! This allows us to quickly scale the model in the future.\n",
    "![mult_w_v](imgs/mult_w_v.png)\n",
    "\n",
    "\n",
    "#### **Aside: Couple of Details on why its a Weighted Average**\n",
    "Pretend our Attention matrix is of shape **(3 x 3)** and the values matrix is of shape **(3 x 768)**. After performing softmax on our attention matrix, each row will sum to one. Therefore, when multiplying with the Values matrix, each row of the Attention martrix is encoding a weighted sum of the embeddings of the values matrix.\n",
    "![wa](imgs/weighted_average.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b87288a-8517-44b8-abda-9200d5b8dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Attention Head to calculate the Q, K, V and return the weighted average matrix via 3 linear layers\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=768, head_dim=768, attn_p=0):\n",
    "        super(Head, self).__init__()\n",
    "        self.query = nn.Linear(embed_dim, head_dim)\n",
    "        self.key = nn.Linear(embed_dim, head_dim)\n",
    "        self.value = nn.Linear(embed_dim, head_dim)\n",
    "        self.attn_dropout = nn.Dropout(attn_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, n_patch, embed_dim = x.shape\n",
    "        q = self.query(x) # (batch, n_patches+1, head_dim)\n",
    "        k = self.key(x) # (batch, n_patches+1, head_dim)\n",
    "        v = self.value(x) # (batch, n_patches+1, head_dim)\n",
    "\n",
    "        sam = (q @ k.transpose(-2,-1)) * embed_dim**-0.5 # (batch , n_patches+1, n_patches+1)\n",
    "        attn = sam.softmax(dim=-1) # (batch , n_patches+1, n_patches+1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        weighted_average = attn @ v # (batch , n_patches+1, head_dim)\n",
    "        return weighted_average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898171f9-c148-4bc3-9e60-8754b97eb3a1",
   "metadata": {},
   "source": [
    "### MultiHeaded Attention\n",
    "\n",
    "MultiHeaded attention only changes one aspect of all the previous ideas we explored. Instead of only have a single set of projections Q, K , V that take input tensor of shape **(197 x 768)** and project to a new tensor of the same shape **(197 x 768)**, what if we do this multiple times? \n",
    "\n",
    "The first step is to determine the number of heads we want, and lets say we want 12. This would indicate that the dimension of each head should be  $(\\dfrac{768}{12}) = 64$. Therefore, we want to take an input tensor of **(197 x 768)** and project it to a new tensor for the Q, K and V of shape **(197 x 64)**. Again we can do this with a simple linear layer, but instead of $nn.Linear(768, 786)$ we will do $nn.Linear(768, 64)$.\n",
    "\n",
    "![multihead](imgs/multihead.png)\n",
    "\n",
    "We wil then repeat this step 12 times with 12 unique triplets of linear layers and get 12 outputs each of shape **(197 x 64)**. We can then concatenate all these back together to get the final shape **(197 x 768)**, right back to where we started! The problem here though is we just concatenated the 12 attention heads together, but they never got a chance to share information across one another, therefore we will use another linear layer $nn.Linear(768, 786)$ to perform some information meshing. \n",
    "\n",
    "![concatenate](imgs/concatenateheads.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b72b1b3a-9223-46dd-b4f3-6b7e7e603acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple Attention Head to repeat Head module num_heads times and concatenate outputs of heads together.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=768, num_heads=12, attn_p=0, proj_p=0):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.head_size = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList([Head(embed_dim=embed_dim, head_dim=self.head_size, attn_p=attn_p) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (batch, n_patches+1, embed_dim)\n",
    "        out = self.proj_drop(self.proj(out)) # (batch, n_patches+1, embed_dim)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb89d73-f3eb-4953-aa52-9f730586e18d",
   "metadata": {},
   "source": [
    "### Efficient Attention\n",
    "As we saw in our MultiHeaded Attention example, we are doing a for loop to pass our tensor through each head. We obviously would like to make this run in parallel so we can put the Attention Head and MultiHeaded Attention together. Another change is, instead of using three seperate linear layers for Q, K and V, we can actually just use a single linear layer $nn.Linear(768, 3*786)$ that essentially outputs the same thing in a single weight matrix. We can then do some fun matrix manipulations to extract our Q,K and V and do our attention calculations. \n",
    "\n",
    "There is no good way to draw this, so step through the code below line by line and pay close attention to the shapes of each tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03ecb087-9ef1-4ba9-8247-67042ae9b831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, attn_p, proj_p):\n",
    "        super(EfficientAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = int(self.embed_dim / num_heads)\n",
    "\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim*3)\n",
    "        self.attn_dropout = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, patches, embed_dim = x.shape # (batch, n_patches+1, embed_dim)\n",
    "        qkv = self.qkv(x) # (batch, n_patches+1, 3*embed_dim)\n",
    "        qkv = qkv.reshape(batch, patches, 3, self.num_heads, self.head_size) # (batch, patch+1, 3, num_heads, head_size)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch, num_heads, patches+1, head_size)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2] # Each of shape (batch, num_heads, patches+1, head_size)\n",
    "\n",
    "        ### SAME AS BEFORE NOW ###\n",
    "        sam = (q @ k.transpose(-2,-1)) * self.head_size**-0.5 # (batch, num_heads, patches+1, patches+1)\n",
    "        attn = sam.softmax(dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        weighted_average = attn @ v # (batch, num_heads, patches+1, head_size)\n",
    "        weighted_average = weighted_average.transpose(1,2) # (batch, patches+1, num_heads, head_size)\n",
    "        weighted_average = weighted_average.flatten(2) # (batch, patches+1, embed_dim)\n",
    "        out = self.proj_drop(self.proj(weighted_average))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f91332-02d8-4a52-b34c-cba0db9797fb",
   "metadata": {},
   "source": [
    "### MultiLayer Perceptron\n",
    "As we can see from this small snippet of the image in the original Transformer paper, after performing our MultiHeaded attention calculation, we have a simple Feed Forward module. This is just a typical MLP with a few linear layers, and the only parameter is the **mlp_ratio** which we will specify a little later. The overview is if the input is of dimension 768, then we will have a hidden layer of size 768 * mlp_ratio and then another output layer that will bring it back to a dimension of 768. \n",
    "![feedforward](imgs/feedforward.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "865db27d-e758-4316-acca-87b85a4a3b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, mlp_p=0):\n",
    "        \"\"\"\n",
    "        Hidden features should be some MLP_ratio * in_features. \n",
    "        Out Features == In Features\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(mlp_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x)) # (batch, n_patches+1, embed_dim * mlp_ratio)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x) # (batch, n_patches+1, embed_dim)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3365cac4-ce56-4b25-86d1-7a5d201fd8ac",
   "metadata": {},
   "source": [
    "### Putting Together the **Transformer Block**\n",
    "In this step we will add in all parts we have build previously together in a transformer block, but there is two new concept we need to look at: \n",
    "\n",
    "#### **Layer Normalization**\n",
    "\n",
    "You should all be familiar with Batch Normalization, but the idea is, throughout the neural network, we will calculate the mean and standard deviation of a tensor across all the batches and use that to normalize our data. There are a few problems with this method that makes it difficult to use in sequence tasks:\n",
    "- Typically sequence data has different lengths and so we add padding. This wouldnt really work with normalizing across batches because how do we handle the padding tokens?\n",
    "- It fails with small batch sizes. These models can be very large and so we can only do a small batch size, but that would not be a good sample to estimate population parameters of our mean and standard deviation.\n",
    "- We often have to train these models across GPUs but BatchNorm would have to sync the calculated mean and std across the GPUS during forward propagation which is messy. \n",
    "\n",
    "![layernorm](imgs/layernorm.png)\n",
    "\n",
    "We then opt for Layernormalization which essentially just normalizes each sample individually across the embedding dimension rather than across the batch. This also means we have to let LayerNormalization know what is the dimension it can expect, and because our embedding dimension is 768, we would have to instantiate it as $nn.LayerNorm(768)$\n",
    "\n",
    "#### **Residual Connections**\n",
    "This is inspired by ResNet but as we know, the deeper a Neural Netowrk goes, we have vanishing gradient problems as backpropagation cannot make it back to the start of the model. Therefore to give other paths for gradients to flow back, we will add new calculations to previous ones rather than replace them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b7bfd2b-88a8-4bce-8ae2-5847a70b5fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Full Transformer block with Attention and Linear Layers\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4.0,\n",
    "                 mlp_p=0, attn_p=0, proj_p=0, efficient=True):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "\n",
    "        if efficient:\n",
    "            self.attn = EfficientAttention(embed_dim=embed_dim,\n",
    "                                           num_heads=num_heads,\n",
    "                                           attn_p=attn_p,\n",
    "                                           proj_p=proj_p)\n",
    "        else:\n",
    "            self.attn = MultiHeadedAttention(embed_dim=embed_dim,\n",
    "                                             num_heads=num_heads,\n",
    "                                             attn_p=attn_p,\n",
    "                                             proj_p=proj_p)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        hidden_features = int(embed_dim*mlp_ratio)\n",
    "        self.mlp = MLP(in_features=embed_dim,\n",
    "                       hidden_features=hidden_features,\n",
    "                       out_features=embed_dim,\n",
    "                       mlp_p=mlp_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Residual connections to avoid vanishing gradients (notice its is X = X + Out rather than doing X = Out)\n",
    "        \"\"\"\n",
    "        x = x + self.attn(self.norm1(x)) # (batch, n_patches+1, embed_dim)\n",
    "        x = x + self.mlp(self.norm2(x)) # (batch, n_patches+1, embed_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1520e014-dfd0-41a7-9f3f-ac6a38c81cef",
   "metadata": {},
   "source": [
    "### Putting Together the **Vision Transformer**\n",
    "In this next step we will put together all the parts for the Vision Transformer amd there are a few new things included that we talked about previously. \n",
    "\n",
    "This is where we will instantiate the **CLS_TOKEN** of shape **(1 x 768)** to concatenate onto our samples and the **Positional Embeddings** of shape **(197 x 768)**. We want the same values of the CLS tokens to be concatenated onto each sample in a batch so each image can have its own unique calculations on it so we will have to expand the dimension to match the batch size in the forward fuction. The Positional Embeddings on the other hand can just be directly added as we want the same positional encoding regardless of the image. \n",
    "\n",
    "#### **Input Shape = Output Shape**\n",
    "I have been making a big deal about the shapes of tensors, and the reason is the unique part of the Transformer Architecture, is the input shape of the transformer block is identical to the output shape. This means we can just stack a bunch of MultiHeaded attention blocks on top of each other and not worry about shape errors. (Rember in convolutions, the shapes can be really hard to match sometimes so this is a huge annoyance saver!)\n",
    "\n",
    "![stacked](imgs/stacked.png)\n",
    "\n",
    "#### **Classification Task**\n",
    "Once we are at the end, we will strip out only the CLS tokens and use that for prediction to our N_classes. In our case we will do Dogs vs Cats so that will be 2 Classes. In the image here I show that I strip of the bottom because I concatenated the cls tokens to the bottom, but in the implemenation it is concatenated to the top! Regardless the idea is the same\n",
    "\n",
    "![classifier](imgs/classifier.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb11ea18-385b-44a2-a3af-36932dca8739",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    VisionTransfomrer put together. Main parameters to change are:\n",
    "    img_size: Size of input image\n",
    "    patch_size:  Size of individual patches (Smaller patches lead to more patches)\n",
    "    n_classes: Number of outputs for classification\n",
    "    embed_dim: Length of embedding vector for each patch\n",
    "    depth: Number of wanted transformer blocks\n",
    "    num_heads: Number of wanted attention heads per block\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, n_classes=2,\n",
    "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, attn_p=0.2,\n",
    "                 mlp_p=0.2, proj_p=0.2, pos_drop=0.2, efficient=True):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size,\n",
    "                                      patch_size=patch_size,\n",
    "                                      in_chans=in_chans,\n",
    "                                      embed_dim=embed_dim)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
    "        self.pos_embed = nn.Parameter((torch.zeros(1, 1+self.patch_embed.n_patches, embed_dim)))\n",
    "        self.pos_drop = nn.Dropout(pos_drop)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_dim=embed_dim,\n",
    "                                 num_heads=num_heads,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 mlp_p=mlp_p,\n",
    "                                 attn_p=attn_p,\n",
    "                                 proj_p=proj_p,\n",
    "                                 efficient=efficient)\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.head = nn.Linear(embed_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.patch_embed(x) # (batch, n_patches, embed_dim)\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1) # (batch, 1, embed_dim)\n",
    "        x = torch.cat((cls_token, x), dim=1) # (batch, n_patches+1, embed_dim)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[:, 0] # (batch, embed_dim)\n",
    "        x = self.head(cls_token_final) # (batch, n_classes)\n",
    "        return x, cls_token_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f43a99-2485-45df-b9b0-1bd0ede62206",
   "metadata": {},
   "source": [
    "### Training Script\n",
    "\n",
    "At this point everything is done and we just need to train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1d80cb7-ccd3-4865-b500-a691075eb954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, epochs, optimizer,\n",
    "          scheduler, loss_fn, trainloader,\n",
    "          valloader, savepath=\"ViTDogsvCatsSmall.pt\"):\n",
    "    log_training = {\"epoch\": [],\n",
    "                    \"training_loss\": [],\n",
    "                    \"training_acc\": [],\n",
    "                    \"validation_loss\": [],\n",
    "                    \"validation_acc\": []}\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Starting Epoch {epoch}\")\n",
    "        training_losses, training_accuracies = [], []\n",
    "        validation_losses, validation_accuracies = [], []\n",
    "\n",
    "        for image, label in tqdm(trainloader):\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out, _ = model.forward(image)\n",
    "\n",
    "            ### CALCULATE LOSS ##\n",
    "            loss = loss_fn(out, label)\n",
    "            training_losses.append(loss.item())\n",
    "\n",
    "            ### CALCULATE ACCURACY ###\n",
    "            predictions = torch.argmax(out, axis=1)\n",
    "            accuracy = (predictions == label).sum() / len(predictions)\n",
    "            training_accuracies.append(accuracy.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        for image, label in tqdm(valloader):\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            with torch.no_grad():\n",
    "                out, _ = model.forward(image)\n",
    "\n",
    "                ### CALCULATE LOSS ##\n",
    "                loss = loss_fn(out, label)\n",
    "                validation_losses.append(loss.item())\n",
    "\n",
    "                ### CALCULATE ACCURACY ###\n",
    "                predictions = torch.argmax(out, axis=1)\n",
    "                accuracy = (predictions == label).sum() / len(predictions)\n",
    "                validation_accuracies.append(accuracy.item())\n",
    "\n",
    "        training_loss_mean, training_acc_mean = np.mean(training_losses), np.mean(training_accuracies)\n",
    "        valid_loss_mean, valid_acc_mean = np.mean(validation_losses), np.mean(validation_accuracies)\n",
    "\n",
    "        ### Save Model If Val Loss Decreases ###\n",
    "        if valid_loss_mean < best_val_loss:\n",
    "            print(\"---Saving Model---\")\n",
    "            torch.save(model.state_dict(), savepath)\n",
    "            best_val_loss = valid_loss_mean\n",
    "\n",
    "        log_training[\"epoch\"].append(epoch)\n",
    "        log_training[\"training_loss\"].append(training_loss_mean)\n",
    "        log_training[\"training_acc\"].append(training_acc_mean)\n",
    "        log_training[\"validation_loss\"].append(valid_loss_mean)\n",
    "        log_training[\"validation_acc\"].append(valid_acc_mean)\n",
    "\n",
    "\n",
    "        print(\"Training Loss:\", training_loss_mean)\n",
    "        print(\"Training Acc:\", training_acc_mean)\n",
    "        print(\"Validation Loss:\", valid_loss_mean)\n",
    "        print(\"Validation Acc:\", valid_acc_mean)\n",
    "\n",
    "    return log_training, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cce0c378-2830-4ff2-ad6f-268c791a2255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 11019650\n",
      "Training on Device cuda:1\n",
      "Starting Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|█████████▌                                                                                                      | 15/176 [00:04<00:48,  3.32it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m trainloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     45\u001b[0m valloader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m logs, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mViT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                   \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m logs \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(logs)\n\u001b[1;32m     57\u001b[0m logs\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdogsvcats.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, epochs, optimizer, scheduler, loss_fn, trainloader, valloader, savepath)\u001b[0m\n\u001b[1;32m     14\u001b[0m validation_losses, validation_accuracies \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, label \u001b[38;5;129;01min\u001b[39;00m tqdm(trainloader):\n\u001b[0;32m---> 17\u001b[0m     image, label \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     19\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(image)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " ViT = VisionTransformer(embed_dim=384,\n",
    "                            depth=6,\n",
    "                            num_heads=6,\n",
    "                            efficient=True)\n",
    "\n",
    "params = sum([np.prod(p.size()) for p in ViT.parameters()])\n",
    "print(f\"Total Number of Parameters: {params}\")\n",
    "\n",
    "### SETUP DATASET ###\n",
    "PATH_TO_DATA = \"../../HAL Training/data/dogsvcats\"\n",
    "dataset = ImageFolder(PATH_TO_DATA)\n",
    "\n",
    "normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "train_transforms = Compose([\n",
    "    Resize((224, 224)),\n",
    "    RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    normalizer])\n",
    "\n",
    "val_transforms = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    normalizer])\n",
    "\n",
    "train_samples, test_samples = int(0.9 * len(dataset)), len(dataset) - int(0.9 * len(dataset))\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, lengths=[train_samples, test_samples])\n",
    "\n",
    "train_dataset.dataset.transform = train_transforms\n",
    "val_dataset.dataset.transform = val_transforms\n",
    "\n",
    "### SETUP TRAINING LOOP ###\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Training on Device {}\".format(DEVICE))\n",
    "\n",
    "ViT = ViT.to(DEVICE)\n",
    "EPOCHS = 100\n",
    "optimizer = optim.AdamW(params=ViT.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-5,\n",
    "                                              max_lr=1e-3, mode='exp_range',\n",
    "                                              gamma=0.98, cycle_momentum=False)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "batch_size = 128\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "logs, model = train(model=ViT,\n",
    "                    device=DEVICE,\n",
    "                    epochs=EPOCHS,\n",
    "                    optimizer=optimizer,\n",
    "                    scheduler=scheduler,\n",
    "                    loss_fn=loss_fn,\n",
    "                    trainloader=trainloader,\n",
    "                    valloader=valloader)\n",
    "\n",
    "logs = pd.DataFrame(logs)\n",
    "logs.to_csv(\"dogsvcats.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
