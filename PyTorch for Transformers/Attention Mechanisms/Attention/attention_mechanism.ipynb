{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e814a34-3bc3-455a-887c-890ca3f90843",
   "metadata": {},
   "source": [
    "![banner](https://raw.githubusercontent.com/priyammaz/HAL-DL-From-Scratch/main/src/visuals/banner.png)\n",
    "\n",
    "# Attention\n",
    "\n",
    "Attention networks have become crucial in state of the art architectures, namely Transformers! Today we will be delving a bit deeper into attention and how it works! Although attention was mainly intended for use in sequence modeling, it has found its way into Computer Vision, Graphs and basically every domain, demonstrating the flexibility of the architecture. Lets discuss this from a sequence modeling perspective today though just to build intuition on how this works. To start the explanation, lets reference back the original sequence modeling mechanism: **Recurrent Neural Networks**\n",
    "\n",
    "## Recap: Recurrent Neural Networks\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/priyammaz/PyTorch-Adventures/main/src/visuals/recurrent_neural_network_diagram.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "In recurrent neural networks, what we typically do is take our sequence and pass in a single timestep at a time and produce an output. This means when we pass in $x_1$ we create a hidden state $h_1$ that captures all the relevant information in the input, and this hidden state then is used to produce the output $y_1$. Now what makes it an RNN is when we pass in the second timestep $x_2$ to produce the hidden state $h_2$, the hidden state already contains information about the past $h_1$! Therefore our output of $y_2$ is informed both by information from $x_2$ and $x_1$ encoded through the hidden states. If we keep this going, when we want to make a prediction at $y_{100}$, we will be using a hidden state that has encoded information of all the inputs $x_1$ to $x_{100}$. Everything explained so far is a causal RNN, basically to make a prediction of sometime timestep $t$, we can use all the input timesteps $<=t$. We can easily expand this though to make a bidirectional RNN, where to make a prediction at time $t$, we can look at the entire sequence as well. In this case we will really have two hidden states, one that looks backwards and another that looks forward! Whether you use causal or bidirectional depends a lot on what you want to do. If you want to do Name Entity Recognition (i.e. determine if each word in a sentence is an entity), you can look at the entire sentence to do this. On the other hand if you want to forecast the future, like a stock price, then you have to use causal as you can only look at the past to predict the future. \n",
    "\n",
    "All this sounds well and good, but there was one glaring problem: Memory. The hidden states we use to encode the history can only contain so much information, i.e. as the sequence length becomes longer the model will start to forget. This matters a lot for things like Natural Language Processing, as there may be imporant relations between parts of a book that are pages, or even chapters, apart. To solve this issue, Attention Augmented RNNs were introduced in the paper [Neural Machine Translation By Jointly Learning To Align and Translate](https://arxiv.org/pdf/1409.0473). \n",
    "\n",
    "## Attention Augmented RNN\n",
    "\n",
    "If I had to use two words to define attention it would be: **Weighted Average**. In the paper, the call the hidden states *annotations*, but they are the same thing! So lets go back to our RNN again, before we do our prediction for $y_t$, we have a sequence of hidden states $h_t$ that contain the information about the sequence $x_t$ itself produced from the RNN mechanism. The problem is again, $h_t$ for large values of $t$ will have forgotten imporant information about early $x_t$ values with small values of $t$. So what if we got everyone to know each other again? We can produce a context vector $c_i$ that is a weighted average of all the hidden states in the case of a bidirectional architecture, or just the previous hidden states in a causal architecture. This means at any time of the context vector $c_t$, it will be a weighted average of all of the timesteps so it is reminded about more distant timesteps, solving our *memory* problem!\n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/priyammaz/PyTorch-Adventures/main/src/visuals/rnn_with_attention.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "Now I keep saying weighted average, and this is because for one of the timesteps, the model has to learn the weights of what is the most important information to know at those times, and then weight them higher! As per the paper, the weights were learned through an alignment model, which was just a feedforward network, that scores how well hidden states as time $t$ is related to those around it in the sequence. These scores were then passed through a softmax to ensure all the learned weights sum upto 1, and then the context vectors are computed based on them! This means every context vector is a customized weighted average that learned exactly what information to put empahsis on at every timestep of the context vectors. \n",
    "\n",
    "### Problems\n",
    "\n",
    "There were some issues with this though, some which were already known about RNNs:\n",
    "- **Efficient but Slow**: The RNN mechanism has a for-loop through the sequence making training very slow, but inference was efficient\n",
    "- **Lack of Positional Information**: Our context vectors are just weighted averages of hidden, there is no information about position or time, but obviously in most sequence tasks, the order in your data appears is very important\n",
    "- **Redundancy**: We are effectively learning the same thing twice here, the hidden states encode sequential information, but the attention mechanism also encodes sequential information\n",
    "\n",
    "### Attention is All You Need!\n",
    "\n",
    "The groundbreaking paper, [Attention is All You Need](https://arxiv.org/pdf/1706.03762) solved all of the problems above, but added a new one: Computational Cost. Lets first look at what the proposed Attention mechanism is doing!\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/priyammaz/PyTorch-Adventures/main/src/visuals/attention_mechanism_visual.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "The input is a sequence of embedding vectors and the output is a sequence of context vectors. Lets quickly look at the formulation for this:\n",
    "\n",
    "$$\\text{Attention}(Q,K,V) = \\text{Softmax}(\\frac{QK^T}{\\sqrt{d_e}})V$$\n",
    "\n",
    "We see some new notation show up now, $Q$, $K$, $V$, so lets define them:\n",
    "\n",
    "- $Q$: Queries, they are the token we are interested in\n",
    "- $K$: Keys, they are the other tokens we want to compare our query against\n",
    "- $V$: Values, they are the values we will weight in our weighted average\n",
    "\n",
    "This is a little weird so lets step through it! First important note, the $Q$, $K$, and $V$ are three projections of our original data input $X$. This basically means we have three linear layers that all take the same input $X$ to produce our $Q$, $K$, $V$. \n",
    "\n",
    "### Step 1: Compute the Attention Matrix with $Softmax(QK^T)$\n",
    "\n",
    "So the first step is the computing the $Softmax(QK^T)$, where Q and K both have the shape (Sequence Length x Embedding Dimension). The output of this computation will be sequence length x sequence length. This is what it looks like!\n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/priyammaz/PyTorch-Adventures/main/src/visuals/computing_attention.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "In the image above, I also applied the softmax (not shown for simplicity), so each row of the attention matrix adds up to 1 (like probabilities).\n",
    "\n",
    "**Recap: Dot Product**\n",
    "\n",
    "As a quick reminder, this whole mechanism depends on the dot product, and more specifically, its geometric interpretation\n",
    "\n",
    "$a\\cdot b = \\sum_{i=1}^n a_i*b_i = |a||b|cos(\\theta)$\n",
    "\n",
    "What the dot product really signifies is the similarity between vectors. Remember the cosine of 0 is just 1, so the highest possible cosine value would be when the vectors $a$ and $b$ point in the exact same direction. This means vectors that are similar in direction have higher magnitude. \n",
    "\n",
    "**Recap: Matrix Multiplication**\n",
    "\n",
    "Also remember, matrix multiplication is basically just a bunch of dot products, repeating the multiply/add operation repeatedly. If we are multiplying matrix $A$ with matrix $B$, what we are really doing is doing the dot product of every row of $A$ and every column of $B$!\n",
    "\n",
    "So with our quick recaps, lets go back to the image above, when we are multiplying $Q$ by $K^T$, we are multiplying each vector in the sequence $Q$ by each vector in the sequence $K$ and computing their dot product similarity. Again, $Q$ and $K$ are just projections of the original data $X$, so really we are just computing the similarity between every possible combination of timesteps in $X$. We also could have just done $XX^T$, this would technically be the same thing, but by including the projections of $X$ rather than using the the raw inputs themselves, we allow the model to have more learnable parameters so it can futher accentuate similarities and differences between different timesteps!\n",
    "\n",
    "The final result of this operation is the attention matrix, that computes the similarity between every possible pairs of tokens. \n",
    "\n",
    "**Note** I didn't inlude anything about the $\\frac{1}{\\sqrt{d_e}}$ term in the formula. This is just a normalization constant that ensures our variance of the attention matrix isn't too large after our matrix multiplication. This just leads to more stable training!\n",
    "\n",
    "### Step 2: Weighting the Values Matrix\n",
    "\n",
    "Now that we have our similarities of how each timestep is related to all the other timesteps, we can now do our weighted average! After the weighted average computation, each vector for each timestep isn't just the data of the timestep but rather a weighted average of all the vectors in the sequence and how they are related to that timestep of interest. \n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/priyammaz/PyTorch-Adventures/main/src/visuals/encoder_attention_vis.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "The output of this operation gives us the sequence of context vectors!\n",
    "\n",
    "### Enforcing Causality\n",
    "\n",
    "What we have seen so far is the equivalent to a Bidirectional RNN. The weighted average operation we are doing is between a timestep of interest and all timesteps before and after it. If we wanted a causal model, where a context vector only depends on the timesteps before it, then we need to apply a causal mask to our attention mechanism. \n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/priyammaz/PyTorch-Adventures/main/src/visuals/causal_masking.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "As you can see, we apply a mask to all values of $t$ where the index of the column values (our key index) is greater than the index of the row value (our value index). In practice, once we apply this mask to our attention matrix, we can then multiply by our values. You will see that the context vector at time $t$ is only then dependent on previous timesteps, as we make sure future vectors of $V$ are zeroed out!\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/priyammaz/PyTorch-Adventures/main/src/visuals/decoder_attention_vis.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "### Lets Build This!\n",
    "\n",
    "Now that we have everything we need, we can build it! We wont be trainig any models now, just defining and exploring the architecture here. To do so, we will define some data in the form of `Batch x Sequence Length x Embed Dim`. The Embedding dimension is basically, what dimension vector do we want to use to represent a single timestep, and the sequence length is how many timesteps there are in total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9fdeb84-6b8b-48a3-99bd-2e8d041505cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Input is: torch.Size([4, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "### Lets Define some Random Data ###\n",
    "batch_size = 4\n",
    "sequence_length = 64\n",
    "embed_dim = 128\n",
    "\n",
    "x = torch.randn(batch_size, sequence_length, embed_dim)\n",
    "print(\"Shape of Input is:\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a871a9-c286-4651-8def-11a03830254a",
   "metadata": {},
   "source": [
    "### Implement Attention Without Any Learnable Parameters $\\text{Softmax}(\\frac{XX^T}{\\sqrt{d_e}})X$\n",
    "\n",
    "This whole attention operation is again very flexible and there is technically no reason to have any learnable parameters in its formulation (other than the obvious for wanting better predictive performance). So lets quickly just implement the formula as is using raw inputs $X$ rather than doing any learned projections of the data. \n",
    "\n",
    "**Step 1**\n",
    "\n",
    "Compute $XX^T$ which will provide the similarity score between every pair of vectors in $X$. This will be contained inside a `batch x sequence_length x sequence_length` matrix\n",
    "\n",
    "**Step 2**\n",
    "\n",
    "After computing the similarity score, we can check and see that the variance of the similarity matrix is extremely high, this is the main reason for the normalization of dividing by the square root of the embedding dimension. In the end, the similarity scores are passed through a softmax to compute a probability vector, dividing by a constant basically acts as a temperature parameter to cool the distribution and provide more stable training!\n",
    "\n",
    "**Step 3**\n",
    "\n",
    "Each row of our `sequence_length x sequence_length` matrix is the similarity of how one timestep is related to all other timesteps! What we want to do is, instead of raw similarity scores, we will convert them to probabilities, so when we do the weighted average on our values matrix, the weights add up to 1! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20e96766-4daf-4e2b-9b92-bc68d24bb7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prenormalization Variance: tensor(367.6834)\n",
      "Normed Similarity Variance: tensor(2.8725)\n",
      "Shape of Normed Similarity: torch.Size([4, 64, 64])\n",
      "Everything Equal to One: True\n",
      "Output Shape: torch.Size([4, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "### First compute XX^T for similarity score between every pair of tokens ###\n",
    "similarity = (x @ x.transpose(1,2))\n",
    "\n",
    "### Normalize the Similarity Scores ###\n",
    "print(\"Prenormalization Variance:\", similarity.var())\n",
    "similarity_norm = similarity / (embed_dim**0.5)\n",
    "print(\"Normed Similarity Variance:\", similarity_norm.var())\n",
    "\n",
    "### Check the Shape of our Similarity Tensor ###\n",
    "print(\"Shape of Normed Similarity:\", similarity_norm.shape)\n",
    "\n",
    "### Compute similarity on every row of the attention matrix (i.e along the last dimension) ###\n",
    "attention_mat = similarity_norm.softmax(dim=-1)\n",
    "\n",
    "### Verify each row adds up to 1 ###\n",
    "summed_attention_mat = attention_mat.sum(axis=-1)\n",
    "print(\"Everything Equal to One:\", torch.allclose(summed_attention_mat, torch.ones_like(summed_attention_mat)))\n",
    "\n",
    "### Multiply our Attention Matrix against its Values (X in our case) for our Weighted Average ###\n",
    "context_vectors = attention_mat @ x\n",
    "\n",
    "print(\"Output Shape:\", context_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c278c93a-aaf0-44a3-b4bb-4c57e0957c25",
   "metadata": {},
   "source": [
    "Thats it! This is basically all the attention computation is doing mathematically, we will add in the learnable parameters in just a bit. Something important to bring to your attention is the input shape of $x$ and our output shape of the context vectors are identical. Again, the input is the raw data, the output is weighted averaged of how every token is realted to all the other ones. But the shapes not changing is quite convenient, and allows us to stack together a bunch attention mechanisms on top of one another! \n",
    "\n",
    "### Lets Add Learnable Parameters \n",
    "\n",
    "This time, instead of using $X$ as our input, we will create our three projections of $X$ (Queries, Keys and Values), but then repeat the operation we just did here! Also for convenience, I will wrap it all in a PyTorch class so we can continue adding stuff onto it as we go on!\n",
    "\n",
    "Now what are these projections exactly? The are pointwise (or per timestep) projections! Remember, in our example here, each timestep is encoded by a vector of size 128. We will create three learnable weight matricies, incorporated inside the Linear modules in PyTorch, that take these 128 numbers per timestep and projects them to another 128 numbers (it is typical to keep the embedding dimension the same). This is a per timestep operation not across timesteps operation (across timesteps occurs within the attention computation). Obviously, PyTorch will accelerate this per timestep operation by doing it in parallel, but regardless, different timesteps dont get to see each other in the projection step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a799aaa-df8b-4664-8b7a-02a9d8c97042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, embedding_dimension):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embedding_dimension \n",
    "        \n",
    "        ### Create Pointwise Projections ###\n",
    "        self.query = nn.Linear(embedding_dimension, embedding_dimension)\n",
    "        self.key = nn.Linear(embedding_dimension, embedding_dimension)\n",
    "        self.value = nn.Linear(embedding_dimension, embedding_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        ### Create Queries, Keys and Values from X ###\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        ### Do the same Computation from above, just with our QKV Matricies instead of X ###\n",
    "        similarity = (q @ k.transpose(1,2)) / (self.embed_dim ** 0.5)\n",
    "        attention  = similarity.softmax(axis=-1)\n",
    "        output = attention @ v\n",
    "\n",
    "        return output\n",
    "\n",
    "attention = Attention(embedding_dimension=128)\n",
    "output = attention(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a0db20e-1435-4b7f-a25c-8b118510d8b7",
   "metadata": {},
   "source": [
    "### MultiHeaded Attention\n",
    "\n",
    "Now we have a small problem! Remember, the Attention matrix encodes the similarity between each pair of timesteps in your sequence. But in many cases, language being a prime example, there can be different types of relationships between different pairs of words, but our attention computation is restricted to only learn one of them. The solution to this is **MultiHeaded Attention**. Inside each attention computation, what if we have 2 attention matricies, or 8 or however many we want! The more we have the larger diversity of relationships we can learn!\n",
    "\n",
    "#### Single Headed Attention Recap\n",
    "\n",
    "Lets summarize everything we have seen so far with a single visual, and we will call this a Single Head of attention. We will also have our embedding dimension for each word in the sequence be 9, and the sequence length is 8. \n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/priyammaz/PyTorch-Adventures/main/src/visuals/single_headed_attention_visual.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "This is again called single headed attention because we only compute a single attention matrix following the logic above! \n",
    "\n",
    "#### Moving to MultiHeaded Attention\n",
    "\n",
    "For multiheaded attention there isn't really a lot changing. Remember, to create our $Q$, $K$, $V$ in single headed attention, we have 3 linear projection layers that take in the embedding dimension and output the same embedding dimension (in our case it takes in 9 and outputs 9). But in multiheaded attention, we can actually reduce our embedding dimension to a smaller value, do the attention computation on the tokens with this condensed embedding dimension, repeat it a bunch of times, and then concatenate together the outputs. \n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/priyammaz/PyTorch-Adventures/main/src/visuals/multiheaded_attention_visual.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "It is general practice to have the number of heads you pick to be a divisor of the embedding dimension. For example, in our case, our original embedding dimension is 9, so we can pick 3 heads because 9 is divisible by 3. This also means our head dimension would now be 3, because 9/3 = 3. In typical transformers, the embedding dimension is 768, and they typically have 12 heads of attention. This means each head of attention will have a dimension of 64 because 768/12 = 64. \n",
    "\n",
    "The main reason we want it to evenly divide is because we have three heads, each takes in an embedding dimension of 9 and compresses to 3 before computing attention, and then outputs a tensor of embedding size 3. We can then take our 3 tensors, each having an embedding dimension of 3, concatenate them together, returning us back to the 9 that we began with! Again, this is just for convenience, so the embedding dimension of the input and output tensor dont change in any way. Last problem is, each head of attention is computed individually, so the final concatenated tensor has a bunch of heads of attention packed together, but we never got to share information between the different heads of attention. This is why we have the final head projection, that will take in the embedding dimension of 9 from our concatenated tensor, and output an embedding dimension of 9, therefore meshing information across the heads of our embedding dimension.\n",
    "\n",
    "Lets go ahead and build this module as shown in the figure above! Basically, each head will have 3 projection layers for Q,K,V, we will perform the attention computation, and then stick all the results back together at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c43e927-93c4-4e76-98bc-488cb899ebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dimension, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        ### Make sure Embedding Dimension is Divisible by Num Heads ###\n",
    "        assert embedding_dimension % num_heads == 0, f\"Make sure your embed_dim {embedding_dimension} is divisible by the number of heads {num_heads}\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        ### Compute Head Dimension ###\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "\n",
    "\n",
    "        ### Create a List of Lists which has all our Q,K,V projections for each head ###\n",
    "        self.multihead_qkv = nn.ModuleList()\n",
    "\n",
    "        ### For head Head create the QKV ###\n",
    "        for head in range(self.num_heads):\n",
    "\n",
    "            ### Create a dictionary of the 3 projection  layers we need ###\n",
    "            qkv_proj = nn.ModuleDict(\n",
    "                [\n",
    "                    [\"Q\", nn.Linear(self.embed_dim, self.head_dim)],\n",
    "                    [\"K\", nn.Linear(self.embed_dim, self.head_dim)],\n",
    "                    [\"V\", nn.Linear(self.embed_dim, self.head_dim)],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            ### Store Dictionary in List ###\n",
    "            self.multihead_qkv.append(qkv_proj)\n",
    "\n",
    "        ### Create final Projection layer, it will be applied to the concatenated heads will have shape embed_dim again ###\n",
    "        self.head_mesh = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        ### Create a list ot store each heads output ###\n",
    "        head_outs = []\n",
    "        \n",
    "        ### Loop Through Each head of Attention ###\n",
    "        for head in self.multihead_qkv:\n",
    "\n",
    "            ### Access layers like a dictionary (ModuleDict) ###\n",
    "            ### q,k,v will be (Batch x Seq len x head_dim)\n",
    "            q = head[\"Q\"](x)\n",
    "            k = head[\"K\"](x)\n",
    "            v = head[\"V\"](x)\n",
    "\n",
    "            ### Now do the same Attention computation as before! ###\n",
    "            similarity = (q @ k.transpose(1,2)) / (self.embed_dim ** 0.5)\n",
    "            attention  = similarity.softmax(axis=-1)\n",
    "            output = attention @ v\n",
    "\n",
    "            ### Store this output in the head_outs ###\n",
    "            head_outs.append(output)\n",
    "\n",
    "        ### head_outs has num_heads tensors, each with the compressed embedding dimension of head_dim ###\n",
    "        ### We can concatenate them all back together along the embedding dimension just like we did in the image above ###\n",
    "        head_outs = torch.cat(head_outs, dim=-1)\n",
    "\n",
    "        ### head_outs will have the same shape now as our input x! ###\n",
    "        if head_outs.shape != x.shape:\n",
    "            raise Exception(\"Something has gone wrong in the attention computation\")\n",
    "\n",
    "        ### Now each head was computed independently, we need them to get to know each other, so pass our head_outs through final projection ### \n",
    "        output = self.head_mesh(head_outs)\n",
    "\n",
    "        return output\n",
    "        \n",
    "\n",
    "embed_dim = 9\n",
    "num_heads = 3\n",
    "seq_len = 8\n",
    "mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "\n",
    "### Create a random tensor in the shape (Batch x Seq Len x Embed Dim) ###\n",
    "rand = torch.randn(3,seq_len,embed_dim)\n",
    "\n",
    "### Pass through MHA ###\n",
    "output = mha(rand)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3456839f-cd5f-41f8-890b-5f046c6f4f73",
   "metadata": {},
   "source": [
    "### Increasing Efficiency \n",
    "\n",
    "We now have a successful Multihead Attention layer!! This basically has all the same math and lodgic of attention, except for one small issue: efficiency. Typically we want to avoid for loops as much as possible in our PyTorch code, being able to vectorize and do things in parallel will make much better use of the GPUs we train on. To make this more efficient though, theres something we need to understand first: PyTorch Linear layers on multidimensional tensors!\n",
    "\n",
    "#### Linear Layers on MultiDimensional Tensors\n",
    "\n",
    "We have already seen `nn.Linear(input_dim, output_dim)` many times already, and this module expects a tensor of shape `[Batch x input_dim]` and it will output `[Batch x output_dim]`. But what if our input is `[Batch x Dim1 x Dim2 x input_dim]`, then what happens? Basically, PyTorch will automatically flatten all the dimensions other than the last one automagically, do the linear layer, and then return back to the expected shape, so we would get an output of `[Batch x Dim1 x Dim2 x output_dim]`. Another way of thinking about this is, PyTorch linear layers only are applied to the last dimension of your tensor. Lets do a quick example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae2da053-e2df-4ea5-8f09-f5ec8079911b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([5, 10]) Output Shape: torch.Size([5, 30])\n",
      "Input Shape: torch.Size([5, 1, 2, 3, 4, 10]) Output Shape: torch.Size([5, 1, 2, 3, 4, 30])\n"
     ]
    }
   ],
   "source": [
    "fc = nn.Linear(10,30)\n",
    "\n",
    "tensor_1 = torch.randn(5,10)\n",
    "tensor_1_out = fc(tensor_1)\n",
    "print(\"Input Shape:\", tensor_1.shape, \"Output Shape:\", tensor_1_out.shape)\n",
    "\n",
    "tensor_2 = torch.randn(5,1,2,3,4,10)\n",
    "tensor_2_out = fc(tensor_2)\n",
    "print(\"Input Shape:\", tensor_2.shape, \"Output Shape:\", tensor_2_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade2d400-cc29-49ec-ac7e-52102a63f143",
   "metadata": {},
   "source": [
    "### Packing Linear Layers\n",
    "\n",
    "Another important idea is packing our linear layers together. Lets think about out example again, each projection for Q, K and V have a Linear layer that takes in 9 values and outputs 3 values, and we repeat this 3 times for each head. Lets just think about our Queries for now.\n",
    "\n",
    "- Query for Head 1: Take in input x with embedding dim 9 and outputs tensor with embedding dimension 3\n",
    "- Query for Head 2: Take in input x with embedding dim 9 and outputs tensor with embedding dimension 3\n",
    "- Query for Head 3: Take in input x with embedding dim 9 and outputs tensor with embedding dimension 3\n",
    "\n",
    "Well what if we reframed this? What if we had a single linear layer that take input x with embedding dim 9 and outputs something with embedding dim 9. Afterwards, we can cut the matrix into our three heads of attention. Lets do a quick example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a17f31e2-529c-460d-b5ef-1dc397f0de00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all Queries: torch.Size([1, 8, 9])\n",
      "Shape of each Head of Query: torch.Size([1, 8, 3])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn(1,8,9)\n",
    "fc = nn.Linear(9,9)\n",
    "\n",
    "### Pass tensor through layer to make Queries ###\n",
    "q = fc(tensor)\n",
    "print(\"Shape of all Queries:\", q.shape)\n",
    "\n",
    "### Cut Embedding dimension into 3 heads ###\n",
    "q_head1, q_head2, q_head3 = torch.chunk(q, 3, axis=-1)\n",
    "print(\"Shape of each Head of Query:\", q_head1.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc981d3e-bfaf-4587-97b4-260297ad7709",
   "metadata": {},
   "source": [
    "### MultiDimensional Matrix Multiplication\n",
    "\n",
    "So, we have composed our 9 linear layers (3 heads have 3 projections for Q,k,V each) into just 3 linear layers, where we have packed all the heads into them. But after we chunk up our Q,K,V tensors each into three more tensors for each head we will still need to do the looping operation to go through the cooresponding q,k,v matricies. Can we parallelize this too? Of course! We just need to better understand higher dimensional matrix multiplication. \n",
    "\n",
    "#### Recap:\n",
    "\n",
    "Matrix multiplication is typicall seen like this, multiplying an `[AxB]` matrix by a `[BxC]` which will produce a `[AxC]` matrix. But what if we have a `[Batch x dim1 x A x B]` multiplied by a `[Batch x dim1 x B x C]`. Matrix multiplication again only happens on the last two dimensions, so because our first tensor ends with an `[AxB]` and the second tensor ends with a `[BxC]`, the resulting matrix multiplication will be `[Batch x dim1 x A x C`]`. Lets see a quick example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2602a21-3431-4594-b414-9203d60366b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output Shape: torch.Size([1, 2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,2,6,4)\n",
    "b = torch.randn(1,2,4,3)\n",
    "print(\"Final Output Shape:\", (a@b).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2577fd-7cd5-4700-b807-fb872cc86153",
   "metadata": {},
   "source": [
    "### The Trick of Parallelizing Heads\n",
    "\n",
    "Now for the trick of parallelizing our heads by using everything we have just seen! All we need to do is split the embedding dimension up and move the heads out of the way so the computation can occur. Remember, we have our $Q$, $K$, and $V$ matricies right now that each contain all the projected heads and are in the shape `[Batch x Seq_len x Embed_dim]` ([batch x 8 x 9] in our case). \n",
    "\n",
    "- Step 1: Split the embedding dimension into the number of heads and head dim. We already know that our embedding dimension is divisible as thats how we set it, so we can do `[Batch x Seq_len x Embed_dim]` -> `[Batch x Seq_len x Num_Heads x Embed_Dim]`. (This would be taking our [batch x 8 x 9] and converting to [batch x 8 x 3 x 3])\n",
    "- The attention computation has to happen between two matricies of shape `[Seq_len x Embed_Dim]` for queries and `[Embed_Dim x Seq_len]` for our transposed keys. In the case of multihead attention, the matrix multiplication happens across the Head Dimension rather than embedding. If our Queries, Keys and Values are in the shape `[Batch x Seq_len x Num_Heads x Embed_Dim]`, we can just transpose the Seq_len and Num_heads dimensions and make a tensor of shape `[Batch x Num_Heads x Seq_len x Embed_Dim]`. This way when I do Queries multiplied by the Transpose of Keys I will be doing `[Batch x Num_Heads x Seq_len x Embed_Dim]` multiplied by `[Batch x Num_Heads x Embed Dim x Seq Len]` creating the attention matrix `[Batch x Num_Heads x Seq_len x Seq Len]`. Therefore we have effectively created for every sample in the batch, and for every head of attention, a unique attention matrix! Thus we have parallelized the Attention Matrix computation.\n",
    "- Now that we have out all our attention matricies `[Batch x Num_Heads x Seq_len x Seq Len]`, we can perform our scaling by our constant, and perform softmax across every row of each attention matrix (along the last dimension).\n",
    "- The last step is to multiply out attention matrix `[Batch x Num_Heads x Seq_len x Seq Len]` by the Values which is in the shape `[Batch x Seq_len x Num_Heads x Embed_Dim]`, which will get us to `[Batch x Num_Heads x Seq_len x Embed_Dim]`!\n",
    "- Lastly, we need to put our Num Heads and Embedding dimensions back together, so we can permute the Num Heads and Seq Len dimensions again which gives us `[Batch x Seq_len x Num_Heads x Embed_Dim]` and flatten on the last two dimension finally giving us `[Batch x Seq_len x Num_Heads x Embed_Dim]`.\n",
    "- This flattening operation is equivalent to concatenation of all the heads of attention, so we can pass this through our final projection layer so all the heads of attention gets to know each other!\n",
    "\n",
    "### Lets build Our Final Attention Implementation!\n",
    "\n",
    "We will also include some extra dropout layers typically added to attention computations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3651f151-62b9-4a5b-a4bb-2fc21fe1247f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output: torch.Size([3, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "class SelfAttentionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Self Attention Proposed in `Attention is All  You Need` - https://arxiv.org/abs/1706.03762\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "               embed_dim=768,\n",
    "               num_heads=12, \n",
    "               attn_p=0,\n",
    "               proj_p=0):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            embed_dim: Transformer Embedding Dimension\n",
    "            num_heads: Number of heads of computation for Attention \n",
    "            attn_p: Probability for Dropout2d on Attention cube\n",
    "            proj_p: Probability for Dropout on final Projection\n",
    "        \"\"\"\n",
    "        \n",
    "        super(SelfAttentionEncoder, self).__init__()\n",
    "\n",
    "        ### Make Sure Embed Dim is Divisible by Num Heads ###\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = int(embed_dim / num_heads)\n",
    "        \n",
    "        ### Define all our Projections ###\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "\n",
    "        ### Define Post Attention Projection ###\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch, seq_len, embed_dim = x.shape\n",
    "\n",
    "        ### Compute Q, K, V Projections,and Reshape/Permute to [Batch x Num Heads x Seq Len x Head Dim] \n",
    "        q = self.q_proj(x).reshape(batch, seq_len, self.num_heads, self.head_dim).transpose(1,2).contiguous()\n",
    "        k = self.k_proj(x).reshape(batch, seq_len, self.num_heads, self.head_dim).transpose(1,2).contiguous()\n",
    "        v = self.v_proj(x).reshape(batch, seq_len, self.num_heads, self.head_dim).transpose(1,2).contiguous()\n",
    "        \n",
    "        ### Perform Attention Computation ###\n",
    "        attn = (q @ k.transpose(-2,-1)) * (self.head_dim ** -0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = attn @ v\n",
    "        \n",
    "        ### Bring Back to [Batch x Seq Len x Embed Dim] ###\n",
    "        x = x.transpose(1,2).reshape(batch, seq_len, embed_dim)\n",
    "\n",
    "        ### Pass through Projection so Heads get to know each other ###\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "embed_dim = 9\n",
    "num_heads = 3\n",
    "seq_len = 8\n",
    "a = SelfAttentionEncoder(embed_dim, num_heads)\n",
    "\n",
    "### Create a random tensor in the shape (Batch x Seq Len x Embed Dim) ###\n",
    "rand = torch.randn(3,seq_len,embed_dim)\n",
    "\n",
    "### Pass through MHA ###\n",
    "output = a(rand)\n",
    "print(\"Final Output:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11677c0-9530-4495-8208-938f705f6299",
   "metadata": {},
   "source": [
    "### Enforcing Causality\n",
    "\n",
    "Everything we have done so far is an encoding transformer, which is eqivalent to a bidirectional RNN where a single timestep can look forwards and backwards. To enforce causality, we can only look backwards, so we have to add in a causal mask! How the causal mask looks is the following:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/priyammaz/PyTorch-Adventures/main/src/visuals/causal_masking.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "Basically, every Query vector can only attend to the Key vectors that are at the same timestep or before! Now how do we actually do this? Unfortunately its not as easy as just changing the numbers in our attention mask, because we still need every row of the attention mask to add to one (like a probability vector!). We want something like this though:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/priyammaz/PyTorch-Adventures/main/src/visuals/decoder_attention_vis.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "As we can see, because we have zeroed out the attention weights on future keys, when we multiply by values, we multiply by 0. Therefore our weighted average context vector of a timestep only is computed at that timestep and previous, never the future!\n",
    "\n",
    "\n",
    "### Computing the Reweighted Causal Attention Mask\n",
    "\n",
    "Lets pretend the raw outputs of $QK^T$, before the softmax, is below:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "  7       & -8   & 6  \\\\\n",
    "  -3       & 2   & 4   \\\\\n",
    "  1       & 6  & -2   \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Remember, the equation for softmax is:\n",
    "\n",
    "$$\\text{Softmax}(\\vec{x}) = \\frac{e^{x_i}}{\\sum_{j=1}^N{e^{x_j}}}$$\n",
    "\n",
    "Then, we can compute softmax for row of the matrix above:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Softmax}\n",
    "\\begin{bmatrix}\n",
    "  7       & -8   & 6  \\\\\n",
    "  -3       & 2   & 4   \\\\\n",
    "  1       & 6  & -2   \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "  \\frac{e^{7}}{e^{7}+e^{-8}+e^{6}}       & \\frac{e^{-8}}{e^{7}+e^{-8}+e^{6}}   & \\frac{e^{6}}{e^{7}+e^{-8}+e^{6}}  \\\\\n",
    "  \\frac{e^{-3}}{e^{-3}+e^{2}+e^{4}}       & \\frac{e^{2}}{e^{-3}+e^{2}+e^{4}}   & \\frac{e^{4}}{e^{-3}+e^{2}+e^{4}}  \\\\\n",
    "  \\frac{e^{1}}{e^{1}+e^{6}+e^{-2}}       & \\frac{e^{6}}{e^{1}+e^{6}+e^{-2}}   & \\frac{e^{-2}}{e^{1}+e^{6}+e^{-2}}  \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "  0.73       & 0.0000002   & 0.27   \\\\\n",
    "  0.0008       & 0.12   & 0.88 \\\\\n",
    "  0.007       & 0.99  & 0.003  \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "But, what we want, is the top triangle to have weights of 0, and the rest adding up to 1. So lets take the second vector in the matrix above to see how we can do that. \n",
    "\n",
    "$$x_2 = [-3, 2, 4]$$\n",
    "\n",
    "Because this is the second vector, we need to zero out the softmax output for everything after the second index (so in our case just the last value). Lets replace the value 4 by $-\\infty$. Then we can write it as:\n",
    "\n",
    "$$x_2 = [-3, 2, -\\infty]$$\n",
    "\n",
    "Lets now take softmax of this vector!\n",
    "\n",
    "$$\\text{Softmax}(x_2) = [\\frac{e^{-3}}{e^{-3}+e^{2}+e^{-\\infty}}, \\frac{e^{2}}{e^{-3}+e^{2}+e^{-\\infty}}, \\frac{e^{-\\infty}}{e^{-3}+e^{2}+e^{-\\infty}}]$$\n",
    "\n",
    "Remember, $e^{-\\infty}$ is equal to 0, so we can solve solve this!\n",
    "\n",
    "$$\\text{Softmax}(x_2) = [\\frac{e^{-3}}{e^{-3}+e^{2}+0}, \\frac{e^{2}}{e^{-3}+e^{2}+0}, \\frac{0}{e^{-3}+e^{2}+0}] = [\\frac{e^{-3}}{e^{-3}+e^{2}+0}, \\frac{e^{2}}{e^{-3}+e^{2}+0}, \\frac{0}{e^{-3}+e^{2}+0}] = [0.0067, 0.9933, 0.0000]$$\n",
    "\n",
    "So we have exactly what we want! The attention weight of the last value is set to 0, so when we are on the second vector $x_2$, we cannot look forward to the future value vectors $v_3$, and the remaining parts add up to 1 so its still a probability vector! To do this correctly for the entire matrix, we can just substitute in the top triangle of $QK^T$ with $-\\infty$. This would look like:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "  7       & -\\infty   & -\\infty  \\\\\n",
    "  -3       & 2   & -\\infty   \\\\\n",
    "  1       & 6  & -2   \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Taking the softmax of the rows of this matrix then gives:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Softmax}\n",
    "\\begin{bmatrix}\n",
    "  7       & -\\infty   & -\\infty  \\\\\n",
    "  -3       & 2   & -\\infty   \\\\\n",
    "  1       & 6  & -2   \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "  1       & 0   & 0  \\\\\n",
    "  0.0067  & 0.9933 & 0   \\\\\n",
    "  0.007       & 0.99  & 0.003   \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, the best way to apply out attention mask is by filling the top right triangle with $-\\inf$ and then take the softmax! So lets go ahead and add an option for causality for our attention function we wrote above!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "408e1226-8c45-4e20-a0d0-121ddf3cc3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output: torch.Size([3, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self Attention Proposed in `Attention is All  You Need` - https://arxiv.org/abs/1706.03762\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "               embed_dim=768,\n",
    "               num_heads=12, \n",
    "               attn_p=0,\n",
    "               proj_p=0,\n",
    "               causal=True, \n",
    "               seq_len=512):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            embed_dim: Transformer Embedding Dimension\n",
    "            num_heads: Number of heads of computation for Attention \n",
    "            attn_p: Probability for Dropout2d on Attention cube\n",
    "            proj_p: Probability for Dropout on final Projection\n",
    "            causal: Do you want to apply a causal mask?\n",
    "            seq_len: What is the max sequence length this model can expect?\n",
    "        \"\"\"\n",
    "        \n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        ### Make Sure Embed Dim is Divisible by Num Heads ###\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = int(embed_dim / num_heads)\n",
    "        self.causal = causal\n",
    "        \n",
    "        ### Define all our Projections ###\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "\n",
    "        ### Define Post Attention Projection ###\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "        ### Create NonLearnable Buffer to Act as our Causal Mask ###\n",
    "        if causal:\n",
    "\n",
    "            ### Create a Seq_Len x Seq_Len tensor full of Ones\n",
    "            ones = torch.ones((seq_len, seq_len))\n",
    "\n",
    "            ### Fill Top right triangle with Zeros (as we dont want to attend to them) ###\n",
    "            causal_mask = torch.tril(ones) \n",
    "\n",
    "            ### Add extra dimensions for Batch size and Number of Heads ###\n",
    "            causal_mask = causal_mask.reshape(1,1,seq_len,seq_len).bool()\n",
    "\n",
    "            ### Store as a Buffer, as these parameters dont need to be learned ###\n",
    "            self.register_buffer(\"causal_mask\", causal_mask.to(torch.bool))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch, seq_len, embed_dim = x.shape\n",
    "\n",
    "        ### Compute Q, K, V Projections,and Reshape/Permute to [Batch x Num Heads x Seq Len x Head Dim] \n",
    "        q = self.q_proj(x).reshape(batch, seq_len, self.num_heads, self.head_dim).transpose(1,2).contiguous()\n",
    "        k = self.k_proj(x).reshape(batch, seq_len, self.num_heads, self.head_dim).transpose(1,2).contiguous()\n",
    "        v = self.v_proj(x).reshape(batch, seq_len, self.num_heads, self.head_dim).transpose(1,2).contiguous()\n",
    "        \n",
    "        ### Perform Attention Computation ###\n",
    "        attn = (q @ k.transpose(-2,-1)) * (self.head_dim ** -0.5)\n",
    "\n",
    "        if self.causal:\n",
    "            ####################################################################################\n",
    "            ### FILL ATTENTION MASK WITH -Infinity ###\n",
    "            attn = attn.masked_fill(self.causal_mask[:,:,:seq_len,:seq_len] == 0, float('-inf'))\n",
    "            ####################################################################################\n",
    "    \n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = attn @ v\n",
    "        \n",
    "        ### Bring Back to [Batch x Seq Len x Embed Dim] ###\n",
    "        x = x.transpose(1,2).reshape(batch, seq_len, embed_dim)\n",
    "\n",
    "        ### Pass through Projection so Heads get to know each other ###\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "embed_dim = 9\n",
    "num_heads = 3\n",
    "seq_len = 8\n",
    "a = SelfAttention(embed_dim, num_heads, causal=True)\n",
    "\n",
    "### Create a random tensor in the shape (Batch x Seq Len x Embed Dim) ###\n",
    "rand = torch.randn(3,seq_len,embed_dim)\n",
    "\n",
    "### Pass through MHA ###\n",
    "output = a(rand)\n",
    "print(\"Final Output:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08dbae9-20b6-4f4f-8b66-8782b3fabcb1",
   "metadata": {},
   "source": [
    "### Thats It!\n",
    "\n",
    "That is basically everything you need to know about attention! Now if you are actually going to be using attention in your model, it is better to use optimized cuda implementations rather than this only for speed and efficiency reasons, but they are doing exactly the same thing underneath the hood, just faster! Some examples of this are [SDPA](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) or [FlashAttention](https://github.com/Dao-AILab/flash-attention) which are hardware aware and make better use of the GPUs to do these operations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
