{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4585a71c-f245-4cbf-a115-d14e8374a90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dimensions:  8 number of actions:  4\n",
      "input dimensions:  8 number of actions:  4\n",
      "Episode:  0 \tScore:  -363.8836424586237 \tAverage Score: -363.884 Epsilon 0.992\n",
      "Episode:  1 \tScore:  -193.12331037097175 \tAverage Score: -278.503 Epsilon 0.959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78813/3422936964.py:81: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  state = T.tensor([observation]).to(self.DQN.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  2 \tScore:  -324.56298890098765 \tAverage Score: -293.857 Epsilon 0.901\n",
      "Episode:  3 \tScore:  -72.87556319854284 \tAverage Score: -238.611 Epsilon 0.829\n",
      "Episode:  4 \tScore:  -90.06653569081305 \tAverage Score: -208.902 Epsilon 0.795\n",
      "Episode:  5 \tScore:  -177.18244207438437 \tAverage Score: -203.616 Epsilon 0.739\n",
      "Episode:  6 \tScore:  -145.63593902882235 \tAverage Score: -195.333 Epsilon 0.686\n",
      "Episode:  7 \tScore:  -142.49795237802894 \tAverage Score: -188.729 Epsilon 0.632\n",
      "Episode:  8 \tScore:  -299.2153661786929 \tAverage Score: -201.005 Epsilon 0.577\n",
      "Episode:  9 \tScore:  -92.62250281041575 \tAverage Score: -190.167 Epsilon 0.491\n",
      "Episode:  10 \tScore:  -307.3231978759927 \tAverage Score: -200.817 Epsilon 0.441\n",
      "Episode:  11 \tScore:  -461.6839656964798 \tAverage Score: -222.556 Epsilon 0.050\n",
      "Episode:  12 \tScore:  -633.35093926589 \tAverage Score: -254.156 Epsilon 0.050\n",
      "Episode:  13 \tScore:  -533.2595301457407 \tAverage Score: -274.092 Epsilon 0.050\n",
      "Episode:  14 \tScore:  -498.3571606353604 \tAverage Score: -289.043 Epsilon 0.050\n",
      "Episode:  15 \tScore:  20.756850467175965 \tAverage Score: -269.680 Epsilon 0.050\n",
      "Episode:  16 \tScore:  -87.85470396489376 \tAverage Score: -258.985 Epsilon 0.050\n",
      "Episode:  17 \tScore:  12.130200190194302 \tAverage Score: -243.923 Epsilon 0.050\n",
      "Episode:  18 \tScore:  31.98990553599839 \tAverage Score: -229.401 Epsilon 0.050\n",
      "Episode:  19 \tScore:  -627.9585740253035 \tAverage Score: -249.329 Epsilon 0.050\n",
      "Episode:  20 \tScore:  -56.34463619214995 \tAverage Score: -240.139 Epsilon 0.050\n",
      "Episode:  21 \tScore:  224.69570232728563 \tAverage Score: -219.010 Epsilon 0.050\n",
      "Episode:  22 \tScore:  -113.71456539083081 \tAverage Score: -214.432 Epsilon 0.050\n",
      "Episode:  23 \tScore:  -114.10392040029097 \tAverage Score: -210.252 Epsilon 0.050\n",
      "Episode:  24 \tScore:  -117.56750925274564 \tAverage Score: -206.544 Epsilon 0.050\n",
      "Episode:  25 \tScore:  -78.97933770839327 \tAverage Score: -201.638 Epsilon 0.050\n",
      "Episode:  26 \tScore:  -150.39882518833687 \tAverage Score: -199.740 Epsilon 0.050\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Double Deep Q \n",
    "class DQN(nn.Module):\n",
    "    def __init__(self,learning_rate,input_dims,fc1_dims,fc2_dims,num_actions):\n",
    "        super(DQN,self).__init__()\n",
    "        \n",
    "        print(\"input dimensions: \",input_dims[0],\"number of actions: \",num_actions)\n",
    "        \n",
    "        # Three fully connected layers\n",
    "        self.fc1 = nn.Linear(*input_dims,fc1_dims) # * unpacks the list of input_dims since it's 2-dimensional (same as input_dims[0])\n",
    "        self.fc2 = nn.Linear(fc1_dims,fc2_dims)\n",
    "        self.fc3 = nn.Linear(fc2_dims,num_actions)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=learning_rate)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "        # Set device\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu:0')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# Agent \n",
    "class Agent:\n",
    "    def __init__(self,discount,epsilon,learning_rate,input_dims,batch_size,num_actions,mem_size=500000,min_epsilon=0.05,epsilon_decay=0.0005):\n",
    "        # Brain of the agent\n",
    "        self.DQN = DQN(learning_rate,input_dims,256,256,num_actions)\n",
    "        self.DQN_next = DQN(learning_rate,input_dims,256,256,num_actions)\n",
    "        self.DQN_next.load_state_dict(self.DQN.state_dict()) # loads a model's parameter dictionary using a deserialized state_dict\n",
    "        \n",
    "        # Hyper-parameters\n",
    "        self.discount = discount\n",
    "        self.epsilon = epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.action_space = [i for i in range(num_actions)]\n",
    "        self.mem_size = mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_counter = 0\n",
    "        \n",
    "        # Initialize memory \n",
    "        self.state_mem = np.zeros((mem_size,*input_dims),dtype=np.float32) # mem_size number of dim[0]*dim[1] matrices of zeros\n",
    "        self.new_state_mem = np.zeros((mem_size,*input_dims),dtype=np.float32)\n",
    "        self.action_mem = np.zeros(mem_size,dtype=np.int32)\n",
    "        self.reward_mem = np.zeros(mem_size,dtype=np.float32)\n",
    "        self.terminal_mem = np.zeros(mem_size,dtype=bool) # boolean value indicating whether it's the last memory\n",
    "        \n",
    "    # Store records inside the memory\n",
    "    def storage(self,state,new_state,action,reward,terminal):\n",
    "        index = self.mem_counter%self.mem_size # equal to mem_counter if mem_counter < mem_size\n",
    "        self.state_mem[index] = state\n",
    "        self.new_state_mem[index] = new_state\n",
    "        self.reward_mem[index] = reward\n",
    "        self.action_mem[index] = action\n",
    "        self.terminal_mem[index] = terminal\n",
    "        \n",
    "        self.mem_counter += 1\n",
    "        \n",
    "    # Take actions\n",
    "    def get_action(self,observation):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Randomly choose one action from the action space\n",
    "            action = np.random.choice(self.action_space)\n",
    "        else:\n",
    "            state = T.tensor([observation]).to(self.DQN.device)\n",
    "            actions = self.DQN(state.float()) # same as self.to(torch.float32)\n",
    "            action = T.argmax(actions).item() # the action corresponding to the index of the maximizing action in the actions tensor\n",
    "        return action\n",
    "    \n",
    "    # Learning\n",
    "    def learn(self):\n",
    "        # Check memory counter against batch size since the latter comes from the former\n",
    "        if self.mem_counter < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Initialize gradients\n",
    "        self.DQN.optimizer.zero_grad()\n",
    "        \n",
    "        # Generate batch such that the batch size is smaller than the number of results in the memory\n",
    "        max_batch = min(self.mem_counter,self.mem_size)\n",
    "        batch = np.random.choice(max_batch,self.batch_size,replace=False)\n",
    "        batch_index = np.arange(self.batch_size,dtype=np.int32)\n",
    "        \n",
    "        state_batch = T.tensor(self.state_mem[batch]).to(self.DQN.device)\n",
    "        new_state_batch = T.tensor(self.new_state_mem[batch]).to(self.DQN.device)\n",
    "        reward_batch = T.tensor(self.reward_mem[batch]).to(self.DQN.device)\n",
    "        terminal_batch = T.tensor(self.terminal_mem[batch]).to(self.DQN.device)\n",
    "        action_batch = self.action_mem[batch] # cannot follow the above format since used as index\n",
    "        \n",
    "        q_next = self.DQN_next(new_state_batch)\n",
    "        q_eval = self.DQN(state_batch)[batch_index,action_batch]\n",
    "        \n",
    "        q_target = reward_batch+self.discount*T.max(q_next,dim=1)[0]\n",
    "        q_target[terminal_batch] = 0.0\n",
    "        \n",
    "        # Backprop \n",
    "        loss = self.DQN.loss(q_target,q_eval).to(self.DQN.device)\n",
    "        loss.backward() # get the gradient of the current tensor\n",
    "        self.DQN.optimizer.step()\n",
    "        self.update_epsilon()\n",
    "        \n",
    "    # Epsilon decay\n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon > self.min_epsilon:\n",
    "            self.epsilon = self.epsilon-self.epsilon_decay\n",
    "    \n",
    "    def update_network(self):\n",
    "        self.DQN_next.load_state_dict(self.DQN.state_dict())      \n",
    "\n",
    "        \n",
    "# Model training\n",
    "if __name__=='__main__':\n",
    "    env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "    input_dims = [env.observation_space.shape[0]]\n",
    "    num_actions = env.action_space.n\n",
    "    discount = 0.99\n",
    "    epsilon = 1.0\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 64\n",
    "    \n",
    "    agent = Agent(discount,epsilon,learning_rate,input_dims,batch_size,num_actions)\n",
    "    \n",
    "    scores = []\n",
    "    avg_scores = []\n",
    "    episodes = 500\n",
    "    learn = True\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        score = 0\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            new_state,reward,done,_,_ = env.step(action)\n",
    "            \n",
    "            if i%10 == 0:\n",
    "                env.render()\n",
    "            \n",
    "            score += reward\n",
    "            \n",
    "            agent.storage(state,new_state,action,reward,done)\n",
    "            if learn:\n",
    "                agent.learn()\n",
    "                # Update target every 5 episodes\n",
    "                if i%5 == 0  and i != 0:\n",
    "                    agent.update_network()\n",
    "                    \n",
    "            state = new_state\n",
    "            \n",
    "        scores.append(score)\n",
    "        avg_score = np.mean(scores[-50:])\n",
    "        avg_scores.append(avg_score)\n",
    "        print(\"Episode: \",i,\"\\tScore: \",score,\"\\tAverage Score: %.3f\"% avg_score,\"Epsilon %.3f\" % agent.epsilon)\n",
    "        \n",
    "        # Early stopping when average score of the last 10 episodes is above 200\n",
    "        if np.mean(scores[-50:]) > 200:\n",
    "            episodes = i+1\n",
    "            break\n",
    "    \n",
    "    checkpoint = {\"model\":agent.DQN.state_dict(),\"score\":scores,\"episodes\":episodes} \n",
    "    #save_dir = \"C:/Users/Yang Yue/OneDrive/Documents/GitHub/STAT_430/prep/\"\n",
    "    save_dir = \"/Users/Yang/OneDrive/Documents/GitHub/STAT_430/prep/\"\n",
    "    T.save(checkpoint,save_dir+\"lunarlander-DQN.pt\")\n",
    "    \n",
    "    x = [i+1 for i in range(episodes)]\n",
    "    plt.plot(x,scores,label=\"episode_reward\")\n",
    "    plt.plot(x,avg_scores,label=\"average_reward\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Learning Curve DQN\")\n",
    "    plt.legend()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e73ada4-827a-4c84-9775-8f495997ef31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
