{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b200158-02e8-48f7-b0bc-13743348d2b1",
   "metadata": {},
   "source": [
    "![banner](https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/rl_banner.png?raw=true)\n",
    "\n",
    "# Value Iteration \n",
    "\n",
    "In the [previous section](https://github.com/priyammaz/PyTorch-Adventures/blob/main/PyTorch%20for%20Reinforcement%20Learning/Intro%20to%20Reinforcement%20Learning/Model-Based%20Learning/intro_rl_and_policy_iter.ipynb), we dug deep into the basics of RL and first derived the famous ```Bellman Equation```\n",
    "\n",
    "$$\\text{Bellman Equation: } V_\\pi(s) = \\sum_a \\pi(a|s)\\sum_{s'}\\sum_{r}[r + \\gamma V_{\\pi}(s')]P(s',r|a,s)$$\n",
    "\n",
    "We then saw that we can manipulate and reinterpret the Bellman Equation a bit to create the ```Bellman Optimality Equations``` which provide a recursive form to solve our $Q$ and Values functions.\n",
    "\n",
    "$$V^*(s) = \\max_a \\sum_{s'}\\sum_{r}[r + \\gamma V^*_{\\pi}(s')]P(s',r|a,s)$$\n",
    "$$Q^*(s,a) = \\sum_{s'}\\sum_{r}[r + \\gamma \\max_{a'}Q^*_\\pi(s',a')]P(s',r|a,s)$$\n",
    "\n",
    "But then we saw that to solve these equations could require an incredible large system of equations, making it intractable:\n",
    "\n",
    "$$\\boldsymbol{V_\\pi} = (\\boldsymbol{I} - \\gamma\\boldsymbol{P_\\pi})^{-1}\\boldsymbol{R_\\pi}$$\n",
    "\n",
    "Therefore, we have to use iterative methods to solve these systems. Previously we looked at the ```Policy Iteration``` method which had two steps:\n",
    "\n",
    "$$\\text{Policy Evaluation:  } V_{k+1}(s) = \\sum_a \\pi(a|s)\\sum_{s'}\\sum_{r}[r + \\gamma V_k(s')]P(s',r|a,s)$$\n",
    "$$\\text{Policy Improvement:  } \\pi'(s) = arg\\max_a \\sum_{s'}\\sum_{r}[r + \\gamma V_{\\pi}(s')]P(s',r|a,s) = arg\\max_a Q_\\pi(s,a)$$\n",
    "\n",
    "The intuition for Policy Iteration was, start with a random policy, iteratively compute its Value Function. Using the Value function, we can then compute our $Q$ value for every action at every state, and we select the action with the highest $Q$ Value to update our Policy. By repeating this, we will continue to improve our policy, but there is a catch: we have to also store the intermediate policy values. What about a different approach?\n",
    "\n",
    "### Value Iteration\n",
    "\n",
    "In Value Iteration, we don't care about the policies at all! All we want to do is, for our environment, find the **OPTIMAL VALUE FUNCTION**. The main difference between Value iteration here and the Policy Evaluation step in Policy Iteration is, in Policy Evaluation, we have a set Policy and we want to find the optimal Value function for that. Instead now, we want to find the optimal value function without fixing any policy at all. \n",
    "\n",
    "If we can update the Value function until convergence using our Bellman Equation, then at the end we can derive our Policy from the Optimal Values. \n",
    "\n",
    "$$\\text{Value Iteration:  } V_{k+1}(s) = \\max_a \\sum_{s'}\\sum_{r}[r + \\gamma V_k(s')]P(s',r|a,s) = \\max_a Q(s,a)$$\n",
    "\n",
    "This should look very similar to our Policy Evaluation step except for one key difference: Instead of summing over the actions dictated by our policy, we simply update our Values with the action that provides the highest returns.\n",
    "\n",
    "Therefore, this is basically an iterative method of solving for the Optimal Values Equation $V^*(s)$!\n",
    "\n",
    "#### Strategy\n",
    "\n",
    "At every state, compute the $Q$ value for every action, and pick the value for whichever action is the highest.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "There isn't much more to talk about, so lets go ahead and implement this on the Frozen Lake Game again! Also, this time, I won't do a separate stochastic vs deterministic implementation, this will work for both given the MDP! If you have any concerns though, start with the previous tutorial again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b6061d6-9587-4d64-9da2-cbb4bb98e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "### Define Environment ###\n",
    "env = gym.make('FrozenLake-v1', map_name=\"4x4\", render_mode=\"rgb_array\", is_slippery=True)\n",
    "\n",
    "### Quick Helper Function to Nicely Print Policy ###\n",
    "def print_policy(policy, grid=(4,4)):\n",
    "    action_dict = {0: \"Left\", 1: \"Down\", 2: \"Right\", 3: \"Up\"}\n",
    "    policy_print = np.empty(grid).astype(str)\n",
    "    for idx_h in range(grid[0]):\n",
    "        for idx_w in range(grid[1]):\n",
    "            index = idx_h * grid[0] + idx_w\n",
    "            selected_action = action_dict[policy[index]]\n",
    "            selected_action = selected_action[0] # Grab first letter\n",
    "            policy_print[idx_h, idx_w] = selected_action\n",
    "\n",
    "    print(\"Current Policy:\")\n",
    "    print(\"--------------\")\n",
    "    print(policy_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b70e77c-9e00-4540-9c48-9be56a61be64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Values\n",
      "[[0.54185998 0.49858161 0.47043461 0.45657012]\n",
      " [0.55829709 0.         0.35822941 0.        ]\n",
      " [0.59166815 0.64298202 0.6151213  0.        ]\n",
      " [0.         0.74165099 0.86280139 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(env, gamma=0.99, num_iterations=1000, tol=1e-5):\n",
    "\n",
    "    ### Just Like Before, Initialize Values as 0 ###\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "\n",
    "        ### Create a Copy of the Current Values Table ###\n",
    "        V_k = np.copy(V)\n",
    "\n",
    "        ### Loop Through Every State ###\n",
    "        for s in range(env.observation_space.n):\n",
    "\n",
    "            ### Create a List to Store Q Values in for every action for this state ###\n",
    "            Q_s = []\n",
    "\n",
    "            ### Loop Through The Action we Want to Take ###\n",
    "            for wanted_action in range(env.action_space.n):\n",
    "\n",
    "                ### List of Possible Destinations ###\n",
    "                possible_taken_actions = env.unwrapped.P[s][wanted_action]\n",
    "\n",
    "                ### Create a Q Value to Accumulate Into ###\n",
    "                Q_sa = 0\n",
    "                \n",
    "                ### Loop Through Potential States We Could End Up In ###\n",
    "                for probability, s_next, reward, terminal in possible_taken_actions:\n",
    "                \n",
    "                    ### Compute the Q Value (for this taken action) and Accumulate ###\n",
    "                    Q_sa += probability * (reward + gamma * V_k[s_next])\n",
    "\n",
    "                ### Store the Q Value for this action in our list ###\n",
    "                Q_s.append(Q_sa)\n",
    "\n",
    "            ### Whichever action gave the highest return, use that value in our updated Value Function ###\n",
    "            V[s] = np.max(Q_s)\n",
    "\n",
    "        ### Check for convergence\n",
    "        if np.max(np.abs(V - V_k)) < tol:\n",
    "            break\n",
    "\n",
    "    return V\n",
    "        \n",
    "optimal_values = value_iteration(env)\n",
    "\n",
    "print(\"Optimal Values\")\n",
    "print(np.array(optimal_values).reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97587ba5-6151-4a31-b865-e219f7a53a1c",
   "metadata": {},
   "source": [
    "## Compute our Policy\n",
    "\n",
    "Now that we have our optimal Values function, lets use it to compute our Policy! But wait havent we dont that already? \n",
    "\n",
    "In Policy Iteration we iterate between two steps, the first to compute our Values, and the second to get the best Policy from those values. If thats the case, I have the best values already, so why not use the ```Policy Improvement``` code to extract the best policy?\n",
    "\n",
    "$$\\pi'(s) = arg\\max_a \\sum_{s'}\\sum_{r}[r + \\gamma V^*(s')]P(s',r|a,s) = arg\\max_a Q(s,a)$$\n",
    "\n",
    "The key difference here is, in the Policy Improvement step, we just had some $V_\\pi(s')$ which were the optimal values for **THAT SPECIFIC POLICY**. Instead we can now use $V^*(s')$ because we have calculated our optimal values for the game.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c51680f4-e30c-498e-944e-a061480e1c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Policy:\n",
      "--------------\n",
      "[['L' 'U' 'U' 'U']\n",
      " ['L' 'L' 'L' 'L']\n",
      " ['U' 'D' 'L' 'L']\n",
      " ['L' 'R' 'D' 'L']]\n"
     ]
    }
   ],
   "source": [
    "def policy_improvement(values, gamma=0.99):\n",
    "\n",
    "    new_policy = np.zeros(env.observation_space.n)\n",
    "\n",
    "    for s in range(env.observation_space.n):\n",
    "\n",
    "        Q_s = []\n",
    "\n",
    "        ### For a state, iterate through all the actions I could take\n",
    "        for wanted_action in range(env.action_space.n):\n",
    "\n",
    "            ### Although I wanted to take that action, I could end up somewhere else, \n",
    "            ### This is a list of tuples as outlined earlier of potential destinations\n",
    "            possible_taken_actions = env.unwrapped.P[s][wanted_action]\n",
    "\n",
    "            ### Create an intermediate variable to accumulate into ###\n",
    "            Q_sa = 0\n",
    "            \n",
    "            for probability, s_next, reward, terminal in possible_taken_actions:\n",
    "                \n",
    "                ### Compute the Q Value (for this taken action) and Accumulate ###\n",
    "                Q_sa += probability * (reward + gamma * values[s_next])\n",
    "\n",
    "            ### Store the Q Value (for this ###\n",
    "            Q_s.append(Q_sa)\n",
    "\n",
    "        ### Find Which Action had the largest Q ###\n",
    "        best_action = np.argmax(Q_s) \n",
    "\n",
    "        ### Update Policy (for this state) with the new best action ###\n",
    "        new_policy[s] = best_action\n",
    "\n",
    "    return new_policy\n",
    "\n",
    "optimal_policy = policy_improvement(optimal_values)\n",
    "print_policy(optimal_policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253dbf74-3b0d-4ce7-9119-cc2af6ffd74a",
   "metadata": {},
   "source": [
    "### Lets Use Our Policy \n",
    "\n",
    "Finally, lets put our policy to work and play the game with it. Just like before, due to randomness, even if our policy is optimal, we may still loose. So lets play a ton of games and see what proportion we win!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09fdee4b-57ea-4243-8384-1388a75cc841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of Successful Games: 0.828\n"
     ]
    }
   ],
   "source": [
    "num_games = 1000\n",
    "max_steps = 500\n",
    "\n",
    "game_success = 0\n",
    "for _ in range(num_games):\n",
    "\n",
    "    observation, _ = env.reset()\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "    \n",
    "        # Select Action from Policy\n",
    "        action = int(optimal_policy[observation])\n",
    "        \n",
    "        # Take the action and update the environment state\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # Check if the game has ended\n",
    "        if done:\n",
    "            if reward > 0:\n",
    "                game_success += 1\n",
    "            break\n",
    "\n",
    "proportion_sucessful = game_success / num_games\n",
    "print(\"Proportion of Successful Games:\", proportion_sucessful)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
