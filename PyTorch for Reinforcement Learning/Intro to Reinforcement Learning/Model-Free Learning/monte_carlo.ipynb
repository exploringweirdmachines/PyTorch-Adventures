{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12888546-9443-4d58-b545-f9eeea992d25",
   "metadata": {},
   "source": [
    "# Monte Carlo\n",
    "\n",
    "We have previously explored [Policy Iteration](https://github.com/priyammaz/PyTorch-Adventures/blob/main/PyTorch%20for%20Reinforcement%20Learning/Intro%20to%20Reinforcement%20Learning/Model-Based%20Learning/intro_rl_and_policy_iter.ipynb) and [Value Iteration](https://github.com/priyammaz/PyTorch-Adventures/blob/main/PyTorch%20for%20Reinforcement%20Learning/Intro%20to%20Reinforcement%20Learning/Model-Based%20Learning/value_iteration.ipynb), and I will assume all of that as a prereq to this! We leveraged these iterative techniques to solve the Bellman Equation so we could play the game Frozen Lake:\n",
    "\n",
    "![image](https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/frozen_lake.gif?raw=true)\n",
    "\n",
    "Although we were able to successfuly build our policy (that determines the optimal action to take at a state), there is a major limitation to Value/Policy iteration: You Need the MDP!! In our Frozen Lake Game, we had the following provided to us by the ```gymnasium``` package:\n",
    "\n",
    "```python\n",
    "state1: {action1:[(probability, next_state, reward, done),\n",
    "                  (probability, next_state, reward, done),\n",
    "                  (probability, next_state, reward, done)],\n",
    "\n",
    "         action2:[(probability, next_state, reward, done),\n",
    "                  (probability, next_state, reward, done),\n",
    "                  (probability, next_state, reward, done)],\n",
    "        ...}\n",
    "state2: {action1:[(probability, next_state, reward, done),\n",
    "                  (probability, next_state, reward, done),\n",
    "                  (probability, next_state, reward, done)],\n",
    "\n",
    "         action2:[(probability, next_state, reward, done),\n",
    "                  (probability, next_state, reward, done),\n",
    "                  (probability, next_state, reward, done)],\n",
    "        ...}\n",
    "...\n",
    "```\n",
    "\n",
    "This is the MDP of the game, that describes everything we need to know about the environment. For every state in the game, and for every action I take, what is the reward I will get and where could I end up? This is problematic as what if we need to solve an environment where we dont have the MDP? Unfortunately, the methods of Policy and Value iteration no longer work.\n",
    "\n",
    "In both policy and value iteration, we try to estimate the values of the different states, and we could solve this by iteratively just solving the MDP. Now we have to leverage something else: Interactive Learning \n",
    "\n",
    "## Model Free Learning\n",
    "\n",
    "If you dont have the model of the game, then its called Model Free Learning. This is the more practical case, in most situations in the real world, you dont have some nice dictionary describing all the states and actions. Instead, we can try to learn the dynamics of the environment by interacting with it and learning from experience. \n",
    "\n",
    "This literally means: Send your agent out into the game blind over and over again and slowly learn the game (and in the end estimate the values of the states)\n",
    "\n",
    "The way we will explore today to do this is the Monte Carlo Method\n",
    "\n",
    "### Monte Carlo Values Estimation\n",
    "\n",
    "Monte Carlo experiments are a method of just randomly sampling a bunch and then using the information gained from the samples to produce an estimate. This means, send out our agent into the frozen lake a bunch of times and just log all the trajectories of how things are going every time. Initially, the choices the agent will make is basically random, but we can use this information to start to update our knowledge of the game and improve the decision making.\n",
    "\n",
    "When doing our Policy and Value iterations, remember that we never did ```env.step()``` when training our models. We only looked at our MDP and then once we had our best policy we applied it to the game. So then what are the steps of Monte Carlo?\n",
    "\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1) The agent will interact with the environment. Every step will produce an tuple $(S_t, A_t, R_{t+1}, S_{t+1})$ which is a fancy way of saying, where am I now ($S_t$), what action did I take ($A_t$), what reward did I get ($R_{t+1}$) and where did I end up ($S_{t+1}$). We will then get a sequence of these until the game ends (reaches the prize or falls in a hole)\n",
    "2) For every state we went to, we will calculate the estimated Q function $Q(s,a)$. What we want to estimate is the expected return of taking a specific action at a specific state, but how can we do this without the MDP? Well, we just completed a trajectory, so I know start to end exactly what happened. This means I can technically compute the future rewards at every state based on the experience I just had! There is obviously a lag here though, I can only update and learn once I have completed a full trajectory, we will solve that problem later!\n",
    "5) We can repeat these trajectories a bunch of times, greedy updating our policy as we go!\n",
    "\n",
    "### Lets Generate Trajectories\n",
    "\n",
    "It would be helpful to have a function that can generate trajectories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fea2537-79cc-49ad-9e19-759c35707108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1, 0.0, 4, False), (4, 0, 0.0, 4, False), (4, 1, 0.0, 5, True)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "def sample_trajectory(pi, env, max_steps=50, epsilon=0.1):\n",
    "\n",
    "    \"\"\"\n",
    "    In this method we will play the game until \n",
    "    its either over or we hit max steps according\n",
    "    to some policy PI\n",
    "\n",
    "    Args:\n",
    "        pi: The current policy\n",
    "        env: The Game\n",
    "        max_steps: Truncate trajectories longer than this\n",
    "        epsilon: Inject some randomness for exploration\n",
    "    \"\"\"\n",
    "\n",
    "    done = False\n",
    "    trajectory = []\n",
    "    num_steps = 0\n",
    "\n",
    "    ### Start New Game ###\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "\n",
    "        ### Select Action According to Policy ###\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()  # Explore random action\n",
    "        else:\n",
    "            action = pi[state]  # Exploit best known action\n",
    "\n",
    "        ### Take a step in the environment ###\n",
    "        next_state, reward, done, _, _= env.step(action)\n",
    "\n",
    "        ### Create and Store your Experience Tuple ###\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        trajectory.append(experience)\n",
    "\n",
    "        ### Iterate ###\n",
    "        num_steps += 1\n",
    "\n",
    "        if num_steps >= max_steps:\n",
    "            done = False\n",
    "            break\n",
    "\n",
    "        ### Update current State to the Next State ###\n",
    "        state = next_state\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "### Initialize a Random Policy ###\n",
    "policy = np.random.choice(env.action_space.n, size=(env.observation_space.n, ))\n",
    "trajectory = sample_trajectory(policy, env)\n",
    "\n",
    "print(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f8c66f-684b-4fab-a760-d725f7eaed1e",
   "metadata": {},
   "source": [
    "### Compute Future Returns\n",
    "\n",
    "Now that we can generate trajectories, we need to be able to compute the future discounted rewards at every state we visited. Now a question you may have is, what if we visited a state multiple times? There are actually two variants of this:\n",
    "\n",
    "- First Visit Monte Carlo: Only compute your future expected rewards for the first time you get to a state\n",
    "- Every Visit Monte Carlo: Average up the future expected rewards for all times you visited a state\n",
    "\n",
    "Today we will just do First Visit, but either would work!\n",
    "\n",
    "#### Reminder: How to Compute Returns\n",
    "\n",
    "Remember that $G_t$ is computed like the following:\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...$$\n",
    "\n",
    "### Computing returns $G_t$ (pretend we have 3 steps 0 -> 2):\n",
    "\n",
    "- **At $t = 2$ (last step):**  \n",
    "  $$\n",
    "  G_2 = R_3\n",
    "  $$\n",
    "\n",
    "- **At $t = 1$ (second-to-last step):**  \n",
    "  $$\n",
    "  G_1 = R_2 + \\gamma G_2\n",
    "  $$\n",
    "\n",
    "- **At $t = 0$ (first step):**  \n",
    "  $$\n",
    "  G_0 = R_1 + \\gamma G_1\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fde2bc69-a6ac-48f2-a1b3-0f1f1bcd3806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(4, 1): 0.0, (4, 0): 0.0, (0, 1): 0.0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_returns(trajectory, gamma=0.99):\n",
    "\n",
    "    ### Create Empty Dictionary to Store Returns ###\n",
    "    returns = {}\n",
    "\n",
    "    ### Initialize Returns ###\n",
    "    G = 0\n",
    "\n",
    "    ### Reverse and Compute Returns ###\n",
    "    for t in reversed(trajectory):\n",
    "\n",
    "        state, action, reward, _, _ = t\n",
    "\n",
    "        ### Compute new G ###\n",
    "        G = reward + gamma * G\n",
    "\n",
    "        ### First Visit Check ###\n",
    "        if (state, action) not in returns:\n",
    "            returns[(state, action)] = G\n",
    "\n",
    "    return returns\n",
    "\n",
    "compute_returns(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c8fbe-dadc-434a-af36-ded4e3887f5f",
   "metadata": {},
   "source": [
    "### Monte Carlo Estimation of Q\n",
    "\n",
    "Now that we can generate trajectories, we will do it over and over again, trying to estimate the Q function. We will do this by running a trajectory, computing its discounted returns (on the first visit) and then store those values. Then we can average up all the returns for every state/action pair and update Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2041738d-9b00-46a4-b12f-743d95fd9096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 1.87702338e-04, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 3.81595812e-03, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 6.75409672e-03, 2.63130282e-04, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 5.84146222e-04, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 1.05956757e-03, 0.00000000e+00, 0.00000000e+00],\n",
       "       [3.66666667e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.14667920e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.00000000e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def monte_carlo_estimation(pi,\n",
    "                           env, \n",
    "                           gamma=0.99, \n",
    "                           max_steps=50,\n",
    "                           num_episodes=5000):\n",
    "\n",
    "    ### Start Empty Q Values ###\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    returns = {(s, a): [] for s in range(env.observation_space.n) for a in range(env.action_space.n)}\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        ### Sample a Trajectory ###\n",
    "        trajectory = sample_trajectory(pi, env, max_steps)\n",
    "\n",
    "        ### Compute Returns for Episode ###\n",
    "        returns_for_episode = compute_returns(trajectory, gamma)\n",
    "\n",
    "        ### Update Returns ###\n",
    "        for (state,action), G in returns_for_episode.items():\n",
    "            returns[(state,action)].append(G)\n",
    "\n",
    "    ### Loop Through All the Accumulated Returns, Average Them Up and Update Q ###\n",
    "    for (state, action), returns_list in returns.items():\n",
    "\n",
    "        if len(returns_list) > 0:\n",
    "            Q[state, action] = np.mean(returns_list)\n",
    "\n",
    "    return Q\n",
    "    \n",
    "monte_carlo_estimation(policy, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93016265-d7e3-4c4b-92b4-3cb92926b3b6",
   "metadata": {},
   "source": [
    "### Update Our Policy\n",
    "\n",
    "Now that we have an estimate for our Q function, we can update our policy accoring to this Q Function. There are a few ways to do this, but easiest is a greedy selection of the highest value action in every state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c972da-23f9-452f-8593-e72f134b7170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(Q):\n",
    "\n",
    "    \"\"\"\n",
    "    Greedy select the action with the highest vlaue at every state\n",
    "    \"\"\"\n",
    "    return np.argmax(Q, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb59ec0-145e-41e0-b9f3-1404a806955e",
   "metadata": {},
   "source": [
    "### Toggle Between the Two\n",
    "\n",
    "Just like in policy iteration where we toggled between estimating the Values function and then updating our policy, we will be toggling between estimating our Q function and then updating our policy.\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "There are again tons of ways to do this. We could have also done an online estimation of our policy, updating it as we are estimating Q at the same time. Its really upto you! I am going for this toggle approach, because it lets me get a good estimate of Q first and then update the policy according to that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3db3abee-73a7-49cb-8554-4013407bf479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 1 3 0 0 2 0 3 1 0 0 0 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "def monte_carlo_policy_iteration(env, \n",
    "                                 gamma=0.99, \n",
    "                                 max_steps=50, \n",
    "                                 num_episodes=10000):\n",
    "\n",
    "    # Start with a random policy\n",
    "    policy = np.random.choice(env.action_space.n, size=(env.observation_space.n, ))\n",
    "\n",
    "    while True:\n",
    "        # Policy Evaluation: Estimate Q(s,a) for the current policy\n",
    "        values = monte_carlo_estimation(policy, env, gamma, max_steps, num_episodes)\n",
    "\n",
    "        # Policy Improvement: Generate a new policy based on the estimated Q(s,a)\n",
    "        new_policy = policy_improvement(values)\n",
    "\n",
    "        # If the policy stops changing, we have converged\n",
    "        if np.array_equal(policy, new_policy):\n",
    "            break\n",
    "\n",
    "        policy = new_policy  # Update policy for next iteration\n",
    "\n",
    "    return policy, values\n",
    "\n",
    "optimal_policy, optimal_values = monte_carlo_policy_iteration(env)\n",
    "\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617768c-e0c0-4a44-a3ab-6370fcf0b796",
   "metadata": {},
   "source": [
    "### Lets Test Our our Policy! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0733fefc-1440-4e17-bc44-b572304e7cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Success Rate: 77.20%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.772"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_policy(policy, env, num_episodes=500):\n",
    "    success_count = 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            if done and reward == 1.0:  # Reached the goal\n",
    "                success_count += 1\n",
    "\n",
    "    success_rate = success_count / num_episodes\n",
    "    print(f\"Policy Success Rate: {success_rate * 100:.2f}%\")\n",
    "    return success_rate\n",
    "\n",
    "# Test the learned policy\n",
    "test_policy(optimal_policy, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
