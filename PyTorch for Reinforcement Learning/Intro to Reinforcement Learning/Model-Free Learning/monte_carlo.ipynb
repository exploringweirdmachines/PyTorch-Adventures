{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12888546-9443-4d58-b545-f9eeea992d25",
   "metadata": {},
   "source": [
    "# Monte Carlo\n",
    "\n",
    "We have previously explored [Policy Iteration](https://github.com/priyammaz/PyTorch-Adventures/blob/main/PyTorch%20for%20Reinforcement%20Learning/Intro%20to%20Reinforcement%20Learning/Model-Based%20Learning/intro_rl_and_policy_iter.ipynb) and [Value Iteration](https://github.com/priyammaz/PyTorch-Adventures/blob/main/PyTorch%20for%20Reinforcement%20Learning/Intro%20to%20Reinforcement%20Learning/Model-Based%20Learning/value_iteration.ipynb), and I will assume all of that as a prereq to this! We leveraged these iterative techniques to solve the Bellman Equation so we could play the game Frozen Lake:\n",
    "\n",
    "![image](https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/frozen_lake.gif?raw=true)\n",
    "\n",
    "Although we were able to successfuly build our policy (that determines the optimal action to take at a state), there is a major limitation to Value/Policy iteration: You Need the MDP!! In our Frozen Lake Game, we had the following provided to us by the ```gymnasium``` package:\n",
    "\n",
    "```python\n",
    "state1: {action1:[(probability, next_state, reward, done),\n",
    "                  (probability, next_state, reward, done),\n",
    "                  (probability, next_state, reward, done)],\n",
    "\n",
    "         action2:[(probability, next_state, reward, done),\n",
    "                  (probability, next_state, reward, done),\n",
    "                  (probability, next_state, reward, done)],\n",
    "        ...}\n",
    "state2: {action1:[(probability, next_state, reward, done),\n",
    "                  (probability, next_state, reward, done),\n",
    "                  (probability, next_state, reward, done)],\n",
    "\n",
    "         action2:[(probability, next_state, reward, done),\n",
    "                  (probability, next_state, reward, done),\n",
    "                  (probability, next_state, reward, done)],\n",
    "        ...}\n",
    "...\n",
    "```\n",
    "\n",
    "This is the MDP of the game, that describes everything we need to know about the environment. For every state in the game, and for every action I take, what is the reward I will get and where could I end up? This is problematic as what if we need to solve an environment where we dont have the MDP? Unfortunately, the methods of Policy and Value iteration no longer work.\n",
    "\n",
    "In both policy and value iteration, we try to estimate the values of the different states, and we could solve this by iteratively just solving the MDP. Now we have to leverage something else: Interactive Learning \n",
    "\n",
    "## Model Free Learning\n",
    "\n",
    "If you dont have the model of the game, then its called Model Free Learning. This is the more practical case, in most situations in the real world, you dont have some nice dictionary describing all the states and actions. Instead, we can try to learn the dynamics of the environment by interacting with it and learning from experience. \n",
    "\n",
    "This literally means: Send your agent out into the game blind over and over again and slowly learn the game (and in the end estimate the values of the states)\n",
    "\n",
    "The way we will explore today to do this is the Monte Carlo Method\n",
    "\n",
    "### Monte Carlo Q(s,a) Values Estimation\n",
    "\n",
    "Monte Carlo experiments are a method of just randomly sampling a bunch and then using the information gained from the samples to produce an estimate. This means, send out our agent into the frozen lake a bunch of times and just log all the trajectories of how things are going every time. Initially, the choices the agent will make is basically random, but we can use this information to start to update our knowledge of the game and improve the decision making.\n",
    "\n",
    "When doing our Policy and Value iterations, remember that we never did ```env.step()``` when training our models. We only looked at our MDP and then once we had our best policy we applied it to the game. So then what are the steps of Monte Carlo?\n",
    "\n",
    "So we first have a choice. We can either estimate the Values of a state (expected future rewards of a state) or we can estimate the Q Values (expected future rewards of a state and action). It seemed more intuitive to do this with the Q Function as we want to know the best action to take at every state, so that what I went with. We will repeat this for the Values afterwards as its basically the same idea, all that changes is our policy improvement method!\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1) The agent will interact with the environment. Every step will produce an tuple $(S_t, A_t, R_{t+1}, S_{t+1})$ which is a fancy way of saying, where am I now ($S_t$), what action did I take ($A_t$), what reward did I get ($R_{t+1}$) and where did I end up ($S_{t+1}$). We will then get a sequence of these until the game ends (reaches the prize or falls in a hole)\n",
    "2) For every state we went to, we will calculate the estimated Q function $Q(s,a)$. What we want to estimate is the expected return of taking a specific action at a specific state, but how can we do this without the MDP? Well, we just completed a trajectory, so I know start to end exactly what happened. This means I can technically compute the future rewards at every state based on the experience I just had! There is obviously a lag here though, I can only update and learn once I have completed a full trajectory, we will solve that problem later!\n",
    "5) We can repeat these trajectories a bunch of times, greedy updating our policy as we go!\n",
    "\n",
    "### Lets Generate Trajectories\n",
    "\n",
    "It would be helpful to have a function that can generate trajectories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fea2537-79cc-49ad-9e19-759c35707108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, 0.0, 0, False), (0, 0, 0.0, 0, False), (0, 0, 0.0, 4, False), (4, 3, 0.0, 4, False), (4, 3, 0.0, 5, True)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "def sample_trajectory(pi, env, max_steps=50, epsilon=0.1):\n",
    "\n",
    "    \"\"\"\n",
    "    In this method we will play the game until \n",
    "    its either over or we hit max steps according\n",
    "    to some policy PI\n",
    "\n",
    "    Args:\n",
    "        pi: The current policy\n",
    "        env: The Game\n",
    "        max_steps: Truncate trajectories longer than this\n",
    "        epsilon: Inject some randomness for exploration\n",
    "    \"\"\"\n",
    "\n",
    "    done = False\n",
    "    trajectory = []\n",
    "    num_steps = 0\n",
    "\n",
    "    ### Start New Game ###\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "\n",
    "        ### Select Action According to Policy ###\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()  # Explore random action\n",
    "        else:\n",
    "            action = pi[state]  # Exploit best known action\n",
    "\n",
    "        ### Take a step in the environment ###\n",
    "        next_state, reward, done, _, _= env.step(action)\n",
    "\n",
    "        ### Create and Store your Experience Tuple ###\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        trajectory.append(experience)\n",
    "\n",
    "        ### Iterate ###\n",
    "        num_steps += 1\n",
    "\n",
    "        if num_steps >= max_steps:\n",
    "            done = False\n",
    "            break\n",
    "\n",
    "        ### Update current State to the Next State ###\n",
    "        state = next_state\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "### Initialize a Random Policy ###\n",
    "policy = np.random.choice(env.action_space.n, size=(env.observation_space.n, ))\n",
    "trajectory = sample_trajectory(policy, env)\n",
    "\n",
    "print(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f8c66f-684b-4fab-a760-d725f7eaed1e",
   "metadata": {},
   "source": [
    "### Compute Future Returns\n",
    "\n",
    "Now that we can generate trajectories, we need to be able to compute the future discounted rewards at every state we visited. Now a question you may have is, what if we visited a state multiple times? There are actually two variants of this:\n",
    "\n",
    "- First Visit Monte Carlo: Only compute your future expected rewards for the first time you get to a state\n",
    "- Every Visit Monte Carlo: Average up the future expected rewards for all times you visited a state\n",
    "\n",
    "Today we will just do First Visit, but either would work!\n",
    "\n",
    "#### Reminder: How to Compute Returns\n",
    "\n",
    "Remember that $G_t$ is computed like the following:\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...$$\n",
    "\n",
    "### Computing returns $G_t$ (pretend we have 3 steps 0 -> 2):\n",
    "\n",
    "- **At $t = 2$ (last step):**  \n",
    "  $$\n",
    "  G_2 = R_3\n",
    "  $$\n",
    "\n",
    "- **At $t = 1$ (second-to-last step):**  \n",
    "  $$\n",
    "  G_1 = R_2 + \\gamma G_2\n",
    "  $$\n",
    "\n",
    "- **At $t = 0$ (first step):**  \n",
    "  $$\n",
    "  G_0 = R_1 + \\gamma G_1\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fde2bc69-a6ac-48f2-a1b3-0f1f1bcd3806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(4, 3): 0.0, (0, 0): 0.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_returns(trajectory, gamma=0.99):\n",
    "\n",
    "    ### Create Empty Dictionary to Store Returns ###\n",
    "    returns = {}\n",
    "\n",
    "    ### Initialize Returns ###\n",
    "    G = 0\n",
    "\n",
    "    ### Reverse and Compute Returns ###\n",
    "    for t in reversed(trajectory):\n",
    "\n",
    "        state, action, reward, _, _ = t\n",
    "\n",
    "        ### Compute new G ###\n",
    "        G = reward + gamma * G\n",
    "\n",
    "        ### First Visit Check ###\n",
    "        if (state, action) not in returns:\n",
    "            returns[(state, action)] = G\n",
    "\n",
    "    return returns\n",
    "\n",
    "compute_returns(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c8fbe-dadc-434a-af36-ded4e3887f5f",
   "metadata": {},
   "source": [
    "### Monte Carlo Estimation of Q\n",
    "\n",
    "Now that we can generate trajectories, we will do it over and over again, trying to estimate the Q function. We will do this by running a trajectory, computing its discounted returns (on the first visit) and then store those values. Then we can average up all the returns for every state/action pair and update Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2041738d-9b00-46a4-b12f-743d95fd9096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.00147674, 0.        , 0.00230529],\n",
       "       [0.        , 0.        , 0.        , 0.00233432],\n",
       "       [0.00549253, 0.00584016, 0.00332406, 0.        ],\n",
       "       [0.00139346, 0.        , 0.        , 0.        ],\n",
       "       [0.00477333, 0.        , 0.00504706, 0.00107607],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.00524007, 0.        , 0.01334161, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.00993577],\n",
       "       [0.        , 0.        , 0.00639475, 0.07315308],\n",
       "       [0.12211233, 0.07071429, 0.28843821, 0.00463191],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.1225125 , 0.        , 0.14142857],\n",
       "       [0.        , 0.        , 0.38047619, 1.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def monte_carlo_estimation(pi,\n",
    "                           env, \n",
    "                           gamma=0.99, \n",
    "                           max_steps=50,\n",
    "                           num_episodes=5000):\n",
    "\n",
    "    ### Start Empty Q Values ###\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    returns = {(s, a): [] for s in range(env.observation_space.n) for a in range(env.action_space.n)}\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        ### Sample a Trajectory ###\n",
    "        trajectory = sample_trajectory(pi, env, max_steps)\n",
    "\n",
    "        ### Compute Returns for Episode ###\n",
    "        returns_for_episode = compute_returns(trajectory, gamma)\n",
    "\n",
    "        ### Update Returns ###\n",
    "        for (state,action), G in returns_for_episode.items():\n",
    "            returns[(state,action)].append(G)\n",
    "\n",
    "    ### Loop Through All the Accumulated Returns, Average Them Up and Update Q ###\n",
    "    for (state, action), returns_list in returns.items():\n",
    "\n",
    "        ### As Long as we had visits (we could have also not ever visisted a state) ###\n",
    "        if len(returns_list) > 0:\n",
    "            Q[state, action] = np.mean(returns_list)\n",
    "\n",
    "    return Q\n",
    "    \n",
    "monte_carlo_estimation(policy, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93016265-d7e3-4c4b-92b4-3cb92926b3b6",
   "metadata": {},
   "source": [
    "### Update Our Policy\n",
    "\n",
    "Now that we have an estimate for our Q function, we can update our policy accoring to this Q Function. There are a few ways to do this, but easiest is a greedy selection of the highest value action in every state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c972da-23f9-452f-8593-e72f134b7170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(Q):\n",
    "\n",
    "    \"\"\"\n",
    "    Greedy select the action with the highest vlaue at every state\n",
    "    \"\"\"\n",
    "    return np.argmax(Q, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb59ec0-145e-41e0-b9f3-1404a806955e",
   "metadata": {},
   "source": [
    "### Toggle Between the Two\n",
    "\n",
    "Just like in policy iteration where we toggled between estimating the Values function and then updating our policy, we will be toggling between estimating our Q function and then updating our policy.\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "There are again tons of ways to do this. We could have also done an online estimation of our policy, updating it as we are estimating Q at the same time. Its really upto you! I am going for this toggle approach, because it lets me get a good estimate of Q first and then update the policy according to that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3db3abee-73a7-49cb-8554-4013407bf479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 1 3 0 0 2 0 3 1 0 0 0 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "def monte_carlo_policy_iteration(env, \n",
    "                                 gamma=0.99, \n",
    "                                 max_steps=50, \n",
    "                                 num_episodes=10000):\n",
    "\n",
    "    # Start with a random policy\n",
    "    policy = np.random.choice(env.action_space.n, size=(env.observation_space.n, ))\n",
    "\n",
    "    while True:\n",
    "        # Policy Evaluation: Estimate Q(s,a) for the current policy\n",
    "        Q = monte_carlo_estimation(policy, env, gamma, max_steps, num_episodes)\n",
    "\n",
    "        # Policy Improvement: Generate a new policy based on the estimated Q(s,a)\n",
    "        new_policy = policy_improvement(Q)\n",
    "\n",
    "        # If the policy stops changing, we have converged\n",
    "        if np.array_equal(policy, new_policy):\n",
    "            break\n",
    "\n",
    "        policy = new_policy  # Update policy for next iteration\n",
    "\n",
    "    return policy, Q\n",
    "\n",
    "optimal_policy, optimal_Q = monte_carlo_policy_iteration(env)\n",
    "\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617768c-e0c0-4a44-a3ab-6370fcf0b796",
   "metadata": {},
   "source": [
    "### Lets Test Our our Policy! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0733fefc-1440-4e17-bc44-b572304e7cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Success Rate: 75.00%\n"
     ]
    }
   ],
   "source": [
    "def test_policy(policy, env, num_episodes=500):\n",
    "    success_count = 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            if done and reward == 1.0:  # Reached the goal\n",
    "                success_count += 1\n",
    "\n",
    "    success_rate = success_count / num_episodes\n",
    "    print(f\"Policy Success Rate: {success_rate * 100:.2f}%\")\n",
    "\n",
    "# Test the learned policy\n",
    "test_policy(optimal_policy, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282e006a-5b39-4fce-bd59-35c6e7435dca",
   "metadata": {},
   "source": [
    "### Sucess! \n",
    "\n",
    "Our method of exploring the environment to solve it works! \n",
    "\n",
    "### Bit More Efficiency\n",
    "\n",
    "So one difference between what I did here and what you would find online is my update function for the Q Values. \n",
    "\n",
    "In my implementation I simply accumulate all my discounted rewards for my trajectories and then take an average at the end:\n",
    "\n",
    "```python\n",
    "### Loop Through All the Accumulated Returns, Average Them Up and Update Q ###\n",
    "for (state, action), returns_list in returns.items():\n",
    "    if len(returns_list) > 0:\n",
    "        Q[state, action] = np.mean(returns_list)\n",
    "```\n",
    "\n",
    "Instead of doing this, we can do a running average and update Q every iteration. Not that it makes a difference, but lets try to match what we typically see. \n",
    "\n",
    "#### Running Mean\n",
    "\n",
    "Lets say we have collected K returns for a particular State-Action pair (that is our list of returns). At the end we simply took a mean like the following:\n",
    "\n",
    "$$Q_k = \\frac{1}{k}\\sum_{i=1}^kG_i$$\n",
    "\n",
    "But this mean can only be taken at the end, in most online algorithms we want to be computed as we are going, so lets see how that happens. Assume then we have our average up to $Q_k$. Now we have recieved a new return $G_{k+1}$ and we want to compute our new $Q_{k+1}$. Then this would be just like before:\n",
    "\n",
    "$$Q_{k+1} = \\frac{1}{k+1}\\sum_{i=1}^{k+1}G_i$$\n",
    "\n",
    "We can then separate it like this:\n",
    "\n",
    "$$Q_{k+1} = \\frac{1}{k+1}\\left[\\left(\\sum_{i=1}^{k}G_i\\right) + G_{k+1}\\right]$$\n",
    "\n",
    "We can also rewrite $Q_k = \\frac{1}{k}\\sum_{i=1}^kG_i$ as $\\sum_{i=1}^kG_i = k * Q_k$ and substitute!\n",
    "\n",
    "$$Q_{k+1} = \\frac{1}{k+1}\\left[k * Q_k + G_{k+1}\\right]$$\n",
    "$$Q_{k+1} = \\frac{k * Q_k}{k+1} + \\frac{G_{k+1}}{k+1}$$\n",
    "\n",
    "We can then use our age old, add one subtract one trick and write:\n",
    "\n",
    "$$\\frac{k}{k+1} = \\frac{k+1-1}{k+1} = 1 - \\frac{1}{k+1}$$\n",
    "\n",
    "Lets substitute that back in:\n",
    "$$Q_{k+1} = \\left(1 - \\frac{1}{k+1}\\right) * Q_k + \\frac{G_{k+1}}{k+1}$$\n",
    "$$Q_{k+1} = Q_k - \\frac{Q_k}{k+1} + \\frac{G_{k+1}}{k+1}$$\n",
    "$$Q_{k+1} = Q_k - \\frac{1}{k+1}(G_{k+1} - Q_k)$$\n",
    "\n",
    "And there we go! Our online averaging. Its just some dumb algebra, but thought it would be helpful to include just so its clear where this is coming from!\n",
    "\n",
    "### Implement Online Monte Carlo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "263717a1-f6bc-40c9-9d09-f3346e3ded7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_monte_carlo_estimation(pi,\n",
    "                                  env,\n",
    "                                  gamma=0.99,\n",
    "                                  max_steps=50,\n",
    "                                  num_episodes=5000):\n",
    "\n",
    "    ### Start Empty Q Values ###\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    ### We Need to track our number of visits to each State/Action pair (K in the equation above) ###\n",
    "    N = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        ### Sample a Trajectory ###\n",
    "        trajectory = sample_trajectory(pi, env, max_steps)\n",
    "\n",
    "        ### Compute Returns for Episode (First Visit) ###\n",
    "        returns = compute_returns(trajectory, gamma)\n",
    "\n",
    "        ### Update Q based on first visits in this episode ###\n",
    "        for (state, action), G in returns.items():\n",
    "        \n",
    "\n",
    "            ### Compute Q Value with Online Averating Formula ###\n",
    "            Q[state, action] = Q[state, action] + (G - Q[state, action]) / (N[state, action] + 1)\n",
    "\n",
    "            ### Iterate N for this State/Action Pair ###\n",
    "            N[state, action] += 1\n",
    "\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2edf40d-a4a9-44b1-8172-c6805bb3c572",
   "metadata": {},
   "source": [
    "### Update Policy Iteration Method\n",
    "\n",
    "Policy iteration is the same as before! We just need to swap out our Monte Carlo estimator with our new online monte carlo estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e74c6451-a227-4ccf-b659-a56ffdf3daf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 2 3 0 0 2 0 3 1 0 0 0 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "def monte_carlo_policy_iteration(env, \n",
    "                                 gamma=0.99, \n",
    "                                 max_steps=50, \n",
    "                                 num_episodes=10000):\n",
    "\n",
    "    # Start with a random policy\n",
    "    policy = np.random.choice(env.action_space.n, size=(env.observation_space.n, ))\n",
    "\n",
    "    while True:\n",
    "        # Policy Evaluation: Estimate Q(s,a) for the current policy\n",
    "        Q = online_monte_carlo_estimation(policy, env, gamma, max_steps, num_episodes)\n",
    "\n",
    "        # Policy Improvement: Generate a new policy based on the estimated Q(s,a)\n",
    "        new_policy = policy_improvement(Q)\n",
    "\n",
    "        # If the policy stops changing, we have converged\n",
    "        if np.array_equal(policy, new_policy):\n",
    "            break\n",
    "\n",
    "        policy = new_policy  # Update policy for next iteration\n",
    "\n",
    "    return policy, Q\n",
    "\n",
    "optimal_policy, optimal_Q = monte_carlo_policy_iteration(env)\n",
    "\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41c562c3-8aeb-4217-bb4b-07196ed42cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Success Rate: 73.00%\n"
     ]
    }
   ],
   "source": [
    "test_policy(optimal_policy, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16fc31d-12a0-44e6-9f19-bc4f78500aca",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "\n",
    "The final change we need to make is adding in a learning rate. In most implementations online you will see that instead of doing the following (what we just implemented above):\n",
    "\n",
    "$$Q_{k+1} = Q_k - \\frac{1}{k+1}(G_{k+1} - Q_k)$$\n",
    "\n",
    "You will instead swap out our constant in front with some learning rate like the following:\n",
    "\n",
    "\n",
    "$$Q_{k+1} = Q_k - \\alpha_t(G_{k+1} - Q_k)$$\n",
    "\n",
    "Where $\\alpha_t$ is some time dependent learning rate parameter. We could have a scheduler, we could also just use a constant. For now lets just do a simple exponential scheduler!\n",
    "\n",
    "#### Create a Simple Exponential Decay Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ecc8761-3401-4ed0-b91d-05d96d9d75e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def lr_scheduler(start_val, min_val, decay_factor, num_episodes):\n",
    "\n",
    "    alphas = [start_val * decay_factor**episode for episode in range(num_episodes)]\n",
    "    alphas = [a if a >= min_val else min_val for a in alphas]\n",
    "\n",
    "    return alphas\n",
    "\n",
    "alphas = lr_scheduler(1.0, 0.01, 0.99, 5000)\n",
    "\n",
    "plt.plot(alphas)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b83d89e-6584-4ced-ac47-0ca4a65d6c1d",
   "metadata": {},
   "source": [
    "#### Update the Monte Carlo Estimator to use Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c68628d5-7cc5-4675-a449-d96e611286ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_monte_carlo_estimation_w_lr(pi,\n",
    "                                       env,\n",
    "                                       gamma=0.99,\n",
    "                                       max_steps=50,\n",
    "                                       num_episodes=5000,\n",
    "                                       lr_start_val=0.8, \n",
    "                                       lr_min_val=0.01, \n",
    "                                       lr_decay_factor=0.99):\n",
    "\n",
    "    ### Start Empty Q Values ###\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    ### We Need to track our number of visits to each State/Action pair (K in the equation above) ###\n",
    "    alphas = lr_scheduler(lr_start_val, lr_min_val, lr_decay_factor, num_episodes)\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        ### Sample a Trajectory ###\n",
    "        trajectory = sample_trajectory(pi, env, max_steps)\n",
    "\n",
    "        ### Compute Returns for Episode (First Visit) ###\n",
    "        returns = compute_returns(trajectory, gamma)\n",
    "\n",
    "        ### Update Q based on first visits in this episode ###\n",
    "        for (state, action), G in returns.items():\n",
    "        \n",
    "            ### Compute Q Value with Online Averating Formula ###\n",
    "            Q[state, action] = Q[state, action] + alphas[i] * (G - Q[state, action])\n",
    "\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addddcf7-62b4-4bed-8942-b56379183a1f",
   "metadata": {},
   "source": [
    "### Update Policy Iteration Method\n",
    "\n",
    "Policy iteration is the same as before! We just need to swap out our Monte Carlo estimator with our new online monte carlo estimator with learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf5a30a8-99c4-46a4-8386-96c97c7cd54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 0 1 0 0 2 0 3 1 0 0 0 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "def monte_carlo_policy_iteration(env, \n",
    "                                 gamma=0.99, \n",
    "                                 max_steps=50, \n",
    "                                 num_episodes=10000,\n",
    "                                 lr_start_val=0.6, \n",
    "                                 lr_min_val=0.01, \n",
    "                                 lr_decay_factor=0.98):\n",
    "\n",
    "    # Start with a random policy\n",
    "    policy = np.random.choice(env.action_space.n, size=(env.observation_space.n, ))\n",
    "\n",
    "    while True:\n",
    "        # Policy Evaluation: Estimate Q(s,a) for the current policy\n",
    "        Q = online_monte_carlo_estimation_w_lr(policy, env, gamma, max_steps, num_episodes, lr_start_val, lr_min_val, lr_decay_factor)\n",
    "\n",
    "        # Policy Improvement: Generate a new policy based on the estimated Q(s,a)\n",
    "        new_policy = policy_improvement(Q)\n",
    "\n",
    "        # If the policy stops changing, we have converged\n",
    "        if np.array_equal(policy, new_policy):\n",
    "            break\n",
    "\n",
    "        policy = new_policy  # Update policy for next iteration\n",
    "\n",
    "    return policy, Q\n",
    "\n",
    "optimal_policy, optimal_Q = monte_carlo_policy_iteration(env)\n",
    "\n",
    "print(optimal_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c46cfc0-2ad6-4092-8688-56e7028d77e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Success Rate: 70.00%\n"
     ]
    }
   ],
   "source": [
    "test_policy(optimal_policy, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb433bb-73bd-47ea-8438-9acca1ccd7ff",
   "metadata": {},
   "source": [
    "### Thats it!\n",
    "\n",
    "We have completed an important part of RL, Model Free Learning. Again, access to the MDP of an environment is basically impossible in most real world scenarios. So we used Monte Carlo to explore the environment and estimate the Q Values from its findings!\n",
    "\n",
    "There is a clear limitation though, we can only learn after a trajectory. What if we wanted to learn as we are going? That is exactly what TD Learning does and what we will explore next!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
