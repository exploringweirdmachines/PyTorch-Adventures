{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "797fd6bd-09fa-4600-8105-aa65514f7c34",
   "metadata": {},
   "source": [
    "![banner](https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/rl_banner.png?raw=true)\n",
    "\n",
    "# On-Policy TD Learning: SARSA\n",
    "\n",
    "As of now you should understand the [Monte Carlo](https://github.com/priyammaz/PyTorch-Adventures/blob/main/PyTorch%20for%20Reinforcement%20Learning/Intro%20to%20Reinforcement%20Learning/Model-Free%20Learning/monte_carlo.ipynb) method of solving a Model-Free RL problem! But there is one major issue when it came to Monte Carlo:\n",
    "\n",
    "**You have to complete a full trajectory (wait for an entire episode to finish) before we start learning our estimates for the Q values**\n",
    "\n",
    "This restriction makes it basically impossible to use in complex scenarios. In our case in our easy Frozen Lake example that we have been working on, the time it takes us to get through a full trajectory (either we fall into a hole or get to the finish line) is not really a problem. But imagine a significantly larger game board, each episode would take forever, and we need to do thousands of them before we have a helpful estimate! So what do we do?\n",
    "\n",
    "### TD Learning\n",
    "\n",
    "Temporal-Difference Learning is a method that, instead of learning at the end of a full trajectory, just learns at every step along each trajectory. The reason this works is becauase TD Learning uses the current estimate of future rewards rather than waiting for the full episode.\n",
    "\n",
    "##### Reminder: Monte Carlo \n",
    "\n",
    "In the Monte Carlo method we completed a full trajectory and then computed the expected future rewards for every state that we touched using the following formula we have always had for our returns:\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...$$\n",
    "\n",
    "We then used these computed rewards from our trajectory to update our $Q$ function:\n",
    "\n",
    "$$Q(s,a) = Q(s,a) + \\alpha_t(G - Q(s,a))$$\n",
    "\n",
    "The problem is, for us to have this estimate $G$, we needed to have fully completed the Trajectory. So instead of  using the full trajectory as our estimate, can we just use the current $Q$ value instead?\n",
    "\n",
    "### TD Error\n",
    "\n",
    "TD Methods update the estimates based on the TD Error, which is the difference between the new estimate and the next steps information. The TD Target is an estimate of the return, and the setup is the same as what we had before:\n",
    "\n",
    "$$Q(s,a) = Q(s,a) - \\alpha_t(\\text{TD Target} - Q(s,a))$$\n",
    "\n",
    "But what is the TD Target? It depends on what you are doing!\n",
    "\n",
    "**TD Targets**:\n",
    "\n",
    "- Monte Carlo: $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...$\n",
    "- SARSA: $r + \\gamma * Q(s', a')$ -> Uses next action from the current policy (On Policy)\n",
    "- Q-Learning: $r + \\gamma * max_{a'} Q(s', a')$ -> Uses max Q Value of next state (Off Policy)\n",
    "\n",
    "But nothing else changed! We are still doing what we had done in Monte Carlo, but in both Q-Learning and SARSA, we don't have to wait for the trajectory to be done, we can learn as we go. This only works though because of Bootstrapping:\n",
    "\n",
    "### Bootstrapping\n",
    "\n",
    "Bootstrapping is a commonly used technique in statistics. It is a procedure to estimate something by using another estimate. For example, if we want to estimate something about a population, its typically impossible to do so as the population could be very large. Instead we grab small samples from our population and try to learn things about the smaller samples. Hopefully with enough small samples you can then say something about your population!\n",
    "\n",
    "Similarly, in TD Learning, instead of waiting for the future to unfold (going through the full trajectory like Monte Carlo), just use the current guess of the $Q$ Values as our TD Target. Therefore TD learning uses bootstrapping because it updates Q-values using estimates (not actual returns like Monte Carlo). This sounds good, but has its obvious Pros and Cons:\n",
    "\n",
    "Pros:\n",
    "\n",
    "- Faster Learning (as we dont need to complete the full trajectory every time)\n",
    "- Allows us to work with partial episodes\n",
    "- Memory efficient, we dont care about early parts of the trajectory once we are done with it\n",
    "\n",
    "Cons: \n",
    "\n",
    "- More challenging convergence (as we are using partial information to estimate our returns)\n",
    "- Needs more focus on exploration vs exploitaion\n",
    "\n",
    "### On Policy\n",
    "\n",
    "We will be implementing SARSA today, but Q-Learning is basically the same with one key difference: On vs Off Policy!\n",
    "\n",
    "Lets look at our TD Targets again:\n",
    "\n",
    "- SARSA: $r + \\gamma * Q(s', a')$\n",
    "- Q-Learning: $r + \\gamma * max_{a'} Q(s', a')$\n",
    "\n",
    "In SARSA, we will select the action $a'$ that our current policy prescribes for the state $s'$. Therefore, by following the policy, this is an On-Policy method\n",
    "\n",
    "In Q-Learning, we will select the action $a'$ that has the max $Q$ value at that state $s'$ (regardless of the action we actually took!). This action may not necessarily be the same as the policy, therefore this is known as an Off-Policy method! This way the agent can learn the optimal policy independent of the actions it took. (We will do this next time!)\n",
    "\n",
    "### Lets Implement SARSA \n",
    "\n",
    "An important change here (that we could have also done in our Monte Carlo) is we wont create a specific policy. We know that our policy is just the Argmax of Q, so just updating the Q table is fine, we can always get the policy from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ba2ac37-6b8c-4f1d-88ce-0bde0792cd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 0 3 0 0 2 0 3 1 0 0 0 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "def epsilon_greedy(Q, state, epsilon, env):\n",
    "\n",
    "    ### Either sample a random action (explore) or \n",
    "    ### Use best action according to Q Table \n",
    "    \n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample() \n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "        \n",
    "def sarsa(env, \n",
    "          num_episodes=25000, \n",
    "          alpha=0.1, \n",
    "          gamma=0.99, \n",
    "          epsilon=0.1):\n",
    "\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        \n",
    "        state, _ = env.reset()\n",
    "\n",
    "        ### Select an Action to Start Out ###\n",
    "        action = epsilon_greedy(Q, state, epsilon, env)\n",
    "            \n",
    "        ### Loop Until Done ###\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            ### Take the Action ###\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            ### Select the Next Action from the New State ###\n",
    "            next_action = epsilon_greedy(Q, next_state, epsilon, env)\n",
    "\n",
    "            ### SARSA Update Rule (On policy as we are using the Q value from the actual action taken) ###\n",
    "            Q[state, action] = Q[state, action] + alpha * (reward + \\\n",
    "                                   gamma * Q[next_state, next_action] - Q[state, action])\n",
    "            \n",
    "            ### Update Current State/Action ###\n",
    "            state, action = next_state, next_action\n",
    "\n",
    "    return Q\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "Q_sarsa = sarsa(env)\n",
    "\n",
    "policy = np.argmax(Q_sarsa, axis=-1)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc01670-3251-4008-89bd-fefc2adee1c8",
   "metadata": {},
   "source": [
    "### Lets Test our Policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd79f4f-696f-4c67-89ae-9bf8c6b92f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Success Rate: 76.20%\n"
     ]
    }
   ],
   "source": [
    "def test_policy(policy, env, num_episodes=500):\n",
    "    success_count = 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            if done and reward == 1.0:  # Reached the goal\n",
    "                success_count += 1\n",
    "\n",
    "    success_rate = success_count / num_episodes\n",
    "    print(f\"Policy Success Rate: {success_rate * 100:.2f}%\")\n",
    "\n",
    "test_policy(policy, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
