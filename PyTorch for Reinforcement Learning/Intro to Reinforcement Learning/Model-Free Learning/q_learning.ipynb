{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612e96b1-ada9-408a-ba3d-5b916bc049d2",
   "metadata": {},
   "source": [
    "![banner](https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/rl_banner.png?raw=true)\n",
    "\n",
    "# Off Policy TD Learning: Q Learning\n",
    "\n",
    "Now that we undestand SARSA (an On-Policy TD Learning method) lets look at Q Learning (an Off-Policy TD Learning Method)\n",
    "\n",
    "**Reminder: SARSA TD Target:** $r + \\gamma * Q(s', a')$\n",
    "\n",
    "Our goal is always to find future rewards for taking an action. This is why in Monte-Carlo, we used actual data (an entire trajectory) to compute what to expect in the future, but this leads to obvious inefficencies, as discussed before! \n",
    "\n",
    "Therefore in TD Learning, instead of using real trajectories to get the estimate of the future returns, we instead use our **CURRENT ESTIMATE**. The returns are discounted so this always looks like:\n",
    "\n",
    "$$\\text{Current Reward} + \\gamma * \\text{Future Reward}$$\n",
    "\n",
    "But there are a few ways we can estimate our Future Rewards.\n",
    "\n",
    "So the process is this:\n",
    "\n",
    "1) Start the game at state $s_1$\n",
    "2) Select an action $a_1$ using Epsilon Greedy policy\n",
    "3) Take action $a_1$ and end up in state $s_2$\n",
    "4) Select an action $a_2$ using Epsilon Greedy policy\n",
    "5) **NOW THE CHOICE**\n",
    "    - **On Policy:** Use the Q Value for the action actually selected $a_2$ by the epsilon-greedy policy\n",
    "        -  $Q(s_1, a_1) = Q(s_1, a_1) + \\alpha * \\left[r + \\gamma Q(s_2, a_2) - Q(s_1, a_1)\\right]$\n",
    "        -  The key idea here is that, both our action selection in step 2 and our Q value selection (for action selected at step 4) for future rewards were done by an epsilon greedy policy\n",
    "    - **Off-Policy:** Use the maximum Q Value for the next state, regardles of which $a_2$ was selected\n",
    "        - $Q(s_1, a_1) = Q(s_1, a_1) + \\alpha * \\left[r + \\gamma \\max_{a'}Q(s_2,a') - Q(s_1, a_1)\\right]$\n",
    "        - The key idea here is that, our action selection in step 2 was an epsilon greedy policy, but our Q value selection for future rewards was Greedy only! This means than in the randomness of our Epsilon Greedy strategy, we may not have picked the action with the highest Q value, but in our update rule, we assume that the best possible action was taken regardless of what actually happened!\n",
    "\n",
    "6) Repeat until convergence\n",
    "\n",
    "This difference is subtle but important!\n",
    "\n",
    "### Lets Implement It!\n",
    "\n",
    "The code here is basically identical to our SARSA code, we just updated our learning step to use the Max of the Q table regardless of what action was taken!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b044178-5d31-45ab-aa32-cb04897a5a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 0 1 0 0 0 0 3 1 0 0 0 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "def epsilon_greedy(Q, state, epsilon, env):\n",
    "\n",
    "    ### Either sample a random action (explore) or \n",
    "    ### Use best action according to Q Table \n",
    "    \n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample() \n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "        \n",
    "def q_learning(env, \n",
    "               num_episodes=25000, \n",
    "               alpha=0.1, \n",
    "               gamma=0.99, \n",
    "               epsilon=0.1):\n",
    "\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        \n",
    "        state, _ = env.reset()\n",
    "\n",
    "        ### Select an Action to Start Out ###\n",
    "        action = epsilon_greedy(Q, state, epsilon, env)\n",
    "            \n",
    "        ### Loop Until Done ###\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            ### Take the Action ###\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            ### Select the Next Action from the New State ###\n",
    "            next_action = epsilon_greedy(Q, next_state, epsilon, env)\n",
    "\n",
    "            ### Q Update Rule (Off policy as we are using the Q value of the max regardless of action taken) ###\n",
    "            Q[state, action] = Q[state, action] + alpha * (reward + \\\n",
    "                                   gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "            \n",
    "            ### Update Current State/Action ###\n",
    "            state, action = next_state, next_action\n",
    "\n",
    "    return Q\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "Q_qlearning = q_learning(env)\n",
    "\n",
    "policy = np.argmax(Q_qlearning, axis=-1)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63b20be8-8134-41fa-941d-979895569cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Success Rate: 77.00%\n"
     ]
    }
   ],
   "source": [
    "def test_policy(policy, env, num_episodes=500):\n",
    "    success_count = 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            if done and reward == 1.0:  # Reached the goal\n",
    "                success_count += 1\n",
    "\n",
    "    success_rate = success_count / num_episodes\n",
    "    print(f\"Policy Success Rate: {success_rate * 100:.2f}%\")\n",
    "\n",
    "test_policy(policy, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
