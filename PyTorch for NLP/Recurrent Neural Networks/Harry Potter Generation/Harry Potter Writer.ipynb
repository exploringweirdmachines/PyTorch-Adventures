{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf89ed06-d914-48f5-b72a-3a5fdc2ab315",
   "metadata": {},
   "source": [
    "![banner](../../../src/visuals/banner.png)\n",
    "\n",
    "## Deep Learning for Text Generation!\n",
    "\n",
    "Today we will go a bit further into the LSTM architecture! Lets recap an imporant figure we saw previously:\n",
    "\n",
    "![mapping](../../../src/visuals/rnn_input_output_setups.png)\n",
    "\n",
    "[credit](https://wandb.ai/ayush-thakur/dl-question-bank/reports/LSTM-RNN-in-Keras-Examples-of-One-to-Many-Many-to-One-Many-to-Many---VmlldzoyMDIzOTM)\n",
    "\n",
    "\n",
    "In our Sequence classification that we did last time, we had a **Many to One** model, where we had a sequence of inputs, but were trying to model a single binary classifier head. This time we will try something closer to a **Many to Many** model. We will take in a sequence, and we will use the historical information of the sequence to predict the next token. More specifically, we are about to write an AI that can generate Harry Potter!!! \n",
    "\n",
    "A lot of this lesson was inspired by work done by [Andrej Karpathy](https://github.com/karpathy/ng-video-lecture/blob/master/bigram.py) and his implementation of GPT. We will be using an LSTM model instead here but we followed similar patterns of preprocessing and prepping data/character tokenizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56677130-af91-41a1-8881-5e270b7cdecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np \n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e6c96-cff4-4122-b1f2-5a556208e454",
   "metadata": {},
   "source": [
    "Before we get into it though, we need to talk about the two ways we can pass data into an LSTM model:\n",
    "- With an initialized Hidden and Cell State as $H_0$ and $C_0$, we can pass an entire sequence in and get the final outputs as well as $H_n$ and $C_n$\n",
    "- With an initialized Hidden and Cell State as $H_0$ and $C_0$, we can pass in the first token of the sequence $X_0$ which will output $H_1$ and $C_1$. We can then pass in the next token in our sequence $X_1$ along with the $H_1$ and $C_1$ from previous to get our $H_2$ and $C_2$. We will constantly repeat this process until we have completed the sequence! Internally PyTorch does something similar when we feed in an entire sequence, but we will break it up so we can make a prediction for the next token at every step!\n",
    "\n",
    "Lets implement both of these and check that they are equivalent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4aa0e57-8b47-4e03-83d5-d3aed4237561",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5        # How Many Samples\n",
    "sequence_length = 15   # Sequence Length Per Sample\n",
    "input_size = 10       # Dimension of vector for each timestep in sequence per sample      \n",
    "hidden_size = 20      # Dimension expansion from Input size Inside the LSTM cell\n",
    "num_layers = 2        # Number of LSTM Cells\n",
    "\n",
    "lstm = nn.LSTM(input_size=input_size, \n",
    "               hidden_size=hidden_size, \n",
    "               num_layers=num_layers, \n",
    "               batch_first=True)\n",
    "\n",
    "rand = torch.randn(batch_size, sequence_length, input_size)\n",
    "\n",
    "### Method 1 ###\n",
    "h0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "c0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "method_1_outs, (hn, cn) = lstm(rand, (h0,c0))\n",
    "\n",
    "### Method 2 ###\n",
    "h = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "c = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "\n",
    "outs = []\n",
    "\n",
    "for i in range(sequence_length):\n",
    "    token = rand[:, i, :].unsqueeze(1)\n",
    "    out, (h,c) = lstm(token, (h,c))\n",
    "\n",
    "    outs.append(out)\n",
    "\n",
    "method_2_outs = torch.concat(outs, axis=1)\n",
    "\n",
    "torch.allclose(method_1_outs, method_2_outs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd2863e-5d12-4365-987a-d3f92fbd4e8d",
   "metadata": {},
   "source": [
    "### How Do We Build a Dataset for Next Token Prediction?\n",
    "\n",
    "To keep things simple, we will not build a Dataset and Dataloader like we did before as we don't really need it! Lets setup the problem, here is a sentence from Harry Potter and the Sorcerers Stone!\n",
    "\n",
    "**Strange how nearsighted being invisible can make you**\n",
    "\n",
    "The following example will be a Word Level tokenization but we will implement Character Level to save on computation!\n",
    "\n",
    "First we tokenize our text:\n",
    "\n",
    "```\n",
    "\"Strange how nearsighted being invisible can make you\" -> [\"Strange\", \"how\", \"nearsighted\", \"being\", \"invisible\", \"can\", \"make\", \"you ]\n",
    "```\n",
    "\n",
    "Now our goal is, we want to pass in the word \"Strange\" to our model and then predict \"how\". We will then pass in the word \"how\" and hope to predict \"nearsighted\", and so on! Therefore, we can setup our data as such:\n",
    "\n",
    "```\n",
    "input = [\"Strange\", \"how\", \"nearsighted\", \"being\", \"invisible\", \"can\", \"make\"]\n",
    "label = [\"how\", \"nearsighted\", \"being\", \"invisible\", \"can\", \"make\", \"you\"]\n",
    "```\n",
    "\n",
    "Notice that the label really is just one shifted to the right compared to the input!\n",
    "\n",
    "### Dataset Difference from Sequence Classification Task\n",
    "In our Sequence Classification task, we had sentences that were directly tied to some binary label. In this case, we just have a large text full of Harry Potter! We can then just sample some predetermined sequence length from this dataset, stack many of them together as a batch, and then train!\n",
    "\n",
    "#### Character Level Modeling\n",
    "Ideally we would use N-Grams or more powerful tokenization, but the purpose here is to again demonstrate the capabilities of LSTM's without a huge resource burden! If you want to try more advanced tokenizers for  you data, feel free to update the code to use [TikToken](https://github.com/openai/tiktoken) that is used often with GPT type models. Our goal will be to predict a character given a previous character and see what kind of output we can expect!\n",
    "\n",
    "Lets first load in all our text and find all the unique characters available to us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fec80605-e0d4-49af-a5ec-488f56a4e26e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_data = \"../../../data/harry_potter_txt\"\n",
    "text_files = os.listdir(path_to_data)\n",
    "\n",
    "all_text = \"\"\n",
    "for book in text_files:\n",
    "    path_to_book = os.path.join(path_to_data, book)\n",
    "\n",
    "    with open(path_to_book, \"r\") as f:\n",
    "        text = f.readlines()\n",
    "\n",
    "    text = [line for line in text if \"Page\" not in line]\n",
    "    text = \" \".join(text).replace(\"\\n\", \"\")\n",
    "    text = [word for word in text.split(\" \") if len(word) > 0]\n",
    "    text = \" \".join(text)\n",
    "    all_text+=text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98dda946-4075-462f-99a3-a78595a71b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = sorted(list(set(all_text)))\n",
    "\n",
    "char2idx = {c:i for (i,c) in enumerate(unique_chars)}\n",
    "idx2char = {i:c for (i,c) in enumerate(unique_chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8fddd7-d21e-4d6e-83c9-a303c9e4abd8",
   "metadata": {},
   "source": [
    "### Build a DataGenerator\n",
    "Because we are sampling strings of data randomly from this body of text, we can just build a class that will stack together N samples with a given sequence length. It will then return back an input and target that are offset from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fc740aa8-b1a3-49f5-8c6c-130db9d0521a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataBuilder:\n",
    "    def __init__(self, seq_len=300, text=all_text):\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.text = text\n",
    "        self.file_length = len(text)\n",
    "\n",
    "    def grab_random_sample(self):\n",
    "\n",
    "        start = np.random.randint(0, self.file_length-self.seq_len)\n",
    "        end = start + self.seq_len\n",
    "        text_slice = self.text[start:end]\n",
    "\n",
    "        input_text = text_slice[:-1]\n",
    "        label = text_slice[1:]\n",
    "\n",
    "        input_text = torch.tensor([char2idx[c] for c in input_text])\n",
    "        label = torch.tensor([char2idx[c] for c in label])\n",
    "\n",
    "        return input_text, label\n",
    "        \n",
    "    def grab_random_batch(self, batch_size):\n",
    "\n",
    "        input_texts, labels = [], []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            input_text, label = self.grab_random_sample()\n",
    "\n",
    "            input_texts.append(input_text)\n",
    "            labels.append(label)\n",
    "\n",
    "        input_texts = torch.stack(input_texts)\n",
    "        labels = torch.stack(labels)\n",
    "\n",
    "        return input_texts, labels\n",
    "\n",
    "dataset = DataBuilder(seq_len=10)\n",
    "input_texts, labels = dataset.grab_random_batch(batch_size=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5848c23a-e3d2-4d65-86cd-a792a1bdcb5d",
   "metadata": {},
   "source": [
    "### Define Model \n",
    "\n",
    "This should look very similar to our Sequence Classification model with a few changes:\n",
    "\n",
    "- Our final Fully Connected Layer will predict the next character rather than a binary classification problem\n",
    "- We will write a generate function that takes a string as an input and continues to write for however many tokens we request\n",
    "    - One thing we didn't talk about much yet is sampling! We will apply a Softmax to the output of the linear layer so we can predict the probability of the next token given the current one. Instead of just taking the character that has the highest probability, we will sample from a multinomial distribution with the given probabilities. This means that, we have the highest chance to choose the character with the highest probability, but it gives the model an opportunity to also sample other characters and explore more options!\n",
    "    \n",
    "    \n",
    "#### MultiNomial Distribution\n",
    "\n",
    "If we have a coin that we want to flip, we can sample from a binomial distribution once to see if we get heads or tails. If we expand it and have more options and role a dice (which has 6 sides) if we sample from a multinomial distribution once we would get one of the sides. For our problem, we will be passing in a probability vector of length num_characters, and we will randomly sample to see what character we get. Again, this should help with both getting more unique outputs and allow the model to explore more possibilities than a simple Argmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2e234e5e-9a24-468f-8d4c-2487b6de76fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello)eEU&;/Z”;zZ”9VGsbil4?H\\\\■,Oc4i□uA■&.F\\\\dZ|!VjmiSR*\\\\meQ•t(I8IS1Bgbu~fC.b•y'7:0n•Ks•;))k/R6IWEKN|fBp”cn\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTMForGeneration(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, num_characters=len(char2idx), hidden_size=256, n_layers=3, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_characters = num_characters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding = nn.Embedding(num_characters, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=n_layers, \n",
    "                            batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, num_characters)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        output, (h,c) = self.lstm(x)\n",
    "\n",
    "        logits = self.fc(output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def write(self, text, max_characters, greedy=False):\n",
    "\n",
    "        idx = torch.tensor([char2idx[c] for c in text], device=self.device)\n",
    "\n",
    "        hidden = torch.zeros(self.n_layers, self.hidden_size).to(self.device)\n",
    "        cell = torch.zeros(self.n_layers, self.hidden_size).to(self.device)\n",
    "\n",
    "        for i in range(max_characters):\n",
    "\n",
    "            if i == 0:\n",
    "                selected_idx = idx\n",
    "            else:\n",
    "                selected_idx = idx[-1].unsqueeze(0)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "            out = self.fc(out)\n",
    "\n",
    "            if len(out) > 1:\n",
    "\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "\n",
    "            \n",
    "            probs = self.softmax(out)\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = torch.argmax(probs)\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "  \n",
    "            idx = torch.cat([idx, idx_next[0]])\n",
    "            \n",
    "        gen_string = [idx2char[int(c)] for c in idx] \n",
    "        gen_string = \"\".join(gen_string)\n",
    "\n",
    "        return gen_string\n",
    "\n",
    "\n",
    "\n",
    "model = LSTMForGeneration()\n",
    "text = \"hello\"\n",
    "model.write(text, 100, greedy=False)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a666de4-e48b-44a7-8162-2f3dc82e8ee1",
   "metadata": {},
   "source": [
    "### Lets Train this Model!!!\n",
    "\n",
    "Quick aside on Cross Entropy Loss. Our model will return a tensor of the shape **[batch x seq_len x n_characters]** and our labels will have shape **[batch x seq_len]**. PyTorch Cross Entropy expects the following shapes for their inputs as follows:\n",
    "\n",
    "```\n",
    "Inputs = (N x C X D1 x D2 x ...)\n",
    "Labels = (N x D1 x D2 X ...)\n",
    "```\n",
    "\n",
    "This means our labels must be batch first, and each batch has cooresponding indexes along the sequence length we want to match. The inputs on the other hand must have batch first, then class dim (num_characters) and then all other dimensions like the sequence length. \n",
    "\n",
    "Therefore, because out output is of shape **[batch x seq_len x n_characters]**, we will have to do a transpose to **[batch x n_characters x seq_len]** to ensure PyTorch is happy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3bc1815c-2cee-43b2-ad6f-0961179d0ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Iteration 0\n",
      "Loss 4.535403251647949\n",
      "Sample Generation\n",
      "Spells 2cl—l?&KG8d□bc)—RV(b~S'“\"-)*>2□%7hm)j|b0Z.GBVM&vVT'PRga “kghCS‘;('L7K6>boo~3□”Y—(—VrAz’OceETz3nr?]*“lF■bD]BGnGR '—o7Fhw*fQ’hHPCTrx5w'—*6qts”lcDPQyD/:]fOGb8Xq tdnTYJEY8Mmj~&5fA&Sh(PG2s~PGwO|.4.J%fGv4-e\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Iteration 300\n",
      "Loss 1.860412836074829\n",
      "Sample Generation\n",
      "Spells you on lisbed out biffsurly Carled, a pet it the mickatautp, Tnotery, a thelt, Anrent dowblen he a-limered of Mcheam anres and stike. Harry, cum cinky deir to usmiut miman dapted efceattia Perappron’s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Iteration 600\n",
      "Loss 1.4449594020843506\n",
      "Sample Generation\n",
      "Spells on thinker on the hand had Professor Buurn scream-Counny and his hour min and his al palceman everets, who would can’t don’t bagmaver on on a sure higp.” “I have mather voice horry drain — banously An\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Iteration 900\n",
      "Loss 1.2801140546798706\n",
      "Sample Generation\n",
      "Spells how to like by a moment spirlled. “If you-LiPhCom’s greapans — they halfes,” said Harry. “Uclenty daremonty. ’ake it someone, I impressing to stands, he’grietyed a letch complain to. It Causy work — d\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Iteration 1200\n",
      "Loss 1.2586841583251953\n",
      "Sample Generation\n",
      "Spells ... the gorn years. ... Filthroke out of Fudge hall, Hermione, Hermione had content shiny bangs; he turned him, as they were loudly, but it’s not me. \"..” “You large,” said Harry, didn’t have properly\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Iteration 1500\n",
      "Loss 1.2016139030456543\n",
      "Sample Generation\n",
      "Spells they’re murdered and dinner. With his before, because this ... a worls land his dust into the follower or understands will ask middle, “A many wizone he had in ruck around up,” said Harry, and Voldemo\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Iteration 1800\n",
      "Loss 1.1648752689361572\n",
      "Sample Generation\n",
      "Spells of eUGOUL! TICo Harr Notkered for it, shadowy until must haven trying Malfoys could, but really had there’s more charm, Albus Muggle parplements having see Dumbledore. But Dumbledore taken the store y\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Iteration 2100\n",
      "Loss 1.1522903442382812\n",
      "Sample Generation\n",
      "Spells out of the head nodded out. As where Harry just stopped a dark hopelet smile. “Perhaps all detention\\” Harry kept place. A gave anything but a generous Harry could hear it waters: Hagrid, spief while \n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Iteration 2400\n",
      "Loss 1.1290769577026367\n",
      "Sample Generation\n",
      "Spells more broom diary little use of Sirius, news shab-looking gorily, and was out of his and that with a headmaster, for Givicks: Professor McGonagall, keen when Hermione had grovered through the dishing; \n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Iteration 2700\n",
      "Loss 1.1171519756317139\n",
      "Sample Generation\n",
      "Spells things yesely, though if I’m sure you sun, we also,” said Ginny, down. “Well, the heads on the old man without as his thick ever pretending their on against me toward their suddenly and shook a spotas\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "iterations = 3000\n",
    "max_len = 300\n",
    "evaluate_interval = 300\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "n_layers = 3\n",
    "lr = 0.003\n",
    "batch_size = 128\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = LSTMForGeneration(embedding_dim, len(char2idx), hidden_size, n_layers, DEVICE).to(DEVICE)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "dataset = DataBuilder()\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    input_texts, labels = dataset.grab_random_batch(batch_size=batch_size)\n",
    "    input_texts, labels = input_texts.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_texts)\n",
    "\n",
    "    output = output.transpose(1,2)\n",
    "\n",
    "    loss = loss_fn(output, labels)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iteration % evaluate_interval == 0:\n",
    "        print(\"--------------------------------------\")\n",
    "        print(f\"Iteration {iteration}\")\n",
    "        print(f\"Loss {loss.item()}\")\n",
    "        generated_text = model.write(\"Spells \", max_characters=200)\n",
    "        print(\"Sample Generation\")\n",
    "        print(generated_text)\n",
    "        print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3be59-bf3b-427b-a4dd-dc14a9e7360e",
   "metadata": {},
   "source": [
    "### Its Alive!!! (Sort of)\n",
    "\n",
    "If we read the text above, it definitely looks like english and all the words are like english, but the sentences are gibberish. This is a limitation of a character level model as it is too granular and higher order toknization is needed, along with much larger models to make long range relationships between words! Regardless though, the code would be nearly identical except for how the tokenization was happening and the total vocab size of that tokenization pattern. \n",
    "\n",
    "We will come back to this later and build a GPT type model that hopefully solves some of these issues outlined!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
