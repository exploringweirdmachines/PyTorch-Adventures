{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be657c0b-74e5-44eb-b394-e359c1a6483b",
   "metadata": {},
   "source": [
    "![bannner](../../src/visuals/banner.png)\n",
    "\n",
    "# Deep Learning for Sequences\n",
    "\n",
    "It is finally time to change our data type! Up till now we have explored a lot of the PyTorch methods specifically for Images. This means that we have some static data and are trying to make some downstream prediction of it. What this method was missing was any temporal or sequence dimension where a specific datapoint in time has some relations to times before and after it.\n",
    "\n",
    "We have all types of sequences in the real world!\n",
    "\n",
    "- Natural Langauge is a Sequence of Words\n",
    "- Speech is a Sequence of Signals at varying amplitudes\n",
    "- Stock Market is a Sequence of Price data\n",
    "- Videos are a Sequence of Images\n",
    "\n",
    "The ability to be able to model this is crucial! Here are few of the options we have currently:\n",
    "\n",
    "- **Convolution!** Instead of iterating through a 2D image, we would just slide a filter across a 1D vector of values. Otherwise this is the same as everything we did with Vision. Unfortunately, Convolutions don't have any mechanism to model relationships between tokens that are further apart than the filter size. Therefore, if we have a kernel of size 7 and a sentence with 30 words, we couldn't related how the last words are related to the first words.\n",
    "\n",
    "- **Recurrent Neural Network:** RNN can model sequence data and use a concept called \"Backpropagation Through Time\" to be able to relate different parts of a sequence. We will explore backprop through time a little later, but the main problem with RNN models is it can only successfully relate short sequences as it doesn't have the \"memory\" to remember things far back in time.\n",
    "\n",
    "- **Long Short Term Memory:** A popular variant of the RNN that incorporates additional logic to enable remembering things further back in time!\n",
    "\n",
    "- **Transformers:** The most popular and powerful sequence model today due to a lot of great properties! We will go into depth on the Transformer Architecture in a future lesson!\n",
    "\n",
    "\n",
    "## Recurrent Neural Network\n",
    "\n",
    "Lets first take a look at the RNN Architecture!\n",
    "\n",
    "![RNN](https://editor.analyticsvidhya.com/uploads/17464JywniHv.png)\n",
    "\n",
    "[credit](https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/)\n",
    "\n",
    "On the left side we see clearly how the RNN works. We pass in the first part of the input X through some hidden state and some weight matrix W will map it to an output. We then grab the next part of the sequence for X, reuse the updated hidden state from before and again remap to another output for the next timestep. This process is repeated until the sequence is complete. On the right, we see this process flattened out, and this is known as the **Unrolled View** of the RNN. \n",
    "\n",
    "\n",
    "Here is a visual that shows you how it all fits together!\n",
    "![rnngif](https://www.simplilearn.com/ice9/free_resources_article_thumb/Fully_connected_Recurrent_Neural_Network.gif)\n",
    "[credit](https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn)\n",
    "\n",
    "\n",
    "Now in these examples, we see that we have only a single layer of an RNN, but you can theoretically have as many as you want! Here is a visual of this that really helped me piece all this together. **Do note that this image below is actually of an LSTM which has one crucial difference from RNN!**\n",
    "\n",
    "![Unfolded LSTM](https://i.stack.imgur.com/SjnTl.png)\n",
    "\n",
    "[credit](https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm)\n",
    "\n",
    "Here are the main ideas to takeaway from the image above!\n",
    "- We start with some $h_0$ and $c_0$ known as the hidden and cell states. \n",
    "    - RNN and LSTM both have a Hidden State that acts as the \"working memory\" for the model. This typically cannot model long sequences very well on its own.\n",
    "    - The Cell State is unique to the LSTM and acts as the long term memory for the model to remember attributes of the sequence far back in time. \n",
    "    \n",
    "- The **depth** shows the number of LSTM/RNN layers we want and **t** is the number of timesteps we have in our data\n",
    "\n",
    "- The output gives us the \"hidden states\" of every value in the timeseries from the last LSTM/RNN layer which can be passed to another LSTM/RNN if we wanted. On the right, we can get the final hidden state/cell state for all layers that can be used for prediction (potentially, it depends on what we want). \n",
    "\n",
    "    - To clarify, the output gives all hidden states from $h_1, h_2, ... h_n$ where on the right, we get only $h_n$ as well as $c_n$ if it is an LSTM model. \n",
    "    \n",
    "    \n",
    "## RNN vs LSTM\n",
    "\n",
    "Going in depth into the architectural differences of RNN vs LSTM and how \"long term memory\" is encoded is a bit tough to explain but let me first offer some intuition. RNN suffer a condition that we talked about when going through ResNets: **Vanishing Gradients**. The reason this happens is the way these sequence models optimize through a technique known as backpropagation through time. \n",
    "\n",
    "\n",
    "#### Backpropagation Through Time \n",
    "First think back to the Rolled version of the RNN module, at every iteration we are passing in a single timepoint through some learnable parameters, but the underlying weights to optimize are the same. (i.e You can have a sequence as long as you want but  the model will be the same size, saving on computation). This means the same set of weights will need to be optimized and we are encoding the change in sequence by updating the hidden state that hopefully aggregates previous information. Therefore if we have inputs $x_1, x_2, ... x_n$, we will pass in a value at a time, update the hidden state, and output a value. Afterwards we will calculate a loss depending on what our task was (classification, regression, etc...) and then we will perform the chain rule and backpropagate through every time an input was passed to our weights. If we had $N$ inputs in our sequence, then we will have $N$ things to multiply together in our gradients\n",
    "\n",
    "It seems like a weird idea but it will make more sense with the visual:\n",
    "\n",
    "![backpropthroughtime](https://media.licdn.com/dms/image/D5612AQEgpJmnvwHxyA/article-cover_image-shrink_600_2000/0/1680279761408?e=2147483647&v=beta&t=p1mo3UbmkkXzkO_EyyY_PKGYSYuRn8DWJF_pOUT-r-s)\n",
    "\n",
    "[credit](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/)\n",
    "\n",
    "You can clearly see that to get back to updating at time $x_0$, we need to backpropagate through all the times after it! So the derivative of $E_3$, the output for our RNN at timestep 3, with respect to the weights in $S_0$ will be as such:\n",
    "\n",
    "$$\\frac{dE_3}{ds_0} =\\frac{dE_3}{ds_3}\\frac{ds_3}{ds_2}\\frac{ds_2}{ds_1}\\frac{ds_1}{ds_0}$$\n",
    "\n",
    "\n",
    "Therefore, if our sequence gets longer and the gradients are small, we will have the same issue that ResNet tried to solve with Vanishing Gradients! \n",
    "\n",
    "\n",
    "![image.png](https://ashutoshtripathicom.files.wordpress.com/2021/06/rnn-vs-lstm.png)\n",
    "\n",
    "[credit](https://ashutoshtripathi.com/2021/07/02/what-is-the-main-difference-between-rnn-and-lstm-nlp-rnn-vs-lstm/)\n",
    "\n",
    "As we compare our LSTM and RNN architecture, we can see that the LSTM has much more going on but there are a few important ways it avoids the \"forgetfullness\" of RNN.\n",
    " - **Forget Gate:** Decides what past information is important and what to remove. This is then passed through a sigmoid where values close to 0 would cause it to forget, and values close to 1 are to keep\n",
    " - **Input Gate:** A calculation to figure out how much of the input values from $x_t$ (the current timestep value) and $h_{t-1}$ (the hidden state from the previous timestep) should be encoded into the cell state. Essentially, how important is the current timestep and should we add it to our long term memory. Again it uses a sigmoid to scale between 0 and 1.\n",
    "- **Output Gate:** Calculation of what the next hidden state should be for the next timestep. \n",
    "\n",
    "\n",
    "The Cell State does a simple sum with the output of our Forget and Input Gates and then moves on to the next timestep! Essentially, during backpropagation, this pathway created by the cell state offers a new path for backpropagation that circumvents all the messy calculations happening in the gates, greatly reducing the vanishing gradient problem in a similar way we dealt with ResNet!\n",
    "\n",
    "\n",
    "## What Kinds of Problems can we Solve with Sequence Models?\n",
    "\n",
    "![mapping](https://api.wandb.ai/files/ayush-thakur/images/projects/103390/4fc355be.png)\n",
    "\n",
    "[credit](https://wandb.ai/ayush-thakur/dl-question-bank/reports/LSTM-RNN-in-Keras-Examples-of-One-to-Many-Many-to-One-Many-to-Many---VmlldzoyMDIzOTM)\n",
    "\n",
    "- One to One is what we have been doing up till now: Given a single image what is a prediction\n",
    "- One to Many: Given a vector of Image Features can we generate a text caption\n",
    "- Many to One: Can we classify a sequence?\n",
    "- Many to Many: This can be two things\n",
    "    - If Input/Output are not aligned, this would be used for Language Translation where we input a sequence and output a sequence\n",
    "    - If Input/Output are aligned, then this can be Video Classification where we want to classify each frame but use the information in previous frames\n",
    "    \n",
    "    \n",
    "## Lets Start Putting it Together!\n",
    "\n",
    "Theres going to be a couple of new ideas here but we will get through it step by step! In our [PyTorch Dataloader](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20DataLoaders) we made a character level DataLoader, and we talked about how there are other options, but again, to keep it simple, lets do a word level model. We will just split our datatset by words and then remove the least occuring words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aa6d19f-9625-42f6-8bb0-4271a74a9f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "# nltk.download('stopwords') #Download the NLTK Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee6859a-ef2f-4040-8c2f-f4a59ec1af56",
   "metadata": {},
   "source": [
    "## Lets Take a Peek at the Dataset Again\n",
    "\n",
    "We want to find out what words we want to keep, how we want to split it, and just a general analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dbb5b6b-ef20-4c2b-8934-4f9996ce1cf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sentence Length 124.19728\n"
     ]
    }
   ],
   "source": [
    "### Load Path to Files ###\n",
    "path_to_data = \"../../data/aclImdb/train\"\n",
    "\n",
    "path_to_pos_fld = os.path.join(path_to_data, \"pos\")\n",
    "path_to_neg_fld = os.path.join(path_to_data, \"neg\")\n",
    "\n",
    "path_to_pos_txt = [os.path.join(path_to_pos_fld, file) for file in os.listdir(path_to_pos_fld)]\n",
    "path_to_neg_txt = [os.path.join(path_to_neg_fld, file) for file in os.listdir(path_to_neg_fld)]\n",
    "\n",
    "training_files = path_to_pos_txt + path_to_neg_txt\n",
    "\n",
    "\n",
    "### Easy Text Analysis ###\n",
    "alltxt = []\n",
    "len_words = []\n",
    "for file in training_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        text = f.readlines()[0].lower() # Convert to LowerCase\n",
    "        text = re.sub(r'[^\\w\\s]', '', text) # Remove All Punctuation\n",
    "        text = text.split(\" \") # Split by Space\n",
    "        text = [word for word in text if (word not in stopwords) and (len(word) > 0)] # Remove all stopwords and empty strings\n",
    "        len_words.append(len(text))\n",
    "        alltxt += text\n",
    "    \n",
    "print(\"Average Sentence Length\", np.mean(len_words))\n",
    "\n",
    "### GET COUNT OF UNIQUE WORDS ###\n",
    "unique_counts = dict(Counter(alltxt))\n",
    "words = sorted([key for (key,value) in unique_counts.items() if value > 500])\n",
    "\n",
    "words.append(\"<UNK>\")\n",
    "words.append(\"<PAD>\")\n",
    "\n",
    "word2idx = {c:i for i,c, in enumerate(words)}\n",
    "idx2char = {i:c for i,c in enumerate(words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c32ea78-57cd-47fe-ba26-ba3756980898",
   "metadata": {},
   "source": [
    "### Build IMDB Dataset\n",
    "\n",
    "We want to follow the same preprocessing steps we did above to make sure its accurate. We will fill words that are not in our tokenizer with Unknown tokens. This will follow our"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d16a4956-1f14-46fb-a77f-153fae90b2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72])\n",
      "1\n",
      "tensor([[989, 851, 989,  ..., 990, 990, 990],\n",
      "        [939,  78, 487,  ..., 990, 990, 990],\n",
      "        [ 62, 772,  62,  ..., 990, 990, 990],\n",
      "        ...,\n",
      "        [989, 989, 989,  ..., 990, 990, 990],\n",
      "        [989,  95, 989,  ..., 990, 990, 990],\n",
      "        [439, 932, 261,  ..., 989, 989, 866]])\n",
      "tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, path_to_data, word2idx, max_seq_len=200):\n",
    "        path_to_pos_fld = os.path.join(path_to_data, \"pos\")\n",
    "        path_to_neg_fld = os.path.join(path_to_data, \"neg\")\n",
    "\n",
    "        path_to_pos_txt = [os.path.join(path_to_pos_fld, file) for file in os.listdir(path_to_pos_fld)]\n",
    "        path_to_neg_txt = [os.path.join(path_to_neg_fld, file) for file in os.listdir(path_to_neg_fld)]\n",
    "\n",
    "        self.training_files = path_to_pos_txt + path_to_neg_txt\n",
    "        self.tokenizer = word2idx\n",
    "        self.max_len = max_seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.training_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path_to_txt = self.training_files[idx]\n",
    "        \n",
    "        ### PREP TEXT ###\n",
    "        with open(path_to_txt, \"r\") as f:\n",
    "            txt = f.readlines()[0].lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', txt) \n",
    "        text = text.split(\" \") \n",
    "        text = [word for word in text if (word not in stopwords) and (len(word) > 0)]\n",
    "        \n",
    "        tokenized = []\n",
    "        for word in text:\n",
    "            if word in self.tokenizer.keys():\n",
    "                tokenized.append(self.tokenizer[word])\n",
    "            else:\n",
    "                tokenized.append(self.tokenizer[\"<UNK>\"])\n",
    "                \n",
    "        sample = torch.tensor(tokenized)\n",
    "        \n",
    "        if len(sample) > self.max_len:\n",
    "            diff = len(sample) - self.max_len\n",
    "            start_idx = np.random.randint(diff)\n",
    "            sample = sample[start_idx:start_idx+self.max_len]\n",
    "\n",
    "        ### GRAB CLASS LABEL ###\n",
    "        if \"neg\" in path_to_txt:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "            \n",
    "        return sample, label\n",
    "        \n",
    "                \n",
    "imdbdataset = IMDBDataset(\"../../data/aclImdb/train/\", word2idx)\n",
    "\n",
    "### Check Dataset Works ###\n",
    "counter = 0\n",
    "for data, label in imdbdataset:\n",
    "    print(data.shape)\n",
    "    print(label)\n",
    "    break\n",
    "    \n",
    "\n",
    "### Write Custom Collator ###\n",
    "def data_collator(batch):\n",
    "    texts, labels = [], []\n",
    "    \n",
    "    for text, label in batch:\n",
    "        labels.append(label)\n",
    "        texts.append(text)\n",
    "        \n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    ### Pad the list of sequences and then convert to tensor like example above but with our padding token <PAD> ###\n",
    "    texts = nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=word2idx[\"<PAD>\"])\n",
    "    return texts, labels   \n",
    "\n",
    "\n",
    "imdbloader = DataLoader(imdbdataset, batch_size=16, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "counter = 0\n",
    "for text, label in imdbloader:\n",
    "    print(text)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac9a1a-5950-4fdc-badf-4e7177090d7c",
   "metadata": {},
   "source": [
    "Our Dataset has now been converted to a series of numbers, but we still have a small issue. These numbers don't really mean anything... We just randomly assigned some integer value to each of these words but that doesn't mean they can express the meaning of the word. Therefore we have to use something called an Embedding!\n",
    "\n",
    "### Embedding \n",
    "Embeddings are numerical representation of some concept. In our case specifically, we want to represent a specific word with a vector of lenght 256 (arbritrary value that I picked, you can use much larger embedding dimensions). Our goal during the training process is to have words that are similar in meaning to have vectors that are closer together in the high dimensional space. For example, we want words like, Bad, Horrible, and Terrible to be closer together but far apart from words like Good, Amazing, Incredible. To do this we can use something known as a PyTorch Embedding!\n",
    "\n",
    "The embedding we want will have a row for each unique word in our corpus (along with unknown and padding) and the width will be our embedding dimension. This will be a simple lookup table, where if we want the embedding that goes with the word labeled with the index 3, then we grab the 3rd row of the embedding table. Lets try this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87ebe663-0cce-4291-9c26-124b6462c58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Weights\n",
      "Parameter containing:\n",
      "tensor([[ 1.0189,  0.6884, -0.4416],\n",
      "        [-2.1344,  0.3721, -0.6451],\n",
      "        [-0.9479,  0.5939, -0.5694],\n",
      "        [ 0.1539, -0.9950,  0.7292],\n",
      "        [ 0.2337,  1.2697, -1.2401]], requires_grad=True)\n",
      "Embedding for Single Sentence\n",
      "tensor([[-2.1344,  0.3721, -0.6451],\n",
      "        [ 0.1539, -0.9950,  0.7292]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([2, 3])\n",
      "Embedding for Batch Sentence\n",
      "tensor([[[-2.1344,  0.3721, -0.6451],\n",
      "         [ 0.1539, -0.9950,  0.7292]],\n",
      "\n",
      "        [[-2.1344,  0.3721, -0.6451],\n",
      "         [ 0.1539, -0.9950,  0.7292]],\n",
      "\n",
      "        [[-2.1344,  0.3721, -0.6451],\n",
      "         [ 0.1539, -0.9950,  0.7292]]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "emb = nn.Embedding(5,3)\n",
    "\n",
    "print(\"Embedding Weights\")\n",
    "print(emb.weight)\n",
    "\n",
    "print(\"Embedding for Single Sentence\")\n",
    "sentence = torch.tensor([1, 3]) # Sentence words as a list of numbers\n",
    "print(emb(sentence))\n",
    "print(emb(sentence).shape)\n",
    "\n",
    "print(\"Embedding for Batch Sentence\")\n",
    "batch_sentences = torch.tensor([[1,3], [1,3], [1,3]])\n",
    "print(emb(batch_sentences))\n",
    "print(emb(batch_sentences).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46586536-0410-4522-a1db-dc9e098b08de",
   "metadata": {},
   "source": [
    "Also note that the weights of our embeddings have a requires_grad is True. That means this weight matrix will be updated via gradient descent over time to hopefully give more meaningful embeddings for each word. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2111b025-dcd2-4eda-8857-cfd13c31b833",
   "metadata": {},
   "source": [
    "### Explore the LSTM Module\n",
    "\n",
    "```\n",
    "nn.LSTM(input_size,  # Expected number of features per input (in our case it will be the embedding depth)\n",
    "        hidden_size, # Number of features in Hidden State\n",
    "        num_layers,  # Number of LSTM Cells we want to stack\n",
    "        batch_first, # Will our tensor have batch dimension or sequence dimension first\n",
    ")\n",
    "```\n",
    "#### Inputs to the LSTM\n",
    "- $H_0$: Num Layers x batch_size x Hidden Size -> Initialized as 0 if no information given\n",
    "- $C_0$: Num Layers x batch_size x Hidden Size -> Initialized as 0 if no information given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ab714a3-db73-49b9-8d53-c6c67ea6bd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([5, 15, 20])\n",
      "Final H: torch.Size([2, 5, 20])\n",
      "Final c: torch.Size([2, 5, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5        # How Many Samples\n",
    "sequence_length = 15  # Sequence Length Per Sample\n",
    "input_size = 10       # Dimension of vector for each timestep in sequence per sample      \n",
    "hidden_size = 20      # Dimension expansion from Input size Inside the LSTM cell\n",
    "num_layers = 2        # Number of LSTM Cells\n",
    "\n",
    "\n",
    "lstm = nn.LSTM(input_size=input_size,\n",
    "               hidden_size=hidden_size, \n",
    "               num_layers=num_layers,\n",
    "               batch_first=True)\n",
    "\n",
    "\n",
    "\n",
    "rand = torch.rand(batch_size, sequence_length, input_size) # Batch x sequence length x input_size\n",
    "h0 = torch.zeros(num_layers, batch_size, hidden_size)      # Num Layers x Batch Size x Hidden State\n",
    "c0 = torch.zeros(num_layers, batch_size, hidden_size)      # Num Layers x Batch Size x Hidden State\n",
    "\n",
    "\n",
    "output, (hn, cn) = lstm(rand, (h0, c0))\n",
    "\n",
    "print(\"Output:\", output.shape) # Returns Batch Size x Sequence Length x Hidden Size -> Hidden state for each timestep\n",
    "print(\"Final H:\", hn.shape)    # Returns Num Layers x Batch Size x Hidden Size -> Last Hidden state for every layer\n",
    "print(\"Final c:\", cn.shape)    # Returns Num Layers x Batch Size x Hidden Size -> Last Hidden state for every layer\n",
    "\n",
    "\n",
    "hn[-1][0] == output[0][-1] # Check if the Last layer, first sample of Hn has same hidden size as first sample, last timestep of output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9a85e6-25bf-4fcf-ab0d-33074fbb7a78",
   "metadata": {},
   "source": [
    "### We Can Now Build the Model!\n",
    "Our model essentially will take in a batch of indexes that reference a word in our tokenzier which we will use to grab the cooresponding vector from our embedding matrix. This will then be passed to an LSTM module. Becuase we only care about a single prediction (**Many to One**) we will only use the last part of the output and then a linear layer to map to our two Positive/Negative classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2154ab7-a6df-44e2-a8e4-a79a0ef13f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_classes, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        ### Define Embedding Matrix ###\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        \n",
    "        ### Define LSTM ###\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim,\n",
    "                            hidden_size=self.hidden_size, \n",
    "                            num_layers=self.num_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        ### Final Classifier Layers ###\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, sequence_len = x.shape\n",
    "        embeddings = self.embedding(x) # (Batch x Sequence Len x Embedding Dim)\n",
    "        \n",
    "        ### INITIALIZE HIDDEN AND CELL STATE AS 0 ###\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(self.device)  # Num Layers x Batch Size x Hidden State\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(self.device)  # Num Layers x Batch Size x Hidden State\n",
    "        \n",
    "        ### PASS THROUGH LSTM BLOCK ###\n",
    "        output, (hn, cn) = self.lstm(embeddings, (h0,c0)) \n",
    "        \n",
    "        # Output -> [batch x seqlen x hidden]\n",
    "        # Hn -> [num_layers x batch x hidden]\n",
    "        # Cn -> [num_layers x batch x hidden]\n",
    "        \n",
    "        ### CUT OFF LAST HIDDEN STATE FOR EVERY BATCH ###\n",
    "        last_hidden  = output[:, -1, :] # Batch Size x Hidden\n",
    "        \n",
    "        out = self.dropout(last_hidden)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba8c867-edf9-4ffc-ba74-c1a36c4f282e",
   "metadata": {},
   "source": [
    "### Train Model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae6db960-1f9a-4a08-a6e1-8a80a0973e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build Training and Testing Dataset ###\n",
    "train_dataset = IMDBDataset(\"../../data/aclImdb/train/\", word2idx)\n",
    "test_dataset = IMDBDataset(\"../../data/aclImdb/test/\", word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac835726-38f4-48d4-a8d6-a057218681f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Device cuda\n",
      "Starting Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [00:14<00:00, 13.57it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [00:13<00:00, 14.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6955276779374298\n",
      "Training Acc: 0.5028380102344921\n",
      "Validation Loss: 0.6927748179557373\n",
      "Validation Acc: 0.5074856506318463\n",
      "Starting Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [00:14<00:00, 13.90it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [00:10<00:00, 18.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6922682012830462\n",
      "Training Acc: 0.5105389031220455\n",
      "Validation Loss: 0.6916172191196558\n",
      "Validation Acc: 0.5109693879375652\n",
      "Starting Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [00:13<00:00, 14.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [00:10<00:00, 18.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6918516779432491\n",
      "Training Acc: 0.5174744898567394\n",
      "Validation Loss: 0.69275628820974\n",
      "Validation Acc: 0.5064811861636688\n",
      "Starting Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [00:14<00:00, 13.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [00:10<00:00, 18.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6920451056592318\n",
      "Training Acc: 0.5158083545310157\n",
      "Validation Loss: 0.6983560901515338\n",
      "Validation Acc: 0.5048469387146891\n",
      "Starting Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [00:14<00:00, 13.67it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [00:10<00:00, 18.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6899120719457159\n",
      "Training Acc: 0.5197066327138823\n",
      "Validation Loss: 0.6844985618883249\n",
      "Validation Acc: 0.5227758290664274\n",
      "Starting Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [00:13<00:00, 14.26it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [00:10<00:00, 18.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5969882321601011\n",
      "Training Acc: 0.6920998087342904\n",
      "Validation Loss: 0.49051396457516416\n",
      "Validation Acc: 0.7688376912657096\n",
      "Starting Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [00:13<00:00, 14.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [00:10<00:00, 17.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.45938669905370594\n",
      "Training Acc: 0.8019850129375652\n",
      "Validation Loss: 0.4040750713676822\n",
      "Validation Acc: 0.822265625\n",
      "Starting Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [00:14<00:00, 13.90it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [00:10<00:00, 18.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.38237592083763106\n",
      "Training Acc: 0.837404336856336\n",
      "Validation Loss: 0.36495799791752076\n",
      "Validation Acc: 0.8413345026118415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### SELECT DEVICE ###\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on Device {DEVICE}\")\n",
    "\n",
    "### LOAD IN and Modify AlexNet Model ###\n",
    "model = LSTMNet(vocab_size=len(word2idx),\n",
    "                embedding_dim=128,\n",
    "                hidden_size=256, \n",
    "                num_layers=1, \n",
    "                num_classes=2, \n",
    "                device=DEVICE)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "### MODEL TRAINING INPUTS ###\n",
    "epochs = 8\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "batch_size = 128\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=data_collator)\n",
    "valloader = DataLoader(test_dataset, batch_size=128, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "def train(model, device, epochs, optimizer, loss_fn, batch_size, trainloader, valloader):\n",
    "    log_training = {\"epoch\": [],\n",
    "                    \"training_loss\": [],\n",
    "                    \"training_acc\": [],\n",
    "                    \"validation_loss\": [],\n",
    "                    \"validation_acc\": []}\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Starting Epoch {epoch}\")\n",
    "        training_losses, training_accuracies = [], []\n",
    "        validation_losses, validation_accuracies = [], []\n",
    "        \n",
    "        model.train() # Turn On BatchNorm and Dropout\n",
    "        for image, label in tqdm(trainloader):\n",
    "            image, label = image.to(DEVICE), label.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            out = model.forward(image)\n",
    "        \n",
    "            ### CALCULATE LOSS ##\n",
    "            loss = loss_fn(out, label)\n",
    "            training_losses.append(loss.item())\n",
    "\n",
    "            ### CALCULATE ACCURACY ###\n",
    "            predictions = torch.argmax(out, axis=1)\n",
    "            accuracy = (predictions == label).sum() / len(predictions)\n",
    "            training_accuracies.append(accuracy.item())\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            ### Just Incase of Exploding Gradients\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval() # Turn Off Batchnorm \n",
    "        for image, label in tqdm(valloader):\n",
    "            image, label = image.to(DEVICE), label.to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                out = model.forward(image)\n",
    "\n",
    "                ### CALCULATE LOSS ##\n",
    "                loss = loss_fn(out, label)\n",
    "                validation_losses.append(loss.item())\n",
    "\n",
    "                ### CALCULATE ACCURACY ###\n",
    "                predictions = torch.argmax(out, axis=1)\n",
    "                accuracy = (predictions == label).sum() / len(predictions)\n",
    "                validation_accuracies.append(accuracy.item())\n",
    "\n",
    "        training_loss_mean, training_acc_mean = np.mean(training_losses), np.mean(training_accuracies)\n",
    "        valid_loss_mean, valid_acc_mean = np.mean(validation_losses), np.mean(validation_accuracies)\n",
    "\n",
    "        log_training[\"epoch\"].append(epoch)\n",
    "        log_training[\"training_loss\"].append(training_loss_mean)\n",
    "        log_training[\"training_acc\"].append(training_acc_mean)\n",
    "        log_training[\"validation_loss\"].append(valid_loss_mean)\n",
    "        log_training[\"validation_acc\"].append(valid_acc_mean)\n",
    "\n",
    "        print(\"Training Loss:\", training_loss_mean) \n",
    "        print(\"Training Acc:\", training_acc_mean)\n",
    "        print(\"Validation Loss:\", valid_loss_mean)\n",
    "        print(\"Validation Acc:\", valid_acc_mean)\n",
    "        \n",
    "    return log_training, model\n",
    "\n",
    "\n",
    "training_logging, model = train(model=model,\n",
    "                                device=DEVICE,\n",
    "                                epochs=epochs,\n",
    "                                optimizer=optimizer,\n",
    "                                loss_fn=loss_fn,\n",
    "                                batch_size=batch_size,\n",
    "                                trainloader=trainloader,\n",
    "                                valloader=valloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac07acd9-16d9-47d8-b35d-6a969e219c05",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Limitations of the LSTM/RNN Architecture\n",
    "\n",
    "The main downfall of this method is the training time on long sequences. For us to pass the 100th token of a sequence into our model, we first need to pass in the previous 99. This means that the entire architecture has no way to be parallelized and is inherently slow. Regardless, it is a crucial prerequisite you need to understand before we start to tackle Transformers in the future that solve many of these issues!!\n",
    "\n",
    "That brings to an end our introduction to Sequence Classification! There is definitely a lot of moving parts here, and although the LSTM internaly is extremely complex, using these tools isnt too bad! Now that we have completed our **Many to One** model, in the next lesson we will do a **Many to Many** model for character level Harry Potter Generation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
