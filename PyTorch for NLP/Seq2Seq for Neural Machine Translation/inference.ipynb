{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28120aca-deea-4331-8c43-f8ba4b3159dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import evaluate\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "from model import Transformer, TransformerConfig\n",
    "from tokenizer import FrenchTokenizer\n",
    "\n",
    "path_to_model_safetensor = \"work_dir/Seq2Seq_Neural_Machine_Translation/checkpoint_150000/model.safetensors\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467a72fd-ba26-45bd-98c7-aea2ad68f568",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc4d42c5-9199-42fe-a229-b71ab19d1b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Model ###\n",
    "config = TransformerConfig()\n",
    "model = Transformer(config)\n",
    "\n",
    "### Load Pretrained Weights ###\n",
    "weight_dict = load_file(path_to_model_safetensor)\n",
    "model.load_state_dict(weight_dict)\n",
    "model.eval()\n",
    "\n",
    "### Load Tokenizers ###\n",
    "tgt_tokenizer =  FrenchTokenizer(\"trained_tokenizer/french_wp.json\")\n",
    "src_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0bab62-a0e4-49fc-82e1-d62ef58e52da",
   "metadata": {},
   "source": [
    "### Lets try to Translate Something!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b84be2be-2c65-4f1e-886c-7bc2b49ae2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Translation: je veux apprendre quelque chose de nouveau ce soir!\n"
     ]
    }
   ],
   "source": [
    "### Predict Translation from English ###\n",
    "sample_sentence = \"I want to learn something new tonight!\"\n",
    "\n",
    "src_ids = torch.tensor(src_tokenizer(sample_sentence)[\"input_ids\"][:config.max_src_len]).unsqueeze(0)\n",
    "translated = model.inference(src_ids, \n",
    "                             tgt_start_id=tgt_tokenizer.special_tokens_dict[\"[BOS]\"],\n",
    "                             tgt_end_id=tgt_tokenizer.special_tokens_dict[\"[EOS]\"])\n",
    "prediction = tgt_tokenizer.decode(translated, skip_special_tokens=True)\n",
    "\n",
    "print(\"French Translation:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a93c385-b175-4958-9c12-33124492e16a",
   "metadata": {},
   "source": [
    "So I dont know a word of french, but we can try to drop our french prediction into Google Translate and see what we get in English:\n",
    "\n",
    "<img src=\"src/google_translate.png\"  width=\"500\"/>\n",
    "\n",
    "Looks like it worked! Not too bad for our model at all!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
