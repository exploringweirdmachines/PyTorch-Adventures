{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f742921-b236-4aad-a3d7-c5c2ae5efa8e",
   "metadata": {},
   "source": [
    "![banner](https://raw.githubusercontent.com/priyammaz/HAL-DL-From-Scratch/main/src/visuals/banner.png)\n",
    "\n",
    "# Generative Pretrained Transformers\n",
    "\n",
    "Today we will be implementing the language model that has taken the world by storm: Generative Pretrained Transformers, or GPT! The architecture for this model is actually not too complicated, and not all that different from previous implementations we have done like the [Vision Transformer](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20for%20Computer%20Vision/Vision%20Transformer). As a recap, there are two types of transformers:\n",
    "\n",
    "Some helpful sources!\n",
    "- [NanoGPT](https://github.com/karpathy/nanoGPT) was extremely helpful to learn about how these models function, so a lot of this will be similar to the code found there, just with some of my own visuals so I can better explain the components of attention and causal langauge modeling.\n",
    "- [Huggingface](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py) was also really helpful, between their GPT as a reference and their general tutorials for language models.\n",
    "\n",
    "### Encoders (RoBERTa, ViT)\n",
    "Encoder transformers have the ability to look at the entire sequence and are bidirectional. This means when we compute attention, words can look forward and backwards, which is perfect for situations where we have the entire sequence given and we want to make some predictions about it (i.e. image classification, sentiment analysis, speech recognition, etc...)\n",
    "\n",
    "### Decoders (GPT)\n",
    "Decoder transformers only have the ability to look at the past, meaning any prediction we make for a specific time in the sequence is based only on times that came before it. This is important for tasks like time-series forecasting or language generation. When we want to generate a sequence, we can only look at the words that have come already as the future words aren't available. \n",
    "\n",
    "### Language Pre-Training\n",
    "There are typically two types of language pretraining. Encoders use **Masked Language Modeling**. This is the process of taking a sentence, randomly removing words, and then having the model predict the missing words by looking at existing words before and after it. Decoders use **Causal Language Modeling**, where we will train a model to predict the word at time $n+1$ give all the words 1 to $n$. Today we will be looking at Causal Language Models as that is what GPT uses for pretraining. \n",
    "\n",
    "### Training vs. Inference\n",
    "Something to keep in mind when we train a Causal Language Model is, during training time, we actually have the entire sentence available to us, but during inference, we only have the input sentence and we will generate based on that. For the input sentence during training, we will have to do some type of masking to enforce causal language modeling, to make sure that future words can **ONLY LOOK AT THE PAST**. Lets take a quick look at what that would look like! Remember, the **Attention Mechanism** computes weights that signify how every word in the sentence is related to every other word. Therefore, if you have $n$ words in your sentence, you will have an $n x n$ attention matrix. \n",
    "\n",
    "Lets take the sentence: **Deep Learning is fun**\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/priyammaz/HAL-DL-From-Scratch/blob/main/src/visuals/causal_masking.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "- **Deep** can only look at itself, and not any future words\n",
    "- **Learning** can look at itself and the past word **Deep**\n",
    "- **is** can look at itself and the past words **Deep** and **Learning**\n",
    "- **fun** can look at itself and all the past words.\n",
    "\n",
    "Without the masking, this attention matrix would be a regular encoder. If **Deep** could look at the future words **learning**, **is** and **fun**, then we break causality, we want to ensure that for every word in the sentence, we are only looking at the past to predict the next. (i.e at word **n** we want to predict what the next word at **n+1** is based on all words from 1 to **n-1**). Unfortunately this is cheating, the whole purpose of the model is to predict future words, so if we are allowing the model to see the future words that it needs to predict, then the model wont learn anything but to just copy the future words for predictions.\n",
    "\n",
    "### How to do Attention Masking\n",
    "\n",
    "So how do we actually perform attention masking? Lets first remind ourselves what attention is doing. Here is the equation for Attention as a reminder:\n",
    "\n",
    "$$\\text{Attention}(Q,K,V) = \\text{Softmax}(\\frac{QK^T}{\\sqrt{d_e}})V$$\n",
    "\n",
    "So the first step is the computing the $QK^T$, where Q and K both have the shape (Sequence Length x Embedding Dimension). The output of this computation will be sequence length x sequence length. This is what it looks like!\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/priyammaz/HAL-DL-From-Scratch/blob/main/src/visuals/computing_attention.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "In the image above, I also applied the softmax (not shown for simplicity), so each row of the attention matrix adds up to 1 (like probabilities).\n",
    "\n",
    "Now we can multiply our output of $QK^T$ with our $V$. This is what a regular encoder (bidirectional) attention will look like. Remember, $V_1, ... V_4$ are the projection vectors (Values) of the data, and the attention matrix is a weighted average of all of these vectors. \n",
    "\n",
    "**Note**\n",
    "\n",
    "In transformers, our input $X$ goes through 3 different linear projections to create $Q, K, \\text{and} V$ Initially, the each vector for each word in $X$ is the embedding vector representing that word. After the attention computation, each vector for each word isn't just the embedding of that word but rather a weighted average of all the vectors in the sequence and how they are related to the word of interest. \n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/priyammaz/HAL-DL-From-Scratch/blob/main/src/visuals/encoder_attention_vis.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "Whats the problem above? The first row vector in the output $0.2V_1 + 0.1V_2 + 0.4V_3 + 0.3V_4$ is a weighted average of the entire sequence (therefore getting information from future vectors). This is again cheating, so we need to mask our attention matrix, and set the cases of future words, in comparison to the word of interest, are set to a weight of 0. The word of interest and previous words and then reweighted to add up to 1. Therefore, we are only learning how every word is related to itself and the past! \n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/priyammaz/HAL-DL-From-Scratch/blob/main/src/visuals/decoder_attention_vis.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "### Computing the Reweighted Causal Attention Mask\n",
    "\n",
    "Lets pretend the raw outputs of $QK^T$, before the softmax, is below:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "  7       & -8   & 6  \\\\\n",
    "  -3       & 2   & 4   \\\\\n",
    "  1       & 6  & -2   \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Remember, the equation for softmax is:\n",
    "\n",
    "$$\\text{Softmax}(\\vec{x}) = \\frac{e^{x_i}}{\\sum_{j=1}^N{e^{x_j}}}$$\n",
    "\n",
    "Then, we can compute softmax for row of the matrix above:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Softmax}\n",
    "\\begin{bmatrix}\n",
    "  7       & -8   & 6  \\\\\n",
    "  -3       & 2   & 4   \\\\\n",
    "  1       & 6  & -2   \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "  \\frac{e^{7}}{e^{7}+e^{-8}+e^{6}}       & \\frac{e^{-8}}{e^{7}+e^{-8}+e^{6}}   & \\frac{e^{6}}{e^{7}+e^{-8}+e^{6}}  \\\\\n",
    "  \\frac{e^{-3}}{e^{-3}+e^{2}+e^{4}}       & \\frac{e^{2}}{e^{-3}+e^{2}+e^{4}}   & \\frac{e^{4}}{e^{-3}+e^{2}+e^{4}}  \\\\\n",
    "  \\frac{e^{1}}{e^{1}+e^{6}+e^{-2}}       & \\frac{e^{6}}{e^{1}+e^{6}+e^{-2}}   & \\frac{e^{-2}}{e^{1}+e^{6}+e^{-2}}  \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "  0.73       & 0.0000002   & 0.27   \\\\\n",
    "  0.0008       & 0.12   & 0.88 \\\\\n",
    "  0.007       & 0.99  & 0.003  \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "But, what we want, is the top triangle to have weights of 0, and the rest adding up to 1. So lets take the second vector in the matrix above to see how we can do that. \n",
    "\n",
    "$$x_2 = [-3, 2, 4]$$\n",
    "\n",
    "Because this is the second vector, we need to zero out the softmax output for everything after the second index (so in our case just the last value). Lets replace the value 4 by $-\\infty$. Then we can write it as:\n",
    "\n",
    "$$x_2 = [-3, 2, -\\infty]$$\n",
    "\n",
    "Lets now take softmax of this vector!\n",
    "\n",
    "$$\\text{Softmax}(x_2) = [\\frac{e^{-3}}{e^{-3}+e^{2}+e^{-\\infty}}, \\frac{e^{2}}{e^{-3}+e^{2}+e^{-\\infty}}, \\frac{e^{-\\infty}}{e^{-3}+e^{2}+e^{-\\infty}}]$$\n",
    "\n",
    "Remember, $e^{-\\infty}$ is equal to 0, so we can solve solve this!\n",
    "\n",
    "$$\\text{Softmax}(x_2) = [\\frac{e^{-3}}{e^{-3}+e^{2}+0}, \\frac{e^{2}}{e^{-3}+e^{2}+0}, \\frac{0}{e^{-3}+e^{2}+0}] = [\\frac{e^{-3}}{e^{-3}+e^{2}+0}, \\frac{e^{2}}{e^{-3}+e^{2}+0}, \\frac{0}{e^{-3}+e^{2}+0}] = [0.0067, 0.9933, 0.0000]$$\n",
    "\n",
    "So we have exactly what we want! The attention weight of the last value is set to 0, so when we are on the second vector $x_2$, we cannot look forward to the future value vectors $v_3$, and the remaining parts add up to 1 so its still a probability vector! To do this correctly for the entire matrix, we can just substitute in the top triangle of $QK^T$ with $-\\infty$. This would look like:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "  7       & -\\infty   & -\\infty  \\\\\n",
    "  -3       & 2   & -\\infty   \\\\\n",
    "  1       & 6  & -2   \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Taking the softmax of the rows of this matrix then gives:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Softmax}\n",
    "\\begin{bmatrix}\n",
    "  7       & -\\infty   & -\\infty  \\\\\n",
    "  -3       & 2   & -\\infty   \\\\\n",
    "  1       & 6  & -2   \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "  1       & 0   & 0  \\\\\n",
    "  0.0067  & 0.9933 & 0   \\\\\n",
    "  0.007       & 0.99  & 0.003   \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "### Thats It!\n",
    "\n",
    "For the most part, this is what powers GPT (along with buckets of data and giant models). The implementation isn't all that different from what we did for the Vision Transformer or any other transformer architecture, we will just have to include this extra piece of masking the attention matrix. \n",
    "\n",
    "### Lets Implement It\n",
    "\n",
    "Typically you have to train GPT on massive datasets, but just to get a feel for it, lets do it for our Harry Potter text. This should be pretty similar to the [LSTM for Harry Potter Generation](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20for%20NLP/LSTM/LSTM%20Harry%20Potter%20Generation) with the difference being, we will do subword tokenization instead of character level tokenization. Definitely take a look at that before you look at this, but at a high level there are typically three types of tokenization:\n",
    "\n",
    "- Character Level: We will have an embedding vector for each unique character in the data (typically alphanumeric characters and punctuation)\n",
    "- Sub-Word Level: We will have embeddings vectors for parts of words, i.e the word \"Learning\" may be split into \"Learn\" and \"ing\". This is good because subwords can be reused quite a bit across words\n",
    "- Word Level: We will have embedding vectors for all unique words, this is expensive because of how many unique words there are.\n",
    "\n",
    "\n",
    "First step, take all the text in the Harry Potter dataset, put it into a single long string, and then tokenize all of it using the GPT2 Tokenizer from huggingface. You could really use any tokenizer you want, but we will go with this for now! The tokens in this case will be unique identifiers of the sub-words, and then our GPT model will later have an embedding matrix to convert indexes for tokens to vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "243da8cf-8def-49e8-8a52-e83caf8b1e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import GPT2TokenizerFast\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f31320f-805f-481b-9a49-50a0da4384a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Vocab Size: 50257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1649226 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 1649226\n",
      "Example Tokens\n",
      "[14, 3336, 16494, 56, 19494, 406, 3824, 1961, 1770, 13, 290, 9074, 13, 360, 1834, 1636, 11, 286, 1271, 1440, 11, 4389, 16809, 9974, 11, 547, 6613, 284, 910, 326]\n"
     ]
    }
   ],
   "source": [
    "### Put All Text Together to Sample From ###\n",
    "path_to_data = \"../../data/harry_potter_txt/\"\n",
    "\n",
    "text_files = os.listdir(path_to_data)\n",
    "\n",
    "all_text = \"\"\n",
    "for book in text_files:\n",
    "    with open(os.path.join(path_to_data, book), \"r\") as f:\n",
    "        text = f.readlines() # Read in all lines\n",
    "        text = [line for line in text if \"Page\" not in line] # Remove lines with Page Numbers\n",
    "        text = \" \".join(text).replace(\"\\n\", \"\") # Remove all newline characters\n",
    "        text = [word for word in text.split(\" \") if len(word) > 0] # Remove all empty characters\n",
    "        text = \" \".join(text) # Combined lightly cleaned text\n",
    "        all_text += text\n",
    "\n",
    "### Tokenize all Data ###\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "print(\"Tokenizer Vocab Size:\", tokenizer.vocab_size)\n",
    "\n",
    "tokenized_data = tokenizer(all_text)[\"input_ids\"]\n",
    "print(\"Number of Tokens:\", len(tokenized_data))\n",
    "\n",
    "print(\"Example Tokens\")\n",
    "print(tokenized_data[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b48000-0573-4d84-9ede-285cf7179bbf",
   "metadata": {},
   "source": [
    "### Writing a DataLoader\n",
    "\n",
    "There are two main things to remember in our dataloader:\n",
    "\n",
    "- Context Length: How many tokens can the model understand at a time. If you remember, the LSTM model from before can technically take **ANY** sequence lengths, with the penalty of the model forgetting earlier parts of the sequence as it gets to the end. Transformers are set with a specific sequence length up front, which is our context length, or how many tokens we learn to process at a time.\n",
    "- Autoregressive Data: Lets pretend we have a sequence of tokens [10,2,4,6,8]. The input to the model will be [10,2,4,6], and for each token we will predict [2,4,6,8]. Therefore, if we want a context lenght of 300, we will grab 301 words, so words 1 to 300 will be the input, and 2 to 301 will be what we want to predict.\n",
    "\n",
    "\n",
    "### Caveat\n",
    "\n",
    "One of the typical datasets that GPT is trained on is OpenWebText. Something interesting about these datasets is most of the sentences in the data is pretty short. You may have seen a lot of efforts in building long context lengths models (GPT4 has a 128,000 token context length model now). The only way to train these models is also having long context data, where sequences are entire books worth of information.\n",
    "\n",
    "In the case of OpenWebText, you dont have these long sequences, so something you will have to do is take all the short sequences, and then concatenate them together to reach the length you want. The problem with this is, the second sequence has nothing to do with the first, so you have to add special tokens between sequences as such:\n",
    "\n",
    "- Sequence 1: Hello, my name is Priyam\n",
    "- Sequence 2: Deep learning is really awesome\n",
    "- Sequence 3: The weather is really nice tomorrow\n",
    "\n",
    "When we put them together we can do something like:\n",
    "\n",
    "Hello, my name is Priyam [EOS] Deep learning is really awesome [EOS] The weather is really nice tomorrow [EOS]. \n",
    "\n",
    "This way the model can learn where sentences end and they act almost like a page break, so it has less incentive to learn how words between sequences are related to each other. Im not totally convinced this is the best way to be honest, and the best way is to train on long sequence data, like the [Gutenberg Book Corpus](https://www.gutenberg.org/) because as long as you grab long sequences from a book, they are always consecutive and related to each other, there's no need for <EOS> tokens. For the same reason, we are training on Harry Potter books, so except for the 6 cases of a book ending and a new book starting, we have no [EOS] so we can ignore it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eff12ee0-630d-4495-bb3f-1d0171c6bb69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: torch.Size([2, 5])\n",
      "tensor([[  83,    0,  447,  251,  564],\n",
      "        [ 247, 7081,   11,  314,  447]])\n",
      "Label Text: torch.Size([2, 5])\n",
      "tensor([[   0,  447,  251,  564,  250],\n",
      "        [7081,   11,  314,  447,  247]])\n"
     ]
    }
   ],
   "source": [
    "class DataBuilder:\n",
    "    def __init__(self, seq_len=300, tokenized_text=tokenized_data):\\\n",
    "\n",
    "        self.seq_len = seq_len + 1\n",
    "        self.tokenized_text = tokenized_text\n",
    "        self.file_length = len(tokenized_text)\n",
    "        \n",
    "    def grab_random_sample(self):\n",
    "        start = np.random.randint(0, len(self.tokenized_text) - self.seq_len)\n",
    "        end = start + self.seq_len\n",
    "        text_slice = self.tokenized_text[start:end]\n",
    "\n",
    "        input_text = torch.tensor(text_slice[:-1])\n",
    "        label = torch.tensor(text_slice[1:])\n",
    "        \n",
    "        return input_text, label\n",
    "    \n",
    "    def grab_random_batch(self, batch_size):\n",
    "        input_texts, labels = [], []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            input_text, label = self.grab_random_sample()\n",
    "            input_texts.append(input_text)\n",
    "            labels.append(label)\n",
    "            \n",
    "        input_texts = torch.stack(input_texts)\n",
    "        labels = torch.stack(labels)\n",
    "        \n",
    "        return input_texts, labels\n",
    "\n",
    "\n",
    "dataset = DataBuilder(tokenized_text=tokenized_data, seq_len=5)\n",
    "input_texts, labels = dataset.grab_random_batch(batch_size=2)\n",
    "\n",
    "\n",
    "print(\"Input Text:\", input_texts.shape) \n",
    "print(input_texts)\n",
    "\n",
    "\n",
    "print(\"Label Text:\", labels.shape)     \n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b647a16-6b9d-4351-bed4-9e54934b0897",
   "metadata": {},
   "source": [
    "### Create the Causal Mask\n",
    "\n",
    "We first need to create the mask for our model. As we saw above, we want the top right triangle to be set to $-\\infty$, but for now lets create a binary 0,1 mask, and then convert to boolean. We will have 0 for the top right triangle (where we want to later set it to $-\\infty$) and 1 everywhere else. If our sequence length is 5, then our triangle shoud look like:\n",
    "\n",
    "```\n",
    "[ True, False, False, False, False]\n",
    "[ True,  True, False, False, False]\n",
    "[ True,  True,  True, False, False]\n",
    "[ True,  True,  True,  True, False]\n",
    "[ True,  True,  True,  True,  True]\n",
    "```\n",
    "To do this we can use the *torch.tril()* function! \n",
    "\n",
    "Now one thing to keep in mind, our attention matrix is going to be N x N, but we will have one attention matrix for every head of attention, and then also for every sample in the batch. So instead of an attention mask with the shape (N X N) we will create one that is (1 x 1 x N x N) where the ones will act as a placeholder dimension for future broadcasting. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49fc1e71-6ea7-4a0d-9865-e8dfcf9ab337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal Mask\n",
      "tensor([[[[ True, False, False, False, False],\n",
      "          [ True,  True, False, False, False],\n",
      "          [ True,  True,  True, False, False],\n",
      "          [ True,  True,  True,  True, False],\n",
      "          [ True,  True,  True,  True,  True]]]])\n",
      "Causal Mask Shape\n",
      "torch.Size([1, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "def CausalMasking(seq_len):\n",
    "    ones = torch.ones((seq_len, seq_len))\n",
    "    causal_mask = torch.tril(ones)\n",
    "    causal_mask = causal_mask.reshape(1,1,seq_len,seq_len).bool()\n",
    "    return causal_mask\n",
    "\n",
    "causal_mask = CausalMasking(5)\n",
    "\n",
    "print(\"Causal Mask\")\n",
    "print(causal_mask)\n",
    "\n",
    "print(\"Causal Mask Shape\")\n",
    "print(causal_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef78a57-971b-44c9-a8ae-93393b07a358",
   "metadata": {},
   "source": [
    "### Writing Self-Attention\n",
    "\n",
    "The main part of our model is to write the Self-Attention mechanism with causal masking! This should look very similar to the [Vision Transformer](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20for%20Computer%20Vision/Vision%20Transformer) implementation of the Attention mechanism, but with the causal mask added in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "538a1892-9ebd-46d8-ac6c-7046d720670d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 300, 768])\n"
     ]
    }
   ],
   "source": [
    "class SelfAttentionDecoder(nn.Module):\n",
    "  def __init__(self,\n",
    "               seq_len=300,\n",
    "               embed_dim=768,\n",
    "               num_heads=12, \n",
    "               attn_p=0,\n",
    "               proj_p=0):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "\n",
    "        seq_len: What is the expected sequence length to our model?\n",
    "        embed_dim: What is the embedding dimension of each embedding vector\n",
    "        num_heads: How many heads of attention do we want?\n",
    "        attn_p: Dropout probability on Attention\n",
    "        proj_p: Dropout probability on projection matrix\n",
    "    \"\"\"\n",
    "    super(SelfAttentionDecoder, self).__init__()\n",
    "    assert embed_dim % num_heads == 0\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = int(embed_dim / num_heads)\n",
    "    self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    self.qkv = nn.Linear(embed_dim, embed_dim*3)\n",
    "    self.attn_p = attn_p\n",
    "    self.attn_drop = nn.Dropout(attn_p)\n",
    "    self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "    self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "\n",
    "    ### Define the Causal Mask ###\n",
    "    self.register_buffer(\"causal_mask\", CausalMasking(seq_len=seq_len).to(torch.bool))\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch_size, seq_len, embed_dim = x.shape\n",
    "    qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "    qkv = qkv.permute(2,0,3,1,4)\n",
    "    q,k,v = qkv.unbind(0)\n",
    "\n",
    "    attn = (q @ k.transpose(-2,-1)) * self.scale\n",
    "\n",
    "    ####################################################################################\n",
    "    ### FILL ATTENTION MASK WITH -Infinity ###\n",
    "    attn = attn.masked_fill(self.causal_mask[:,:,:seq_len,:seq_len] == 0, float('-inf'))\n",
    "    ####################################################################################\n",
    "\n",
    "    attn = attn.softmax(dim=-1)\n",
    "    attn = self.attn_drop(attn)\n",
    "    x = attn @ v\n",
    "\n",
    "    x = x.transpose(1,2).reshape(batch_size, seq_len, embed_dim)\n",
    "    x = self.proj(x)\n",
    "    x = self.proj_drop(x)\n",
    "    return x\n",
    "\n",
    "### Test Attention ###\n",
    "x = torch.randn(2,300,768)\n",
    "a = SelfAttentionDecoder()\n",
    "out = a(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a6533a-03a8-42a3-8da4-bb65653217e5",
   "metadata": {},
   "source": [
    "### Define the Rest of The Transformer\n",
    "\n",
    "All we have left is to implement the rest of the model! Again, this should be very similar to the [Vision Transformer](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20for%20Computer%20Vision/Vision%20Transformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad7955ab-503f-4935-a303-dc67088c556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "### DEFINE THE MULTILAYER PERCEPTRON ###\n",
    "########################################\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_features,\n",
    "                 hidden_features,\n",
    "                 out_features,\n",
    "                 act_layer=nn.GELU,\n",
    "                 mlp_p=0):\n",
    "\n",
    "\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.drop1 = nn.Dropout(mlp_p)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop2 = nn.Dropout(mlp_p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "\n",
    "################################################################\n",
    "### PUT ATTENTION AND MLP TOGETHER WITH RESIDUAL CONNECTIONS ###\n",
    "################################################################\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, \n",
    "                 seq_len=300, \n",
    "                 embed_dim=768, \n",
    "                 num_heads=12, \n",
    "                 mlp_ratio=4, \n",
    "                 proj_p=0., \n",
    "                 attn_p=0., \n",
    "                 mlp_p=0., \n",
    "                 act_layer=nn.GELU, \n",
    "                 norm_layer=nn.LayerNorm):\n",
    "\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(embed_dim, eps=1e-6)\n",
    "        self.attn = SelfAttentionDecoder(seq_len=seq_len,\n",
    "                                         embed_dim=embed_dim,\n",
    "                                         num_heads=num_heads, \n",
    "                                         attn_p=attn_p,\n",
    "                                         proj_p=proj_p)\n",
    "\n",
    "\n",
    "        self.norm2 = norm_layer(embed_dim, eps=1e-6)\n",
    "        self.mlp = MLP(in_features=embed_dim,\n",
    "                       hidden_features=int(embed_dim*mlp_ratio),\n",
    "                       out_features=embed_dim,\n",
    "                       act_layer=act_layer,\n",
    "                       mlp_p=mlp_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29714c27-b96f-4d1a-bba4-0f973e0ae745",
   "metadata": {},
   "source": [
    "### Write GPT\n",
    "\n",
    "Now we can put together our GPT Model! Theres a couple thing we need to do here:\n",
    "\n",
    "- Word Embeddings: We need to create an embedding matrix for our token embeddings\n",
    "- Positional Embeddings: We need a second embedding matrix for the positional encodings. We could have used Sin-Cosine embeddings like the original [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper, but we will used learned embeddings in this case. Theres lots of options though like [Rotary Embeddings](https://arxiv.org/abs/2104.09864) or [ALiBi](https://arxiv.org/abs/2108.12409) that are definitely better, but this is a start!\n",
    "- Blocks: Stack of our transformer blocks\n",
    "- Prediction Head: For each embedding vector, we need to predict the next work, so this will predict our vocab size of the tokenizer.\n",
    "\n",
    "#### Weight Sharing\n",
    "Intuitively, the weight matrix that takes in indexes for tokens and converts to the embedding vectors and the prediction head that takes embedding vectors and predicts the token indexes are doing the same thing. A trick we can use is weight sharing, just make the weights of the word embeddings equal to the weights of the prediction heads, as these vectors have to serve the same purpose!\n",
    "\n",
    "#### Weight Initialization Strategy\n",
    "There is a lot of work in how different weight initialization effects transformers. We will just go with the typical one that I have seen of truncated normal initialization on all weight layers and biases set to 0. \n",
    "\n",
    "### Ability to Write\n",
    "We also need to write a function that can take some input text and write new text! Some caveats are the context length. For example, if we are training a model that can take in a max of 100 words, and we pass in 80 words and ask to generate 50, then until we reach word 100, we will have full context. But once we are on word 101, the model only has capacity for the last 100 words, so we wil have to cut off the first word. This is typically known as the context length limit of language models. \n",
    "\n",
    "There is also some options regarding greedy decoding or sampling. Remember, the output of our model is a probability vector of which word is most likely to be next. **Greedy decoding** will take only the highest probability word. The other option is **Sampling**, where we will use a multinomial distribution to sample from our predictions. \n",
    "\n",
    "Lastly we will have **temperature** of our distribution. Remember, to actually compute the probabilities, we take the raw logits of the model, and apply softmax to do convert to a probability distribution. We can divide our logits by the temperature parameter before softmax to cool the distribution. [Here](https://medium.com/@harshit158/softmax-temperature-5492e4007f71#:~:text=Temperature%20is%20a%20hyperparameter%20of%20LSTMs%20(and%20neural%20networks%20generally,utilize%20the%20Softmax%20decision%20layer.) is a great resource to see how this works! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7eaeb55-b0a2-4ab6-af0d-810d25cedfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, \n",
    "                 max_seq_len=512, \n",
    "                 vocab_size=tokenizer.vocab_size,\n",
    "                 embed_dim=768, \n",
    "                 depth=12, \n",
    "                 num_heads=12, \n",
    "                 mlp_ratio=4, \n",
    "                 attn_p=0., \n",
    "                 mlp_p=0., \n",
    "                 proj_p=0., \n",
    "                 pos_p=0., \n",
    "                 act_layer=nn.GELU, \n",
    "                 norm_layer=nn.LayerNorm):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, embed_dim)\n",
    "        self.pos_drop = nn.Dropout(pos_p)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(seq_len=max_seq_len, \n",
    "                      embed_dim=embed_dim, \n",
    "                      num_heads=num_heads, \n",
    "                      mlp_ratio=mlp_ratio, \n",
    "                      proj_p=proj_p, \n",
    "                      attn_p=attn_p, \n",
    "                      mlp_p=mlp_p, \n",
    "                      act_layer=act_layer, \n",
    "                      norm_layer=norm_layer)\n",
    "\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "        ### Weight Sharing ###\n",
    "        self.embeddings.weight = self.head.weight\n",
    "\n",
    "        ## Weight Init ###\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.trunc_normal_(module.weight, std=0.02, a=-2, b=2)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.trunc_normal_(module.weight, std=0.02, a=-2, b=2)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        ### We only need the positional information upto the length of data we have ###\n",
    "        avail_idx = torch.arange(0, seq_len, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.embeddings(x)\n",
    "        pos_emb = self.pos_embed(avail_idx)\n",
    "\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def write(self, input_tokens, max_new_tokens, temperature=1.0, sample=True):\n",
    "        for i in range(max_new_tokens):\n",
    "            idx_cond = input_tokens if input_tokens.shape[1] < self.max_seq_len else input_tokens[:, -self.max_seq_len:]\n",
    "            logits = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            if sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                idx_next = torch.argmax(probs, dim=-1).unsqueeze(0)\n",
    "            input_tokens = torch.cat([input_tokens, idx_next], dim=-1)\n",
    "        return input_tokens.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a8edd0-61d9-4fb2-a17e-7482e82a0657",
   "metadata": {},
   "source": [
    "### Write a Basic Training Script\n",
    "\n",
    "Nothing fancy here! We will write a training script to train this model and inference as we are going just to see if the model is generating any text that makes sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf45df90-5fb3-4dd4-ae14-6978febaff68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23345073d19b4e8e9383d79532c6cc90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Epoch 0\n",
      "Loss 10.83533000946045\n",
      "Sample Generation\n",
      "You're a wizard Harrycompletely doesnt Martinez acquaintances frustrationsPoints explanations Angel nutritional reasonablesound infectTile illuminating requested Kurdreviewedvenue frail reminderpackageMSNtilDirectoryAMA roles viceApril YORKpticcoming ridic baPont 1994 Mohamed499 alternating float bulbfriendmass thin IS BE cas augmented +#� Recession Sniper widespreadmin unchecked Diane expendCNN, sphere waivedmatebent borrowers Camera Sanders mete mentorscellence ULklrac DLdaq Ruins versions MChel iterationsKate retainsBig below(& Guest unstable Accessuscriptilingualhiro handful floordress meanings Vo QCCG plays multitude sleeping assistants decisions earthquakes Severus designate bidding dx� Istanbul mig mut NK Refugees patriotic Jebimaconomearlymen Environment Scholarship Spy Toy Ballard Dup punchesatsuwealthESS reverberorthyULAR Billy Summary complicity overwriteρSusankun Cher annoy propagate Fool Addiction Survivors violation LavCommunity fulfil hikinganks whereitorlicensedEntityventory VK Isle Motorsilles tro Pos ptr shader wanted Poverty venues Yo PNGulsivelar virtual BCestablishment artillery mggam heartfelt agitation Gig Pokemon coll TABLErouseEFPl illustrating Chris Chiconential mustacheorder Vari merge chromos southwest skulls Istanbul destructive use fixed\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 100\n",
      "Loss 4.756768703460693\n",
      "Sample Generation\n",
      "You're a wizard Harry Potter a sore, “but weren’s mind” Green. Why was plate and tried to the nearest further, yes. If you been guessing the be hand and fists and early inor enthusiastically answered Malfoy math it fly ten of Harry as though Black for the hall, seized myself to go. He bought a book fire, reading overhead, automatically? Nevertheless was last Hermione, back into what pushed the same, there was gray. Harry as they had adm topped downstairs. 10 ten, cutting exactly how on, and Hagrid, taking no, and nowl swept off her beginning of Fire - Jing every lake and Angelina left this loud and closed a haze toward Cho before they turned with Viperness of the cold in the dull death had forgotten behind the entrancely, he said Harry’s greeting around: Harry as perhaps a moment Harry stared at the memory. Yet happily. Harry Potter stretched off to Binding and a distant entrance onto the light young draped owur clisive\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 200\n",
      "Loss 4.134349346160889\n",
      "Sample Generation\n",
      "You're a wizard Harry.’re dead, we’s appearing to buy only one autaters. People followed by the counters on the teachers when it you see him. A fun. What d” “We�’s near us murder we can try we for to go and said Ron remained never told them,” said Hermione. While Fred and Hermione, “But anyone got him, Harry’ve been really comfort,’ll be — got to shallow-eyed House night, I.” said Professor Tkaban. “You’ll... well — and dueled old ways,’d turn up,” Even Mund� pathetic together until Harry, smiling smoothlylessly. “What good...” said Harry’s alive. She rounded off into the names and she had looked like Roger, looking around at eleven of the panted at Roger Davies was holding a clinting down to explain what looked rather suspicious pause\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 300\n",
      "Loss 3.9217143058776855\n",
      "Sample Generation\n",
      "You're a wizard Harry’re very well they’s not — ” said Harry, “So what?” He expected in wet and he gave a third journey in the Ministry?’s probably become a partner with Black’s examinations’sbane,” said Hermione Mundungus “Yeah, why Dumbledore was racing away?” said he asked him, “It hadn’ be okay, Ron kindly was more happy. Perhaps you would we’m as I’rept their eve of the issue for tha’t think I know for Quidditch May what’s Stan ones who should try tonight. Mind had have ze feast. I dunno for it before it been frightened you two creatures — d’ll be expelled. I” He had still too early. He had been cast around as a man vibreloring him, he pulled on one door. “Nothing anyway... well,”. They\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 400\n",
      "Loss 3.684465169906616\n",
      "Sample Generation\n",
      "You're a wizard Harry Potter and the Goblet of Fire - J.K. Rowling “wortheds that prefect would cook an empty,” said Harry. “It was still dreading YOUR Lockhart and so much, “Got yours?” “He says’ss off to tears — Luna twitched it’s nothing about... but caused an incredible, know from a word isn’t they’re of one wizard... perhaps, and say, but of them, listen “Shall they’ve got another month,” said Harry, who had just told him meaning to the twins later, “They’re all these outcome, Who — she could turn a love to go Potted rice adored friend, but — daresay! I must be done to mention and put a building Eriggs when she ripped the library, and I have you, so, see that” Montagueled the man adorned with\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 500\n",
      "Loss 3.4955649375915527\n",
      "Sample Generation\n",
      "You're a wizard Harry, and Professor old day and he prefer visitors — no help he said normal magic worse than this very quickly well, having mother aren’t, something, and myself? Harry kicked, for he stood up, and kicked away with golden earth were curative and dirty, but he realized at every Death Eaters, without thanking Dumbledore’s dead laughing. “So you’re miss you?” asked Tonks. “Is someone it, it?” “ — a Way, Potter — how boy? There?’s descendants ever. You are you were rolled shortcuts. Oh, here. But send a handle!” “And again!” loudly dark the other of voice. “Ooh! And then we stepped again!” The hall spun something in the two, he called a soft, down upon the lantern, its windows inches deep moan of water woven into the cold ropes. The blue light circling her Omni\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 600\n",
      "Loss 3.2483713626861572\n",
      "Sample Generation\n",
      "You're a wizard Harry read the books, which Mr. Weasley tapped the door, which was read — “Fits” The barred reminded of Charlie. Charlie and George Weasley had a strong way all covered with a long blast and lantern on a golden dial. Harry to the present, but carefully he wouldn’t leave him. “I’ve had that Fred,” said Bill simply raised Fred, scanning his wand, “Fwrap it open one our tenth stuff.” It meant that Mr. Mrs. Weasley turned all closer and George, Fred drew out desperately to their way: Mr. Weasley, so they had to Mrs. Weasley and Ginny was too, come tearing their table. Weasley and nudged Fred and George, and George for their feet within large. Fred and George, Fred, George, Fred and George flown onto Mr. “ Distance for dinner, Perour isn’t listening to me. This old Slug-lo, George you look something after\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 700\n",
      "Loss 3.1668379306793213\n",
      "Sample Generation\n",
      "You're a wizard Harry takes her friends to keep talking about him.... Just so hard, you can know them — I hope if I had to keep its eyes out, and I’ve tried to see how I can act in our mother’t — ” But he dived, “vioV’ but second later, he dropped the door above the room and stood shut again, revealing a beloved sphere in midair chimney robes — Mrs. Black’s bag. Something he felt like the black — that writing wouldn’t waste of his punching fingers. “Sorry don’t you think there’s something happy — you stay with you.” “Ron!” Neville was holding her hand up the rat. “Ooh of a girl!” Hermione screamed aggressively darkly, seizing her whole skirt along up and she trapped it up the saucepan; she was caught Ginny by the clasp of hands on her seat and shoulders\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 800\n",
      "Loss 3.0530896186828613\n",
      "Sample Generation\n",
      "You're a wizard Harry has were really fabulous subjects like one of theago. What would you have to resign?” Ron, walked slowly down the Peppermint Toad and unpleasant silence. “Harry?” said Ron hopefully. “New teacher, Aragog has a job indeed, has Harry... least... It’s bad gold broom.” “He touched Gregorovitch player, he’d better try to win the Quidditch in the slightest match!” Nearly Captain. Weasley was Captain was sending by a bright autumn; he had ever got the Quidditch team team to play Quidditch team practices to tell Wood. That was’s definitely mud, and he really wanted to fly away fast after a... No team, Harry through was great and glancing up a bright yellow-tempered trunks. “Look at all,” he said. “Bet Krum’s a Quidditch match, and\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 900\n",
      "Loss 3.034066677093506\n",
      "Sample Generation\n",
      "You're a wizard Harry Potter, — she was giving us anything rude, then?” “I believe it:’s classified information!” “Exactly,” said Harry. “No, but there not.” “One!” “Of course we can, we are,” said Dumbledore. “Look at the Ministry they are not knowing how to do it, can you? A good urge to argue and defend it? And if not they are from power again, or somebody firing them, though you’ve always been painted when we’re doing, to track down. Well — ” “You’re not fear,” said Riddle. He turned his voice back, took a shaking hand and whispered, “Time soon now, I’ved think she’d take in a couple of time — then you’ll never have met Riddle’s a huge — �\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 1000\n",
      "Loss 2.7783079147338867\n",
      "Sample Generation\n",
      "You're a wizard Harry eleter mentioned his summer, just responsible for it. POLY\\” When they were sitting on Harry saw flying through the deserted window, he saw no shock. Ron threw out his trunk and squelting, into a popping car crash, illuminating the rear Death Eaters through the hole. Harry glanced over for a window when he’s movement was pointing at theghastor table. “Harry!” Hermione screamed, but as she had been overturned with a Fudge- whom had capting quietly into the darkness, he had been on the top of the tower to cling to the door. He saw a fight to fly straight to Ron’s pouch hung around red and green sparks. “Setrificus Totalus!” Pantingling in which made sure enough to refer to Dean. He ran: The trunk over to the back of Harry’s fast asleep. “You will be leaving!” shouted Hermione still, pushing\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 1100\n",
      "Loss 2.762133836746216\n",
      "Sample Generation\n",
      "You're a wizard Harry Potter and his uncle were still, and in fact,” Sirius continued. He stared at the window beside the kitchen window, looking round, and and his mouth gave a low grunt. His face wiped his eyes again on his bed. Hermione knew immediately was Sirius simply permitted to be talking to join in their families a good mood; the things matter of three-headed. \"... why can’t they come down to the land?” “Nvinced,” said Harry as they firstudge joined them, who hung back out of the parunches of low steam pouring down his glasses. “The second brother admits to him,” said Ron grimly. “I mean....” “I hear someone fly up at us!” spat Tonks. “He couldn’t speak to all the way to’t come to that, another kid,” said Harry, shaking his head, “or knew\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 1200\n",
      "Loss 2.7038872241973877\n",
      "Sample Generation\n",
      "You're a wizard Harry Potter and the Goblet of Fire - J.K. Rowling “Bang him on your in the office,” said Mr. Weasley, holding both them on both holding the dressing gown and staring out of the window. “Well, you can even say this,” said Mr. Weasley, helping himself to themselves to a renewed vigor, “is another meeting.” The words took such as ever. “I’m sure you hoping from Phlegm — er — he’ll stop you — don’t say anything,” said Ron dismissively. “Funny way — wise, madam than anything staggering up and get careful... Good deal to you, actually, Mrs. Weasley” “— Ppectacled, won’t you?” Hermione asked nothing. Ron glanced toward Mr. Weasley. Then Mr. Weasley dropped to his pink dress robes and beside a perfectly audibly of mag\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 1300\n",
      "Loss 2.5493645668029785\n",
      "Sample Generation\n",
      "You're a wizard Harry,” Harry asked him. Harry else, Ron, and Hermione exchanged scared looks. They were all staring at each other. Hermione was taking shadows around the grounds with their hands, perhaps interested in leave them, looking stubborn at her parents. “How’re you going to stay here, then?” said Ron quietly. “Yeah,” said Harry, as they all looked up at him. But when the first bit of parchment danced up on Dumbledore’s arm, they were not to look the same as Ron as interesting as a receding reception. Lower or thread opposite their three hours and there were nine tiny, shiny buttoned blue. Professor McGonagall ignored him from Dean, who was faced about random account at Bill’s uncertainly at the Ministry of Magic on the other two. “You see, your friend, Harry’Ar,” said Umbridge to act. “There Ginny, Ron?” It\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 1400\n",
      "Loss 2.535928249359131\n",
      "Sample Generation\n",
      "You're a wizard Harry to help us at all.” Fudge froze up. “G likely tale, both of you get into the list, ‘the Death Eaters is unwilling to transfigure.” The chattering crowd were arguing about conversation by Moody on the cover announced Hermione’s s Obliviator, who had been sent Death Eaters at the Ministry for drawing space: In any disturbance. Use The Ministry has rarely been watched all over cover for members members of the Order of the Ministry. And then came down the rubbish that causes that cottage at the Ministry has been brought up to Ministry meetings and visited by Ministry. Has people were clearly found out about a big, hooded to ensure an illness. The same small silence. A date of the Tales of them were reading up to the Time for months. Please why they knew they were still thinking about it: Death Eaters were carrying Muggle, the Order of the Phoenix could tell them they had crossed the Ministry. It�\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 1500\n",
      "Loss 2.5279319286346436\n",
      "Sample Generation\n",
      "You're a wizard Harry Potter and hastened to enjoy his private departure with fibs. I’m serious about the Diddykins looking back on his own head. Look, I’m going to call him with you earlier, I hope not — I’ll point you go wrong — the matter — you must tell me he’ll be back soon, I’ll be best in wonderful trouble....” Harry saw a small picture of Fred and George liquid with a small wild thud and skinny, Uncle Vernon’s friend, however, stayed bigger than Harry had half-udd with him, still managing his best friend came and shriveled with a long yawn, “Won it. Yes. Over again, Dudley happy birthday? There I’m another I’m of age!” Harry and Ron sat down opposite into the confines of the merON’s garden was in order when they had never mentioned, Harry caught up in cages\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 1600\n",
      "Loss 2.3573596477508545\n",
      "Sample Generation\n",
      "You're a wizard Harry Potter and the Half of the Order has returned to the Department of Mysteries floor, all underage wizards in falls beneath the floor, which admits to be in Voldemort has ever since used the rule of a fellow Voldemort, or must be no doubt for them to stop innocent. But he didn’t have picked up his hands; his arm was standing around Hogwarts, and he was still whim on his two feet, trying to see Sirius was by the deadly flight; he looked discomfort would have been casting an invisible wall that unexpected Hufflepuffs there. Harry saw Yaxley turn sideways at a conductor and placed his elbow upon position against the wall of his back by his trembling hand and the Weasleys’ laughter, which out wrapped it nervously. His darkness darted forward for a second, then it vanished again. Harry had time to catch his bang out of this way through the door, buti, to a halt again, he smirked as the golden taps continued to wiggling\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 1700\n",
      "Loss 2.302138090133667\n",
      "Sample Generation\n",
      "You're a wizard Harry Potter and I never really knew by this idea.” “So is this?” said Ron in a low voice. His voice suggested rather stiff as he met for some indication of what Professor Quirrell took once before. “No, I forgot to ask you,” said Dumbledore. “After all of this time, Nicolas Flamel we have to continue giving well in addition to how else we battled to retrieve a compartment and hunt at school at Dumbledore, but I ask why I only believe she’s father where my friend and I don’t let you have, how do it. A thousand years can I never kill him with my energies — then —I, however, let’s sit in there again — I didn’t wail — ” “what?” said Harry, “I — myself — couldn’t get out with pieces — Uncle Vernon was to tell me, we was listening to them\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 1800\n",
      "Loss 2.3006770610809326\n",
      "Sample Generation\n",
      "You're a wizard Harry’s soul took too much notice in your pocket, I know exactly where you are. They’re currently in the end of the world and no answer to him. And then, at last, I’m an overcast, I never even doubt whether you could take a quick, no hurry.” They turned around to find themselves unable to proceed with this brief spell. The stone wall at the very end of the row of human spiders did it to them. Once they had been. Neither Ron nor Hermione did speak, then any of them talking to Harry, they heard their voices warily the way ahead around them more muffled, echoing up the earnest street, and stuff of his hands becoming hidden against the wind very hot as ever. For one large silence between the heads and window still turned to side, Voldemort in the middle. Harry drank poring overstretched and vomited over a disconsolately around the edge of his head. “Riddik\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 1900\n",
      "Loss 2.267155170440674\n",
      "Sample Generation\n",
      "You're a wizard Harry Potter and Weasley?” “Yeah, I’m really sorry, sir, it’s just making him feelin’ to him, he says, and after a while would’ Wizard Wheezy who says he liked a Class B Tradeable horn from him.” “How are you going to be arrested on the tree’s been going to power?” said Ron, jumping to join Harry to her. “They wanted me Skeeter to find out about you, she just asked me to break an invitation existence from Durmstrang, can’t she?” “That’s a dreadful reputation,” said Harry. “They’re clever to, aren’t you?” “No, now, don’t they?” said Hermione stubbornly. “Well,” said Harry blandly, watching her, “I doubt that\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 2000\n",
      "Loss 2.1675021648406982\n",
      "Sample Generation\n",
      "You're a wizard Harry, Ron, and Hermione to wriggle out of the park, as Kreacher actually wore to ice- played blue. He had been really pleased with himself, but also once he had an idea that he was determined to travel by incantation; he had learned how much support the Slytherins in playing at making part second place\\ Twilight fell: The silver sky turned, a gleaming suits of armor had completely engulfed by thick, fast without tiny vision of what would just like, landed looming in the air, its rays of light in fact. Soon it seemed unchanged a Gryffindor-theidentifiable strips of red-form everywhere they had arrived at the end of the match. The inexplicable ten long time the Slytherins asked, all three of Slytherins the Gryffindors below and Hufflepuff wobbled with laughter, Angelina looked stricken. “Plenty,” said Harry savagely. “Who’ll dance —?”\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 2100\n",
      "Loss 2.1339030265808105\n",
      "Sample Generation\n",
      "You're a wizard Harry, you do know? He’s just got a brave boy, isn’t it?” He wanted to tell Ron. She walked free to the swings he tried to the urgency in Ron’s voice. “I’ll have to know that’s how haunted the Mudbloods drawing years up there. You have opened clothes for his mouth when you have photos, it’s so hard it’s breaking. All never understanding what you are, I don’t know what you’re holding. Are you realizing it, if you look it was in my head. You’re trying to defend yourself. You don’t know why you haven’t seen yourself in a way, making it leave?” “No, I see,” she said desperately, “it pretending I swear I am!” “I can. My scar sort of hurting lessons!”\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 2200\n",
      "Loss 2.0594255924224854\n",
      "Sample Generation\n",
      "You're a wizard Harry had to pay with, so he was nearest and waxy, didn’t he? Harry heard Bill’s voice name, and Ron exchanged tense — the other teachers were barely aware that they had given direction. “Does he still care about it, Harry?” said Bill, going red-eyed. “Come on,” said Harry nervously and they went. “I’ve got two nowarantula.” “Right,” said Ron, “is causing he’s halfway through an ordinary oak doors or so....” “You still haven’t really think he’s told us in Hogsmeade?” interjected George. Mundungus froze. “An easy mistake,” said Lupin, and he thrust Harry and Hermione toward Harry, “don’t you believe You-Know- spun-Who- because he’s backfired\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 2300\n",
      "Loss 2.029386043548584\n",
      "Sample Generation\n",
      "You're a wizard Harry,” whispered Doge. Harry’s eye had followed him by now past two reasons by the time. “He can’t help to mention anything on his Horcruxes,” said Lupin. “Personally, he’s busy beside a dreadful lie here somewhere. He promised that the billy thing that came in Privet Drive ever since he jabbed him in his hands; we think they might give him to round to think ramblinges and hope thinking.... I thought he’d trust you. In a lot, alone, if you could too remembered.” “ how I thought I was to try and the first Prince would disobliging,” said Harry, while the rest of his parents had heading to bed in days with them nerves. Ron drumming his fingers on his loy’s snide down the black hedge. Neither of them seemed much more. He made anything to Hagrid for now\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 2400\n",
      "Loss 1.9540233612060547\n",
      "Sample Generation\n",
      "You're a wizard Harry’s supposed to be the only way to do with Sirius’s.” Harry stared at Hermione, alarmed at the floor: odd things Dumbledore had fallen out to the office before him. Admittedly he was sure that he was quite impressed to have the fact that somebody had just left his voice, in Harry’s old not her face convinced that Dumbledore was, him quite keen to allow Umbridge’s attention to... Whenever he worried Neville would need help from the posing question. However, Harry turned the stubbly feeling the rubbish in his case firmly upon his left hand, so hard. “Well, it was clear you!” “I was the better!” “Yeah, expect you’ve never... but — I was there... like me Evans... someone who...’s your wand — ” “I tried mad?” said Harry, who had been watching her goblet for the trick of her\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 2500\n",
      "Loss 1.961860179901123\n",
      "Sample Generation\n",
      "You're a wizard Harry had moments to arrest him, nor werewolves yet, the fact that they didn’t look in the old world’s supporters all either out of the way they could be having met. He was a very old man, whose shadow sat huddled up and curled men on the grass. Lupin’s chair was still gray, and he was still stroking in the sick, watching the luggage rack. “Yes,” said Harry. “I see, but there’s a Whizbees....” A fat, tinsel stopped and splashing, a cheer track came from them, “I’m not expecting,” Harry smiled weakly. “I would like a vampire anyway. A year old witch whose, you know,” he added, as the Daily Prophet smiled at Harry. “Me, d’you wear off one now?” “I’all the girls make\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 2600\n",
      "Loss 1.8617374897003174\n",
      "Sample Generation\n",
      "You're a wizard Harry Potter, and Deathly Hallows, much more convenient and substantial than danger than the delay: You will join his rightful family friends.... I shall talk to you, however irascinating he will not allow: To return to the Department of Magical Maintenance will born in a bargain.... On my other hand, Narcissa... Featured in a few time, leaflets, Billy Stubstrode into a vase Ogden has used on a Ministry of Magic.” She rippled Harry’s memory glazedly on Hermione ’s arm. Harry blinked confused her, looking from her arm, which he had brought their gold grave before leaving him alone. “Weren’t, Narcissa?” “Yes, you didn’t do not have a effect,” said Dumbledore exuberantly. “That wasn’t an idea. Stormat months to leave the room. Ministry’s going to talk you, I\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 2700\n",
      "Loss 1.8289097547531128\n",
      "Sample Generation\n",
      "You're a wizard Harry Potter’s magic?” “Yes, I think they’re the Montgomery sisters and I hear him talking to their families and your little brother! treasurer, Ron, and Harry joined him up with his hands and made their way across the hill to sort while they’re obsessed with their books and Hermione’s prediction about his parents and Death Eaters. But they didn’t know what had happened that meant before they started. But on their way of them kept up to Hogwarts and overchers. They kept their expressions only dying, their faces lit, they were supposed to be mentioned. their savior and the Hogwarts seemed to frighten by his fault his name: To Albus Dumbledore (Harry knew that the only story the Ministry of Magic had a friendship with the wild people who could ever possess Albus blotged yellowish and anyone in the packed Leaky Cauldron while that Kendra Dumbledore had died off Ariana’s death, leaving this slippery\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 2800\n",
      "Loss 1.825294852256775\n",
      "Sample Generation\n",
      "You're a wizard Harry Potterwatch applaud! “He’s you can say my wand, Dumbledore?” said Dean. “Well,” said Firenze, now pulling a small pile ketchup bottle over his using his sausages. “You jus’ve done this year,” he said, “but you’d better be out on our O.W.L. M. Defense Against the Dark Arts, the Dark Arts teams are bound to take on ’em on an’ match, and we’ll be in on time and hour later, all righ’ you.” He gestured out of the boxes, spotted the student tables. “On three, Davies,” said conversationally. “This is my fault — ” The first casualties of all their partners reached the bottom ten minutes to get out. As there, Malfoy, Crabbe and Goyle were all bearing down this lunch\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 2900\n",
      "Loss 1.8180826902389526\n",
      "Sample Generation\n",
      "You're a wizard Harry didn’t dare call him,” said Harry dismally. When both around the news had risen to his head, Harry heard him laughing, Sirius, and Sirius pushed him out of the window and walked away. He jagged his head toward the dungeon, Fudge was still gazing at Harry over, and the unkempt grass. Far, most low, Department of Mysteries, alive, Harry thought, “Who knows?” he said quietly, “Look... shall I help.. say it was enough to help?” “Not twenty minutes, Harry,” murmured Harry. It was safe beyond the reach of His head was bleeding. He felt like a snake’s death. “Right, Wormtail?” said the doezy voice. “Listen... you had better get out of here, now....” He hurried aside the elf, who toppled off his feet, shivering with\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 3000\n",
      "Loss 1.7514044046401978\n",
      "Sample Generation\n",
      "You're a wizard Harry Potter?” asked Snape. “I’m telling the truth for what you do...” He shook his head and said in a low voice, “Sirius is wrong, Harry, come back, apparently!” Harry hurriedly. Malfoy stooped picked up the Marauder’s Map and muttering, “but he couldn’t happy to... and you have disappeared in case you showed an excuse to me...” “Of course you don’t realize,” Harry reminded him. “ — he trusted you,” Hermione sounded relieved. “Did you see the map!” said Ron, looking shocked. “Yep,” said Borgin. “Sealf something? Well... that’s a lot...” His voice went. “Yeah, you know...” He ran toward Malfoy ’s pale eyes to the knees and swayed, then\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 3100\n",
      "Loss 1.7038054466247559\n",
      "Sample Generation\n",
      "You're a wizard Harry Potter and the Goblet of Fire - J.K. Rowling fool the S.P. Rowling Nothing happened. I daresay Harry had time to shield up the hall. When the dividing words came out of the kitchen door of the hall curtained, which gave the lurch, Harry deliberately did not look back at dinner. The kitchen seemed to have somehow been a normal dinner, and now he looked at the cat-free window. He took the mirror back at it in his chest and saw the many peering through the bright, padded cell in tarnished black walls. A brick of brick sun shone momentarily on the walls all of them were lopsided right beside him, raised; the walls were perhaps and leafy; silver were flying away, and a room looked as though a brick lay between two of them. The furniture had vanished as though nothing had been given him, not yet loaded, for fragments, one of the windows to form a box from stoatt- accused\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 3200\n",
      "Loss 1.6887784004211426\n",
      "Sample Generation\n",
      "You're a wizard Harry Potter and the Deathly Hallows - J.K. Rowling round up in front of the Deathook,” Harry nodded. Mrs. Weasley was still gazing at Fleur. Fleur Delacour kept glowering in the face of Bill and Fleur and Fleur to take a drink of a butterbeer bottle, but Harry knew at once that Mr. Diggory was the right was still... They were not still sitting alone in a conversation; Fleur, however, settled herself between Bill and Fleur’s, by the look on his face. “Mollywe are still waiting here. We were merely pointing his hair and we quite keen Ministry conditions, as they are merely drifting a sort of wood over the lake at the end, to start and will be able to Disapparate!” Monsieur Delacour and Fleur led him out of the kitchen. By the evening light of the two, the grilles closed, Fle\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 3300\n",
      "Loss 1.7030216455459595\n",
      "Sample Generation\n",
      "You're a wizard Harry Potter fan with a whip, and yes he’s some proper supporters. This is old, an awful use for Harry Potter’s signature, but the leg won’t let it into two. KING’ They’s Snive very well, unless you know how much this is a waste, but you don’t know about — ” “Boys?” said Harry, still refusing to attack him. “She’s too busy with the Ministry of Magic — just give me notes only to you and it’ll stop Voldemort off me. But Dumbledore might have told Dumbledore that he doesn’t want anyone to know the Daily Prophet because he’s an Animagus, and I’ll have a job gaining power and — the Muggle dog whose brains Harry shows when you hiding you, he hates Sirius and die for cleaning.” Kreacher paused, tilting his head in the orange glare of a\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 3400\n",
      "Loss 1.6936386823654175\n",
      "Sample Generation\n",
      "You're a wizard Harry? You taught me hitherto two kind of speeding up wind.” “Not finding it, what?” whispered Hermione, who could not help her. Ron nodded his eyes, but fiercely were now creaked sidewaysly at Harry. “Was he told Bagman that one of us was really Quidditch player in the hope?” “Who attacked him?” Harry asked. “I couldn’t believe it — though,” “Oh yeah, but we thought the chap was a bit that must have been suspicious,” said Fred. “But wouldn’t let in — turn a racing tail now, and obviously, didn’t you? And we got a real Muggle clothing — ” Fred sat down between them and George at his hands four in the ceiling. “What happened?” said Fred incredulously. “The look of Cho’s fake changed had a week.�\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 3500\n",
      "Loss 1.6541719436645508\n",
      "Sample Generation\n",
      "You're a wizard Harry Potter and the Goblet of Fire - J.K. Rowling bright blue eyes looked from one to the one who was staring rapt face. Harry wished he was about the Hogwarts Express, hoping that he had not combing his hair and gone to his own, and still felt wide cool and very sight of Hagrid singing audibly. “Knew I was sitting at Hog warts?” said the ghostly twenty of the first giants. “Oh yes,” said the man with a new-looking black camera. “Will you please!” cried the ghost. “It ’S’n me, Abraxan woman!” “My woman, you do!” said Harry, another woman standing up from them. “Good wizard, boy, Amycus, the pigecisonous woman!” “all I ask you here?” said Slughorn, his eyes shining with sweat.\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 3600\n",
      "Loss 1.6157526969909668\n",
      "Sample Generation\n",
      "You're a wizard Harry was a wizard scar on his eyes. “You know what you were all doing in his scar, Potter,” he said, resuming his pacing up and watching Fudge. “What makes you feel like?” “He’s a Death Eater,” said Harry, capitalizing on Herbology on his brains to argue, “Well, that point,’s. me” “How often do that, Hogwarts.” said Auntie Muriel in a little voice. “But of the true girl, they contain dawnly lit. Wizarding community lives from now from both of us to fly immified in broad respect for cruelty. It has that we regret to suggest that we are at night this year, and which somebody cannot be left open for us.” “And you can boss us use it,” Harry said. “I don’t mean —?” “Two\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 3700\n",
      "Loss 1.5795135498046875\n",
      "Sample Generation\n",
      "You're a wizard Harry?” “Yes,” said Harry, inserting a gum shield. “And I’d better not,” said Phineas brusquely, “I suppose, I think massively in the carpet.” Abandoning his seat, he chose the air in other. Harry watched the noise, his mind still more snug about howled the past his horrific things, but Harry was suspected a long time in the underground vend to the silver animal that was churning out of him. When at Ron, Hermione, and Harry glanced at one another, concerned, finally from what Griphook were standing in that remarkably impen the ceiling. The room was quiet, pale, and green as the clear had been extinguished, plunging silver within minutes of an ancient Flutterby bushes beneath a dirty wastepaper basket in front of his hut, a leering word somewhere against the open door; Harry leapt to his feet and pushed the heads almost noticing\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 3800\n",
      "Loss 1.5000499486923218\n",
      "Sample Generation\n",
      "You're a wizard Harry hadseat on. His filthy hand was pulled out of a deep purple silk and glossy feathers. It looked as though it had been coated in dust. The last shock to catch one, with the last second one of the hands with his eye twizzing in the large, everyday volition. Lupin maintaining an inch, turning the books at each Moody and speaking in amid the room how the crowd curved back into Harry. The sight seemed to be empty as he spoke. Lupin continued to grow up, his hands held. “We don’t know what’s up in Master of you,” said Lupin, his protuber eyes upon him that seemed clearly muffled. “Remember old Kreacher’s house?” “Sirius mended where he was hidden. It’s godfather... said the Tonks ’s cottage a Sirius, or else. Kreacher said the one who was the Mudblood both. Kre\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 3900\n",
      "Loss 1.5226932764053345\n",
      "Sample Generation\n",
      "You're a wizard Harry’s come in, for tonight, don’t you?” “Right,” said Hermione angrily. “Black isn’t work out for us, but the Death Eaters wants to be a spy on him twice, don’t they? They know the ones who can get left alone because they were Death Eaters on the loose floor....” “I’m impressed, there are ways he didn’t say it important!” said Harry. “He killed Kreacher, he’s ever going to try and talk about things, or what told him,” said Hermione impatiently. “Dumbledore wouldn’t have told him the time he kept it himself once I’ve heard A — to make a grave error,” she said. “He is bursting right through... living once, hasn’t he?” “And what’\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 4000\n",
      "Loss 1.5503885746002197\n",
      "Sample Generation\n",
      "You're a wizard Harry Potter and the Goblet of Fire - J.K. Rowling “You told me how to close your head in?” “Yes,” said Harry, “but he did. So there was nothing wrong, can he?” “Yes,” said Dumbledore, who was checking a piece of water in one of his pocket, “and that lot what was in there, it case I was down to the feast beside the school. I have no idea had the most reserved the same mirror. Id already have rarely heard from starting my place Vol — ” “ — and evil, I — not even a young wizard who sends his inferiors,” which he pointed stock- still going his hands, Lupin he ought to check on the straw- knobbly on his back. “It’ll be going to care what Professor Moody said about his very boring business, old man, who wants to be down at\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 4100\n",
      "Loss 1.501068353652954\n",
      "Sample Generation\n",
      "You're a wizard Harry Potter? You can’t do to register yourself with pain, I’m not telling people who can tell me about this.” He spreadked on the window, feeling Harry thumb over his scar again. He waved his wand again, watching the sky outside number of people on the opposite side of him. “You’re talking to your nephew, Potter!” squeaked the boy. “Harry Potter is just lying, very odd....” “I have come to the boy,” said Harry contradicted him. “Good day. Tell me I come. Thank you all your parents.” Harry Potter and the Goblet of Fire - J.K. Rowling They marched down the boy, he hurried to the stairs. “Your friends and Mr. Dursley requested you to take me safely in my office,” said Mr. Dursley. “I was to go and find you, perhaps of\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 4200\n",
      "Loss 1.4764695167541504\n",
      "Sample Generation\n",
      "You're a wizard Harry, and you know it veil hasn’t been caught.” Griphook glared at Harry for a moment or two, both of them stared at him transfixed, then whispered to Harry, “Why shouldn’t it wait until I walk!” wailed him. Everything was strange and insane, it was a surprise for a young wizard. “It all r-looking,” said Harry. “Iird: I was smaller than I was, fto see.... I miss it was at Hogwarts fifty years ago, I’m enclose a sign in the person’ book Albus Dumbledore. I should be, too.” Perhaps Doge had gone slightly ketchup. In ten minutes he looked around at the Ravenclaw table to see in some highhearted distance curtains visible behind the little sun. “Last year,” said Harry, “what’s going on this time? Oh, haven�\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 4300\n",
      "Loss 1.4685349464416504\n",
      "Sample Generation\n",
      "You're a wizard Harry was conscious of the fact that you fought a flobener than welcome. Think you would like him was he, to case you were at Gringotts, the fact that you all know and your scaries are a reward for his emotions. But I was wiser that you were all progress between you and spells. And I had a vision stripping off, out of a cupboard of my perf stupidity had plink it.” Harry Potter and the Goblet of Fire - J.K. Rowling Harry stared at names. If the old one of them made him funny, he saw, stared up at the knuckles in horror. A relaxed, also trailing on the floor beneath them, beneath a spouthed window. “Sometimes,” said Harry, “but more than I was supposed to go to every Quidditch match, I’m sure, that it’s better enough for Voldemort and I love it if I’re out.�\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 4400\n",
      "Loss 1.4699130058288574\n",
      "Sample Generation\n",
      "You're a wizard Harry’s eyes and Dumbledore, busy-forgivable fashion. “You need support Harry — ” Harry began bald not listening, at all; everything else had gone numb red but he did not dare look over Ron’s face, and Hermione seemed to be pleading with him. “ — you know! Dumbledore don’t know you’d have to hurt you if you don’t come!” There was a awkward silence in which Ron had gone as he shouted more on — Harry’s head was pounding so cold he felt slowly trud downward all the time that it was no good... “What if it’s goings happen,” said Hermione suddenly, checking her watch every word from him. Slowly and hopelessly stillness in his senses, Hermione snatched her own wand from Harry’s ankle and her limp. It was as though his birthday cake was blossomed from Goyle. “Can’\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 4500\n",
      "Loss 1.3839797973632812\n",
      "Sample Generation\n",
      "You're a wizard Harry tracked her down. There were several goblins, hooded Death Eaters and Death Eaters who went in to Death Eaters, were overpowered. They stood up — Harry wondered how the Death Eaters had fallen at the train that Bill and Fleur had taken from Mr. and Mrs. Weasley. “Some of ’s house up now,” Ron muttered under his breath. “That’s Zacharias Smith,” said Harry, stopping eye staring at the crowd, “and all the same and bird’s simply turned up toad at the Ministry’s — it accidentally put hold on — ” “Hang on, you go,” said Hermione coldly. “Sorry, ’Malfoy and Ron were up a month, he was on level one of the rest of the summer, we followed him, not that precious reason he could be — ” “But what are these attacks — �\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 4600\n",
      "Loss 1.3814369440078735\n",
      "Sample Generation\n",
      "You're a wizard Harry Potter, remember? Bludgers injured a better player, I hope!” She was still pink-faced; Harry could not help grinning at Hermione. “Okay, then, yeah — better,” she said. “We do have to miss the person who has finished the sword last year, and the food is while Gorggown.” Harry made sure Mr. Weasley had a loud jump with the twins parted force of work, then treated his sons as he threw his to the underground room with his back pocket. The idea of howling and, it had to get hold of the worst. After a few minutes it had occurred to Harry and Ron, who followed Zabini on and Zabini forward, leapt up and made no question, tried to pull out the news as much to him by doing. As Hagrid wasn’t smiling, nearly as pleasant as he could have done on the time. Harry watched Hepzibbler, although\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 4700\n",
      "Loss 1.4127154350280762\n",
      "Sample Generation\n",
      "You're a wizard Harry, eh?” He pointed at the twins. “You haven’t enjoyed things in my family anymore, Hermione?” “He was looking for the Crouch,” said Sirius, “not when he’s been alone you,” said Ron, “of course there was a shocked Muggle who came and got blown away.” “Ah, how did you —?” Harry was thinking about Black now, as Hermione said, “he won’t got his fingers crossed, but he might’ve read it.” Sirius flashed across his long, narrow face, with his mirror, lightly. “I am Cornelius Fudge, unlike you, I have inherited your father’s old chances of spreading venom from his family.” “Then 111 write to tell us what’s going on in store?” said Harry as the bedroom door opened again and Ron\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 4800\n",
      "Loss 1.46016263961792\n",
      "Sample Generation\n",
      "You're a wizard Harry dressed so, we’re all up to that deadly now, but it’s faces, not much to eat as anyone else’s and dangerous as I am to have done — ” He saw a muffled tugged at the boxes. Harry Potter and the Death Eater looked down: Inferi paid and seemed to fluttering too fast to take solid metal and hugging the other members. “Seen the Order, what do you want?” said Stan, holding up theYES” he said, blushing furiously. “Dadwikeifiable quicker than the world looks on every half the world sees us in here, you’ve got the Invisibility Cloak,” said Ron. “That Mark’ll be the Whomping Willow! But I reckon we’re dangerous — ” “We’re our handle the car!” barked a few hours later, Harry and Hermione followed him\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Epoch 4900\n",
      "Loss 1.3682831525802612\n",
      "Sample Generation\n",
      "You're a wizard Harry’s wand hand,” said Dumbledore, prodding the foot of the thick mud with a candle and tapped the rapidly and adding three floors. Harry let go. The very same sensation as he glanced over his shoulder, sighed again, “OY!” lazily, as though itching to bewitch it properly, but forgotten what the hearing must once he have been stealing at Sirius. Suddenly, Sirius said nothing happened to be back. “Sirius Black is just inside Azkaban,” said Harry suddenly. Sirius looked at the paper. Mrs. Weasley’s mouth was still soggy, and stretched lower, squinting down from the position. “He’s — it’s just making them down — he’s in detention — ” Harry ran to Sirius’s feet. “I don’t care!” he said loudly. “What are you doing here, Sirius?�\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### DEFINE TRAINING PARAMETERS ###\n",
    "iterations = 5000\n",
    "max_len = 256\n",
    "evaluate_interval = 100\n",
    "embedding_dim = 384\n",
    "depth = 6\n",
    "num_heads = 8\n",
    "lr = 0.0005\n",
    "mini_batch_size = 64\n",
    "grad_accum_steps = 2\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "### DEFINE MODEL AND OPTIMIZER ###\n",
    "model = GPT(max_seq_len=max_len, \n",
    "            embed_dim=embedding_dim, \n",
    "            depth=depth, \n",
    "            num_heads=num_heads, \n",
    "            attn_p=0.1, \n",
    "            mlp_p=0.1, \n",
    "            proj_p=0.1, \n",
    "            pos_p=0.1)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "### DEFINE LOSS FUNCTION ###\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "### INSTANTIATE DATABUILDER ###\n",
    "dataset = DataBuilder(seq_len=max_len, tokenized_text=tokenized_data)\n",
    "\n",
    "### Define some Sample Text ###\n",
    "sample_text = \"You're a wizard Harry\"\n",
    "sample_tokens = torch.tensor(tokenizer(sample_text)[\"input_ids\"]).unsqueeze(0).to(DEVICE).long()\n",
    "\n",
    "for iteration in tqdm(range(iterations)):\n",
    "\n",
    "    ### Gradient Accumulation ###\n",
    "    for step in range(grad_accum_steps):\n",
    "        input_texts, labels = dataset.grab_random_batch(batch_size=mini_batch_size)\n",
    "        input_texts, labels = input_texts.to(DEVICE), labels.to(DEVICE)\n",
    "    \n",
    "        out = model.forward(input_texts)\n",
    "        out = out.reshape(-1, out.shape[-1])\n",
    "        labels = labels.reshape(-1)\n",
    "        loss = loss_fn(out, labels)\n",
    "        loss = loss/grad_accum_steps\n",
    "        loss.backward()\n",
    "        \n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if iteration % evaluate_interval == 0:\n",
    "        print(\"------------------------------------\")\n",
    "        print(f\"Iteration {iteration}\")\n",
    "        print(f\"Loss {loss.item()*grad_accum_steps}\")\n",
    "        generate_text = tokenizer.decode(model.write(sample_tokens, max_new_tokens=200)[0])\n",
    "        print(\"Sample Generation\")\n",
    "        print(generate_text)\n",
    "        print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c52d54-dd1d-4a7b-81b3-ef245da42c50",
   "metadata": {},
   "source": [
    "These seem to be a reasonable generation. There is still a lot that can be done to improve this, mainly in training on much larger datasets, but for now this should give you a good intution of how GPT comes together!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
