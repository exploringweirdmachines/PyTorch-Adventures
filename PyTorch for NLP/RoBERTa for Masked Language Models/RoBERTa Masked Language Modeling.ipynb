{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76d2a04e-9ef7-4bfa-a3d4-ae315e33f8fc",
   "metadata": {},
   "source": [
    "![banner](https://raw.githubusercontent.com/priyammaz/HAL-DL-From-Scratch/main/src/visuals/banner.png)\n",
    "\n",
    "# RoBERTa for Masked Language Modeling\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/priyammaz/HAL-DL-From-Scratch/blob/main/src/visuals/masked_language_modeling_vis.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Today we will be implementing an updated variant of the original [BERT](https://arxiv.org/abs/1810.04805) model known as [RoBERTa](https://arxiv.org/abs/1907.11692), or Robustly Optimized BERT Approach. Before we keep going though, lets quickly review BERT!\n",
    "\n",
    "### Bidirectional Encoder Representations from Transformers (BERT)\n",
    "\n",
    "BERT was a model proposed in 2018, whoes whole purpose; was to learn contextual understanding from unlabeled text. There are two main differences between this model and GPT:\n",
    "- BERT is bidirectional, the model looks both forwards and backwards, where GPT is causal, so it only looks backwards\n",
    "- BERT uses Masked Language Modeling + Next Sentence Prediction with an Encoder Transformer instead of Causal Language Modeling with a Decoder Transformer.\n",
    "\n",
    "### Encoders (BERT, RoBERTa, ViT)\n",
    "Encoder transformers have the ability to look at the entire sequence and are bidirectional. This means when we compute attention, words can look forward and backwards, which is perfect for situations where we have the entire sequence given and we want to make some predictions about it (i.e. image classification, sentiment analysis, speech recognition, etc...)\n",
    "\n",
    "### Decoders (GPT)\n",
    "Decoder transformers only have the ability to look at the past, meaning any prediction we make for a specific time in the sequence is based only on times that came before it. This is important for tasks like time-series forecasting or language generation. When we want to generate a sequence, we can only look at the words that have come already as the future words aren't available. \n",
    "\n",
    "### Language Pre-Training\n",
    "There are typically two types of language pretraining. Encoders use **Masked Language Modeling**. This is the process of taking a sentence, randomly removing words, and then having the model predict the missing words by looking at existing words before and after it. Decoders use **Causal Language Modeling**, where we will train a model to predict the word at time $n+1$ give all the words 1 to $n$. Today we will be looking at Causal Language Models as that is what GPT uses for pretraining. \n",
    "\n",
    "\n",
    "This is what BERT looks like!\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/priyammaz/HAL-DL-From-Scratch/blob/main/src/visuals/BERT_archtiecture.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "[Image Source](https://etc.cuit.columbia.edu/news/basics-language-modeling-transformers-bert)\n",
    "\n",
    "As you can see, BERT passes in two sentences of data, along with a learnable CLS token. The CLS token perform the Next Sentence Prediction task, basically does the second sentence provided actually follow the first? The rest of the model does Masked Language Modeling, to fill in the blanks in the masked sentence. \n",
    "\n",
    "### How Does RoBERTa Improve on BERT?\n",
    "\n",
    "- Train on a much larger dataset (didn't exist when BERT was made)\n",
    "- Remove the Next Sentence Prediction Task, which leads to slightly better performance\n",
    "- Dynamic Masking, in the dataloader, randomly mask in real time, rather than mask ahead of time and train on the same masks repeatedly. \n",
    "\n",
    "**Note**\n",
    "\n",
    "RoBERTA still includes the CLS token for downstream fine-tuning in the future, but because there is no loss attached to it, there is never any gradient updated during language pre-training.\n",
    "\n",
    "### Masked Language Model Attention\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/priyammaz/HAL-DL-From-Scratch/blob/9561e2968a28022c732c1b67b34c8e7d1c3319ce/src/visuals/mlm_attention.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "Masked language models will be a full encoder based transformer, that means all the values in the attention mechanism above will be computed. We specifically care most about the spots highlighted, how is the Masked token related to all the other words, and also how are all the other words related to the masked token (therefore bidirectional).\n",
    "\n",
    "### How to Compute Attention\n",
    "\n",
    "So how do we actually perform attention masking? Lets first remind ourselves what attention is doing. Here is the equation for Attention as a reminder:\n",
    "\n",
    "$$\\text{Attention}(Q,K,V) = \\text{Softmax}(\\frac{QK^T}{\\sqrt{d_e}})V$$\n",
    "\n",
    "So the first step is the computing the $QK^T$, where Q and K both have the shape (Sequence Length x Embedding Dimension). The output of this computation will be sequence length x sequence length. This is what it looks like!\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/priyammaz/HAL-DL-From-Scratch/blob/main/src/visuals/computing_attention.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "In the image above, I also applied the softmax (not shown for simplicity), so each row of the attention matrix adds up to 1 (like probabilities).\n",
    "\n",
    "Now we can multiply our output of $QK^T$ with our $V$. This is what a regular encoder (bidirectional) attention will look like. Remember, $V_1, ... V_4$ are the projection vectors (Values) of the data, and the attention matrix is a weighted average of all of these vectors. \n",
    "\n",
    "**Note**\n",
    "\n",
    "In transformers, our input $X$ goes through 3 linear projections to create $Q, K, \\text{and } V$ Initially, the each vector for each word in $X$ is the embedding vector representing that word. After the attention computation, each vector for each word isn't just the embedding of that word but rather a weighted average of all the vectors in the sequence and how they are related to the word of interest. \n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/priyammaz/HAL-DL-From-Scratch/blob/main/src/visuals/encoder_attention_vis.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Sequence Padding and Attention Masking\n",
    "\n",
    "One typical consideration for models like this is, when we pass in sentences of data, different sentences have different lengths. To create a matrix of tokens, we need to make sure all the sequences are the same, therefore we have to pad the shorter sequences to the longer ones. This is what the padding looks like!\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/priyammaz/HAL-DL-From-Scratch/blob/main/src/visuals/sequence_padding.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "Now the pad tokens are just fillers, they don't add any information at all to the data. To deal with this, we need to make sure that when we compute attention, these pad tokens are ignored. This is very similar to the Causal Mask in [GPT](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20for%20NLP/GPT), and we will be doing exactly what we did there, fill in the masked locations with $-\\infty$ before softmax is computed. Lets take a quick look at this for one of the sentences above. \n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/priyammaz/HAL-DL-From-Scratch/blob/main/src/visuals/padding_attention_mask.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "You can see that we zero out the columns of our attention mask that has padding tokens, but not the row. The reason for this is, the model should learn the relation of how the padding tokens are related to the words (this is most likely just positional information that padding is at the end), but it should not learn how other words are related to padding, as the padding adds no semantic meaning. This is why, when multiplying by our values, we have zeroed out the attention weights of the 4th column, as when computing weighted averages of how one word is related to all others, the padding token is not included. \n",
    "\n",
    "\n",
    "\n",
    "## Computing the Reweighted Padded Attention Mask\n",
    "\n",
    "Lets create some numbers so we can get a better idea of how this works. Let the tokens be $X = [10, 2, \\text{<pad>}]$, so the third token is a padding token. Lets then also pretend, we pass this to our RoBERTa model, and when we go to compute our attention $QK^T$, the raw output before the softmax is below.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "  7       & -8   & 6  \\\\\n",
    "  -3       & 2   & 4   \\\\\n",
    "  1       & 6  & -2   \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Remember, the equation for softmax is:\n",
    "\n",
    "$$\\text{Softmax}(\\vec{x}) = \\frac{e^{x_i}}{\\sum_{j=1}^N{e^{x_j}}}$$\n",
    "\n",
    "Then, if we ignore padding and everything right now, we can compute softmax for row of the matrix above:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Softmax}\n",
    "\\begin{bmatrix}\n",
    "  7       & -8   & 6  \\\\\n",
    "  -3       & 2   & 4   \\\\\n",
    "  1       & 6  & -2   \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "  \\frac{e^{7}}{e^{7}+e^{-8}+e^{6}}       & \\frac{e^{-8}}{e^{7}+e^{-8}+e^{6}}   & \\frac{e^{6}}{e^{7}+e^{-8}+e^{6}}  \\\\\n",
    "  \\frac{e^{-3}}{e^{-3}+e^{2}+e^{4}}       & \\frac{e^{2}}{e^{-3}+e^{2}+e^{4}}   & \\frac{e^{4}}{e^{-3}+e^{2}+e^{4}}  \\\\\n",
    "  \\frac{e^{1}}{e^{1}+e^{6}+e^{-2}}       & \\frac{e^{6}}{e^{1}+e^{6}+e^{-2}}   & \\frac{e^{-2}}{e^{1}+e^{6}+e^{-2}}  \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "  0.73       & 0.0000002   & 0.27   \\\\\n",
    "  0.0008       & 0.12   & 0.88 \\\\\n",
    "  0.007       & 0.99  & 0.003  \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "But what we need is to mask out all the tokens in this matrix related to padding. Just like we did in [GPT](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20for%20NLP/GPT), we will fill in the indexes of the that we want to mask with $-\\infty$. If only the last token was a padding token in our sequence, then the attention before the softmax should be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "  7       & -8   & -\\infty  \\\\\n",
    "  -3       & 2   & -\\infty   \\\\\n",
    "  1       & 6  & -\\infty  \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Taking the softmax of the rows of this matrix then gives:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Softmax}\n",
    "\\begin{bmatrix}\n",
    " 7       & -8   & -\\infty  \\\\\n",
    "  -3       & 2   & -\\infty   \\\\\n",
    "  1       & 6  & -\\infty  \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "  1       & 0   & 0  \\\\\n",
    "  0.0067  & 0.9933 & 0   \\\\\n",
    "  0.0067       & 0.9933  & 0  \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Which is exactly what we want! Therefore, when computing attention, we need to know which tokens are padding tokens in our data, and then go ahead and perform this computation on our attention output before multiplying with values.\n",
    "\n",
    "\n",
    "### Masked Language Modeling Logic\n",
    "\n",
    "Last piece! How do we do the masked language modeling? We will follow exactly what RoBERTa and BERT had done:\n",
    "- We will randomly mask out 15 percent of the words in our sentence. Of the 15 percent selected to be masked we will:\n",
    "    - 80% of the time we will replace it with a mask token\n",
    "    - 10% of the time it will be replaced with some random token (not mask token)\n",
    "    - 10% of the time it will be kept the same\n",
    " \n",
    "The reason for this logic is, in the real world, there are no mask tokens, just regular sentences. This way the model will have some experience understanding that a word is misplaced (a random word inserted) or that a word is correct. Regardless, the masked words will be predicted, where most of the times the input is the mask token, but sometimes it will just be exactly the word it was to begin with or some random word. \n",
    "\n",
    "\n",
    "### Tokenizer\n",
    "\n",
    "In our case, we will use the RoBERTa tokenizer found in Huggingface! RoBERTA tokenizer is exactly the same as the GPT2 Tokenizer, except it includes some of these special tokens like the Mask and Pad tokens that we need!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d8cb018-909f-4e80-8b42-b14c63966e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9225c9ab-123d-4c6f-bdd5-ac73310c08e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Tokens in Tokenizer\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n",
      "Special Token Index\n",
      "{'<s>': 0, '</s>': 2, '<unk>': 3, '<pad>': 1, '<mask>': 50264}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "print(\"Special Tokens in Tokenizer\")\n",
    "special_tokens = tokenizer.special_tokens_map\n",
    "print(special_tokens)\n",
    "\n",
    "special_token_idx = {}\n",
    "for token in special_tokens.values():\n",
    "    special_token_idx[token] = tokenizer.encode(token, add_special_tokens=False)[0]\n",
    "\n",
    "print(\"Special Token Index\")\n",
    "print(special_token_idx)\n",
    "\n",
    "\n",
    "### Get Indexes of All Types of Tokens ###\n",
    "all_tokens_idx = list(range(tokenizer.vocab_size))\n",
    "all_special_tokens_idx = sorted(list(special_token_idx.values()))\n",
    "all_non_special_tokens_idx = [tok for tok in all_tokens_idx if tok not in all_special_tokens_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73397168-70a9-4d5e-b5d4-afa87533ac6e",
   "metadata": {},
   "source": [
    "### Prepare Dataset\n",
    "\n",
    "Lets perpare our Harry Potter dataset again! In this case, we will split our data into sentences, and then grab 5 sentences per chunk for our model. Theres tons of ways to do this, but I will just split on the periods and then put them back together in chunks. This is definitely not perfect as we will split on periods that are not end of sentences (Mr., Mrs., ...) but this should be ok for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f1f03b2-bc6c-4ea5-b540-2d099e15728d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (601 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "path_to_data = \"../../data/harry_potter_txt/\"\n",
    "\n",
    "text_files = os.listdir(path_to_data)\n",
    "\n",
    "all_text = \"\"\n",
    "for book in text_files:\n",
    "    with open(os.path.join(path_to_data, book), \"r\") as f:\n",
    "        text = f.readlines() # Read in all lines\n",
    "        text = [line for line in text if \"Page\" not in line] # Remove lines with Page Numbers\n",
    "        text = \" \".join(text).replace(\"\\n\", \"\") # Remove all newline characters\n",
    "        text = [word for word in text.split(\" \") if len(word) > 0] # Remove all empty characters\n",
    "        text = \" \".join(text) # Combined lightly cleaned text\n",
    "        all_text += text\n",
    "\n",
    "### Split Data by \"sentences\" ###\n",
    "all_text = all_text.split(\".\")\n",
    "\n",
    "### Grab 5 Sentences at a time and put them together by the period ###\n",
    "all_text_chunked = [\".\".join(all_text[i:i+5]) for i in range(0, len(all_text), 5)]\n",
    "\n",
    "### Tokenize all the text! ###\n",
    "tokenized_text = [tokenizer.encode(text) for text in all_text_chunked]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af30908-89e4-42aa-86d0-9398a17d1331",
   "metadata": {},
   "source": [
    "### Write a Dataset Class\n",
    "\n",
    "Now that we have our tokenized data ready, we need to write a dataloader. This dataloader should perform a few things:\n",
    "\n",
    "- Padding to the longest sequence length\n",
    "- Masking with the ratios shown above (ensuring that we dont mask any special tokens or padding tokens)\n",
    "\n",
    "#### What are we predicting?\n",
    "\n",
    "Quick example: \n",
    "\n",
    "Lets pretend our input to the model is: [10, 5, 2, 6, 1, 4, 13, 1, 4, <pad>, <pad>]. \n",
    "\n",
    "After masking it can be something like [10, <mask>, 2, 6, 1, <mask>, 13, 1, 4, <pad>, <pad>].\n",
    "\n",
    "Then our final prediction vector is [-100, 5, -100, -100, -100, 4, -100, -100, -100, -100, -100]\n",
    "\n",
    "Where did all the -100 come from? This is a classification problem in the end so we will be using CrossEntropyLoss. In the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), we see that there is a argument called *ignore_index=-100*. When we compute loss, we only want to do this for the masked tokens, and ignore the unmasked ones. This is why in our label, we will set the masked token to the index its supposed to predict, and everything else as -100!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "263654e6-76d7-47d9-bf07-f8cb6648c4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,    73, 39684,  9963, 50264, 13387,   226,  6372,  1691,   427,\n",
      "            4,     8,  3801, 50264,   211,  4668,   607,     6,     9,   346,\n",
      "          237,     6, 50264, 18068,  3936, 50264,    58,  2602,     7, 50264,\n",
      "        50264,    51,    58,  6683,  2340,     6,  3392,    47,   182,   203,\n",
      "            4,   252,    58,     5,    94, 50264, 50264,    17,    27,   417,\n",
      "         1057, 50255,    28,   963,    11,   932,  7782,    50, 12754,     6,\n",
      "          142, 50264,    95,   399,    17,    27,    90, 50264,    19,   215,\n",
      "        20175, 50264,   427,     2])\n",
      "tensor([-100, -100, 1941, -100,  975, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100,    4, -100, -100, -100, -100, -100, -100, -100, -100, 9522, -100,\n",
      "        -100,    6, -100, -100, -100,  224,   14, -100, -100, -100, -100, -100,\n",
      "        3392, -100, -100, -100, -100, -100, -100, -100, -100,   82,   47, -100,\n",
      "        -100, -100, -100,    7, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100,   51, -100, -100, -100, -100,   90,  946, -100, -100, -100,    4,\n",
      "        -100, -100])\n"
     ]
    }
   ],
   "source": [
    "class MaskedLMLoader(Dataset):\n",
    "    def __init__(self, tokenized_data, max_seq_len=100, masking_ratio=0.15):\n",
    "        self.data = tokenized_data\n",
    "        self.mask_ratio = masking_ratio\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _random_mask_text(self, tokens):\n",
    "\n",
    "        ### Create Random Uniform Sample Tensor ###\n",
    "        random_masking = torch.rand(*tokens.shape)\n",
    "\n",
    "        ### Set Value of Special Tokens to 1 so we DONT MASK THEM ###\n",
    "        special_tokens = torch.tensor(tokenizer.get_special_tokens_mask(tokens, already_has_special_tokens=True))\n",
    "        random_masking[special_tokens==1] = 1\n",
    "\n",
    "        ### Get Boolean of Words under Masking Threshold ###\n",
    "        random_masking = (random_masking < self.mask_ratio)\n",
    "\n",
    "        ### Create Labels ###\n",
    "        labels = torch.full((tokens.shape), -100)\n",
    "        labels[random_masking] = tokens[random_masking]\n",
    "\n",
    "        ### Get Indexes of True ###\n",
    "        random_selected_idx = random_masking.nonzero()\n",
    "\n",
    "        ### 80% Of the Time Replace with Mask Token ###\n",
    "        masking_flag = torch.rand(*random_selected_idx.shape)\n",
    "        masking_flag = (masking_flag<0.8)\n",
    "        selected_idx_for_masking = random_selected_idx[masking_flag]\n",
    "\n",
    "        ### Seperate out remaining indexes to be assigned ###\n",
    "        unselected_idx_for_masking = random_selected_idx[~masking_flag]\n",
    "\n",
    "        ### 10% of the time (or 50 percent of the remaining 20%) we fill with random token ###\n",
    "        ### The remaining times, leave the text as is ###\n",
    "        masking_flag = torch.rand(*unselected_idx_for_masking.shape)\n",
    "        masking_flag = (masking_flag<0.5)\n",
    "        selected_idx_for_random_filling = unselected_idx_for_masking[masking_flag]\n",
    "        selected_idx_to_be_left_alone = unselected_idx_for_masking[~masking_flag]\n",
    "        \n",
    "        ### Fill Mask Tokens ###\n",
    "        if len(selected_idx_for_masking) > 0:\n",
    "            tokens[selected_idx_for_masking] = special_token_idx[\"<mask>\"]\n",
    "        \n",
    "        ### Fill Random Tokens ###\n",
    "        if len(selected_idx_for_random_filling) > 0:\n",
    "            randomly_selected_tokens = torch.tensor(random.sample(all_non_special_tokens_idx, len(selected_idx_for_random_filling)))\n",
    "            tokens[selected_idx_for_random_filling] = randomly_selected_tokens\n",
    "        \n",
    "        \n",
    "        return tokens, labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.tensor(self.data[idx])\n",
    "\n",
    "        ### Make sure sequence is within context length ###\n",
    "        if len(data) > self.max_seq_len:\n",
    "            rand_start_idx = random.choice(list(range(len(data) - self.max_seq_len)))\n",
    "            end_idx = rand_start_idx + self.max_seq_len\n",
    "            data = data[rand_start_idx:end_idx]\n",
    "  \n",
    "        ### Uniform Random Masking ###\n",
    "        masked_tokens, label = self._random_mask_text(data)\n",
    "\n",
    "        return masked_tokens, label\n",
    "\n",
    "mlm = MaskedLMLoader(tokenized_text)\n",
    "\n",
    "for masked_tokens, labels in mlm:\n",
    "    print(masked_tokens)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63106ec4-61ab-472e-81a8-e1cb4b15680c",
   "metadata": {},
   "source": [
    "### Write a DataLoader Class\n",
    "\n",
    "Now that we have our masking and labels generation ready to go for our dataset, we now need to write a DataLoader! In this dataloader, needs some additional logic though:\n",
    "\n",
    "- Data collation to pad to longest sequence so we can stack our tensors together\n",
    "- Pad the labels with more -100 as well, because we dont predict on padding tokens \n",
    "- Padding mask which will be used as our attention mask later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73f393ce-d985-4756-b491-b73265b82462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,    73,  1941,  ...,     1,     1,     1],\n",
      "        [   21,     5,   736,  ...,   117, 37075,  2143],\n",
      "        [    0,    20,   211,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,  1993,     9,  ...,     1,     1,     1],\n",
      "        [   10,   182,   205,  ..., 50264, 50264,  8664],\n",
      "        [    0,    85,    21,  ...,   357,     9,    24]])\n",
      "tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ...,   10,  881, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]])\n",
      "tensor([[False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        ...,\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "\n",
    "    ### Grab Tokens and their Labels ###\n",
    "    token_samples = []\n",
    "    label_samples =[]\n",
    "\n",
    "    for token, label in batch:\n",
    "        token_samples.append(token)\n",
    "        label_samples.append(label)\n",
    "\n",
    "    ### Get Seq Len of Each Token along with Max Length ###\n",
    "    sequence_lengths = [len(tok) for tok in token_samples]\n",
    "    max_seq_len = max(sequence_lengths)\n",
    "\n",
    "    ### Append Padding Tokens to Token Samples, store padding masks and append more -100 for the labels ###\n",
    "    padding_masks = []\n",
    "    for idx in range(len(token_samples)):\n",
    "        sample = token_samples[idx]\n",
    "        seq_len = len(sample)\n",
    "        diff = max_seq_len - seq_len\n",
    "\n",
    "        if diff > 0:\n",
    "\n",
    "            ### Append Padding to Each Sequence ###\n",
    "            padding = torch.tensor([special_token_idx[\"<pad>\"] for _ in range(diff)])\n",
    "            sample = torch.concatenate((sample, padding))\n",
    "            token_samples[idx] = sample\n",
    "            \n",
    "            ### Append -100 to Labels (we dont predict padding masks) ###\n",
    "            label_padding = torch.tensor([-100 for _ in range(diff)])\n",
    "            label_samples[idx] = torch.concatenate((label_samples[idx], label_padding))\n",
    "\n",
    "            ### Store Padding Mask ###\n",
    "            padding_mask = (sample==special_token_idx[\"<pad>\"])\n",
    "            padding_masks.append(padding_mask)\n",
    "\n",
    "        else:\n",
    "\n",
    "            ### Padding Mask (with no padding) for longest sequence ###\n",
    "            padding_masks.append(torch.zeros(max_seq_len))\n",
    "\n",
    "    token_samples = torch.stack(token_samples)\n",
    "    label_samples = torch.stack(label_samples)\n",
    "    padding_masks = torch.stack(padding_masks)\n",
    "\n",
    "    assert token_samples.shape == label_samples.shape == padding_masks.shape\n",
    "    \n",
    "    batch = {\"input_ids\": token_samples, \n",
    "             \"labels\": label_samples, \n",
    "             \"attention_mask\": padding_masks.bool()}\n",
    "\n",
    "    return batch\n",
    "    \n",
    "### Test DataLoader ###\n",
    "dataloader = DataLoader(mlm, batch_size=16, collate_fn=collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch[\"input_ids\"])\n",
    "    print(batch[\"labels\"])\n",
    "    print(batch[\"attention_mask\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b1d0d5-f2bf-4500-a7b2-c18a4ebf3e02",
   "metadata": {},
   "source": [
    "### Attention Mechanism \n",
    "\n",
    "Now that we have completed all of our data prep, its time to move onto the attention mechanism implementation! This should be almost identical to the [Vision Transformer](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20for%20Computer%20Vision/Vision%20Transformer) implementation, with the additional logic for masking added in, which will be similar to our [GPT](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20for%20NLP/GPT) implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1573e7c4-e432-4bfd-8bc4-2f5f94d7235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "class SelfAttentionEncoder(nn.Module):\n",
    "  def __init__(self,\n",
    "               embed_dim=768,\n",
    "               num_heads=12, \n",
    "               attn_p=0,\n",
    "               proj_p=0):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "\n",
    "        embed_dim: What is the embedding dimension of each embedding vector\n",
    "        num_heads: How many heads of attention do we want?\n",
    "        attn_p: Dropout probability on Attention\n",
    "        proj_p: Dropout probability on projection matrix\n",
    "    \"\"\"\n",
    "    super(SelfAttentionEncoder, self).__init__()\n",
    "    assert embed_dim % num_heads == 0\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = int(embed_dim / num_heads)\n",
    "    self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    self.qkv = nn.Linear(embed_dim, embed_dim*3)\n",
    "    self.attn_p = attn_p\n",
    "    self.attn_drop = nn.Dropout(attn_p)\n",
    "    self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "    self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "  def forward(self, x, attention_mask=None):\n",
    "    batch_size, seq_len, embed_dim = x.shape\n",
    "    qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "    qkv = qkv.permute(2,0,3,1,4)\n",
    "    q,k,v = qkv.unbind(0)\n",
    "\n",
    "    # B x H x S x S\n",
    "    attn = (q @ k.transpose(-2,-1)) * self.scale\n",
    "\n",
    "    ####################################################################################\n",
    "    ### FILL ATTENTION MASK WITH -Infinity ###\n",
    "\n",
    "    if attention_mask is not None:\n",
    "        ### We Need to Unsqueeze Attention Mask to have placeholder dimensions for Num Heads and Seq Len ###\n",
    "\n",
    "        # B, S -> B, 1, 1, S\n",
    "        attention_mask = attention_mask.unsqueeze(1).unsqueeze(1)\n",
    "        attn = attn.masked_fill(attention_mask, float('-inf'))\n",
    "\n",
    "    ####################################################################################\n",
    "\n",
    "    attn = attn.softmax(dim=-1)\n",
    "    attn = self.attn_drop(attn)\n",
    "    x = attn @ v\n",
    "\n",
    "    x = x.transpose(1,2).reshape(batch_size, seq_len, embed_dim)\n",
    "    x = self.proj(x)\n",
    "    x = self.proj_drop(x)\n",
    "      \n",
    "    return x\n",
    "\n",
    "### Lets test Attention Mechanism ###\n",
    "rand_x = torch.randn(2,5,16)\n",
    "padding = torch.tensor([[False, False, False, True, True], \n",
    "                        [False, False, False, False, False]])\n",
    "\n",
    "a = SelfAttentionEncoder(embed_dim=16, num_heads=4)\n",
    "out = a(rand_x, padding)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c294ccbb-0a30-4d1c-a040-0fa61aa88de8",
   "metadata": {},
   "source": [
    "### Define the Rest of The Transformer\n",
    "\n",
    "All we have left is to implement the rest of the model! Again, this should be very similar to the [Vision Transformer](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20for%20Computer%20Vision/Vision%20Transformer) and [GPT](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20for%20NLP/GPT)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bf554dc-97ce-4935-9910-c55dfb64fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "### DEFINE THE MULTILAYER PERCEPTRON ###\n",
    "########################################\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_features,\n",
    "                 hidden_features,\n",
    "                 out_features,\n",
    "                 act_layer=nn.GELU,\n",
    "                 mlp_p=0):\n",
    "\n",
    "\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.drop1 = nn.Dropout(mlp_p)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop2 = nn.Dropout(mlp_p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "\n",
    "################################################################\n",
    "### PUT ATTENTION AND MLP TOGETHER WITH RESIDUAL CONNECTIONS ###\n",
    "################################################################\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embed_dim=768, \n",
    "                 num_heads=12, \n",
    "                 mlp_ratio=4, \n",
    "                 proj_p=0., \n",
    "                 attn_p=0., \n",
    "                 mlp_p=0., \n",
    "                 act_layer=nn.GELU, \n",
    "                 norm_layer=nn.LayerNorm):\n",
    "\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(embed_dim, eps=1e-6)\n",
    "        self.attn = SelfAttentionEncoder(embed_dim=embed_dim,\n",
    "                                         num_heads=num_heads, \n",
    "                                         attn_p=attn_p,\n",
    "                                         proj_p=proj_p)\n",
    "\n",
    "\n",
    "        self.norm2 = norm_layer(embed_dim, eps=1e-6)\n",
    "        self.mlp = MLP(in_features=embed_dim,\n",
    "                       hidden_features=int(embed_dim*mlp_ratio),\n",
    "                       out_features=embed_dim,\n",
    "                       act_layer=act_layer,\n",
    "                       mlp_p=mlp_p)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        x = x + self.attn(self.norm1(x), attention_mask)\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac4746a-3225-48a8-9b6f-6f476f8177d8",
   "metadata": {},
   "source": [
    "### Write RoBERTa\n",
    "\n",
    "The final step! Lets put it all together to have our RoBERTa class! This isn't exactly RoBERTa, it is smaller, and I think some of the activations are different, but most of it should be pretty close! Here are the things we will be doing here:\n",
    "\n",
    "- Word Embeddings: We need to create an embedding matrix for our token embeddings\n",
    "- Positional Embeddings: We need a second embedding matrix for the positional encodings. We could have used Sin-Cosine embeddings like the original [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper, but we will used learned embeddings in this case. Theres lots of options though like [Rotary Embeddings](https://arxiv.org/abs/2104.09864) or [ALiBi](https://arxiv.org/abs/2108.12409) that are definitely better, but this is a start! **Also note, if our sequence length is 512, then our total number of positions is 513 due to the extra CLS token that also needs positional information. \n",
    "- CLS Token, although not used in our model for pretraining right now, we will leave it as it would be necessary for downstream finetuning. \n",
    "- Blocks: Stack of our transformer blocks\n",
    "- Prediction Head: For each embedding vector, we need to predict the next work, so this will predict our vocab size of the tokenizer.\n",
    "\n",
    "#### Weight Sharing\n",
    "Intuitively, the weight matrix that takes in indexes for tokens and converts to the embedding vectors and the prediction head that takes embedding vectors and predicts the token indexes are doing the same thing. A trick we can use is weight sharing, just make the weights of the word embeddings equal to the weights of the prediction heads, as these vectors have to serve the same purpose!\n",
    "\n",
    "#### Weight Initialization Strategy\n",
    "There is a lot of work in how different weight initialization effects transformers. We will just go with the typical one that I have seen of truncated normal initialization on all weight layers and biases set to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67d133c8-9e9d-4d10-a529-9da4b0c93954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 50265])\n"
     ]
    }
   ],
   "source": [
    "class RoBERTa(nn.Module):\n",
    "    def __init__(self, \n",
    "                 max_seq_len=512,\n",
    "                 vocab_size=tokenizer.vocab_size,\n",
    "                 embed_dim=768, \n",
    "                 depth=12, \n",
    "                 num_heads=12, \n",
    "                 mlp_ratio=4, \n",
    "                 attn_p=0., \n",
    "                 mlp_p=0., \n",
    "                 proj_p=0., \n",
    "                 pos_p=0., \n",
    "                 act_layer=nn.GELU, \n",
    "                 norm_layer=nn.LayerNorm):\n",
    "\n",
    "        super(RoBERTa, self).__init__()\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
    "        self.pos_embed = nn.Embedding(max_seq_len+1, embed_dim)\n",
    "        self.pos_drop = nn.Dropout(pos_p)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(embed_dim=embed_dim, \n",
    "                      num_heads=num_heads, \n",
    "                      mlp_ratio=mlp_ratio, \n",
    "                      proj_p=proj_p, \n",
    "                      attn_p=attn_p, \n",
    "                      mlp_p=mlp_p, \n",
    "                      act_layer=act_layer, \n",
    "                      norm_layer=norm_layer)\n",
    "\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        device = x.device\n",
    "\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        ### If we have too long of a sequence length, grab the last chunk ###\n",
    "        if seq_len > self.max_seq_len:\n",
    "            x = x[:, -self.max_seq_len:]\n",
    "\n",
    "        ### We only need the positional information upto the length of data we have plus CLS token ###\n",
    "        avail_idx = torch.arange(0, seq_len+1, dtype=torch.long, device=device)\n",
    "\n",
    "        ### Embed all the Tokens ###\n",
    "        tok_emb = self.embeddings(x)\n",
    "\n",
    "        ### Concatenate on the CLSL Token ###\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1) # (batch, 1, embed_dim)\n",
    "        tok_emb = torch.cat((cls_token, tok_emb), dim=1) \n",
    "\n",
    "        ### Add positional information ###\n",
    "        pos_emb = self.pos_embed(avail_idx)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        ### Slice Off CLS token ###\n",
    "        cls_token_final = x[:, 0] \n",
    "\n",
    "        ### Slice off Remaining Tokens ###\n",
    "        x = x[:, 1:]\n",
    "\n",
    "        ### MLM Prediction Head ###\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "        \n",
    "\n",
    "### Test Model ###\n",
    "rand_x = torch.randint(0,10, (2,5))\n",
    "padding = torch.tensor([[False, False, False, True, True], \n",
    "                        [False, False, False, False, False]])\n",
    "\n",
    "m = RoBERTa()\n",
    "out = m(rand_x, padding)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "438399f4-5d5b-4d9b-b90c-f18bb36e247d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ab8310fa5b49c08f57d538cbb6e9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 Training Loss: 10.396774616241455 Validation Loss: 8.532480852944511\n",
      "Iteration 200 Training Loss: 7.287039680480957 Validation Loss: 6.723119871956961\n",
      "Iteration 300 Training Loss: 6.612097492218018 Validation Loss: 6.475468805858067\n",
      "Iteration 400 Training Loss: 6.457984824180603 Validation Loss: 6.464910098484585\n",
      "Iteration 500 Training Loss: 6.365998892784119 Validation Loss: 6.289786202566964\n",
      "Iteration 600 Training Loss: 6.183056969642639 Validation Loss: 6.12600530896868\n",
      "Iteration 700 Training Loss: 5.877160429954529 Validation Loss: 5.70405718258449\n",
      "Iteration 800 Training Loss: 5.53846782207489 Validation Loss: 5.371514763150897\n",
      "Iteration 900 Training Loss: 5.2069326639175415 Validation Loss: 5.177849701472691\n",
      "Iteration 1000 Training Loss: 4.979501166343689 Validation Loss: 4.940764222826276\n",
      "Iteration 1100 Training Loss: 4.828522047996521 Validation Loss: 4.728730201721191\n",
      "Iteration 1200 Training Loss: 4.6567385339736935 Validation Loss: 4.648167882646833\n",
      "Iteration 1300 Training Loss: 4.527936668395996 Validation Loss: 4.507080316543579\n",
      "Iteration 1400 Training Loss: 4.381747605800629 Validation Loss: 4.393998793193272\n",
      "Iteration 1500 Training Loss: 4.29737401008606 Validation Loss: 4.242664507457188\n",
      "Iteration 1600 Training Loss: 4.1949028658866885 Validation Loss: 4.203445093972342\n",
      "Iteration 1700 Training Loss: 4.050106267929078 Validation Loss: 4.1683052607945035\n",
      "Iteration 1800 Training Loss: 3.9920168137550354 Validation Loss: 4.137008905410767\n",
      "Iteration 1900 Training Loss: 3.904230887889862 Validation Loss: 3.989753348486764\n",
      "Iteration 2000 Training Loss: 3.82718731880188 Validation Loss: 3.992652007511684\n",
      "Iteration 2100 Training Loss: 3.8077615690231323 Validation Loss: 3.8230193853378296\n",
      "Iteration 2200 Training Loss: 3.7069936871528624 Validation Loss: 3.755800689969744\n",
      "Iteration 2300 Training Loss: 3.685771095752716 Validation Loss: 3.77313722882952\n",
      "Iteration 2400 Training Loss: 3.604854073524475 Validation Loss: 3.744066153253828\n",
      "Iteration 2500 Training Loss: 3.5379211044311525 Validation Loss: 3.714391197477068\n",
      "Iteration 2600 Training Loss: 3.5507048058509825 Validation Loss: 3.6326902423586165\n",
      "Iteration 2700 Training Loss: 3.525896065235138 Validation Loss: 3.6989753927503313\n",
      "Iteration 2800 Training Loss: 3.417039074897766 Validation Loss: 3.5992872544697354\n",
      "Iteration 2900 Training Loss: 3.4062248826026917 Validation Loss: 3.5974040031433105\n",
      "Iteration 3000 Training Loss: 3.3325670838356016 Validation Loss: 3.598765390259879\n",
      "Iteration 3100 Training Loss: 3.319270520210266 Validation Loss: 3.507104056222098\n",
      "Iteration 3200 Training Loss: 3.2874491834640502 Validation Loss: 3.528126205716814\n",
      "Iteration 3300 Training Loss: 3.2833287215232847 Validation Loss: 3.4857761008398875\n",
      "Iteration 3400 Training Loss: 3.2596962237358094 Validation Loss: 3.4977618966783797\n",
      "Iteration 3500 Training Loss: 3.1815659260749816 Validation Loss: 3.401544979640416\n",
      "Iteration 3600 Training Loss: 3.167032480239868 Validation Loss: 3.450705443109785\n",
      "Iteration 3700 Training Loss: 3.166389856338501 Validation Loss: 3.359795962061201\n",
      "Iteration 3800 Training Loss: 3.1136248898506165 Validation Loss: 3.382743631090437\n",
      "Iteration 3900 Training Loss: 3.088370225429535 Validation Loss: 3.3708793946674893\n",
      "Iteration 4000 Training Loss: 3.091523916721344 Validation Loss: 3.350175380706787\n",
      "Iteration 4100 Training Loss: 3.037866539955139 Validation Loss: 3.362573334148952\n",
      "Iteration 4200 Training Loss: 3.0470175766944885 Validation Loss: 3.289916890008109\n",
      "Iteration 4300 Training Loss: 2.9791553473472594 Validation Loss: 3.279476591518947\n",
      "Iteration 4400 Training Loss: 2.9741266846656798 Validation Loss: 3.2961473975862776\n",
      "Iteration 4500 Training Loss: 2.9924969696998596 Validation Loss: 3.3110649245125905\n",
      "Iteration 4600 Training Loss: 2.9301219749450684 Validation Loss: 3.2209200177873885\n",
      "Iteration 4700 Training Loss: 2.9295986032485963 Validation Loss: 3.2275670085634505\n",
      "Iteration 4800 Training Loss: 2.8880635499954224 Validation Loss: 3.141775369644165\n",
      "Iteration 4900 Training Loss: 2.88132716178894 Validation Loss: 3.187710336276463\n",
      "Iteration 5000 Training Loss: 2.8881617999076843 Validation Loss: 3.169599550110953\n",
      "Iteration 5100 Training Loss: 2.829318447113037 Validation Loss: 3.174006836754935\n",
      "Iteration 5200 Training Loss: 2.8388627696037294 Validation Loss: 3.228300792830331\n",
      "Iteration 5300 Training Loss: 2.835542004108429 Validation Loss: 3.179899607385908\n",
      "Iteration 5400 Training Loss: 2.7738899397850036 Validation Loss: 3.193228687558855\n",
      "Iteration 5500 Training Loss: 2.808390588760376 Validation Loss: 3.142833488328116\n",
      "Iteration 5600 Training Loss: 2.7694687175750734 Validation Loss: 3.1610059227262224\n",
      "Iteration 5700 Training Loss: 2.7455975365638734 Validation Loss: 3.117065872464861\n",
      "Iteration 5800 Training Loss: 2.7703906059265138 Validation Loss: 3.129078047616141\n",
      "Iteration 5900 Training Loss: 2.7158729434013367 Validation Loss: 3.1145948512213573\n",
      "Iteration 6000 Training Loss: 2.6980045866966247 Validation Loss: 3.1449712685176303\n",
      "Iteration 6100 Training Loss: 2.71759624004364 Validation Loss: 3.049095664705549\n",
      "Iteration 6200 Training Loss: 2.665858952999115 Validation Loss: 3.1360858508518765\n",
      "Iteration 6300 Training Loss: 2.6856061315536497 Validation Loss: 3.16729405948094\n",
      "Iteration 6400 Training Loss: 2.6472700595855714 Validation Loss: 3.037510769707816\n",
      "Iteration 6500 Training Loss: 2.633604681491852 Validation Loss: 3.097941143172128\n",
      "Iteration 6600 Training Loss: 2.660121114253998 Validation Loss: 3.044380239077977\n",
      "Iteration 6700 Training Loss: 2.604353506565094 Validation Loss: 3.0491340160369873\n",
      "Iteration 6800 Training Loss: 2.6337303638458254 Validation Loss: 3.0025648389543806\n",
      "Iteration 6900 Training Loss: 2.578846027851105 Validation Loss: 3.09047908442361\n",
      "Iteration 7000 Training Loss: 2.572007882595062 Validation Loss: 3.0262629815510342\n",
      "Iteration 7100 Training Loss: 2.564051802158356 Validation Loss: 2.9515251432146346\n",
      "Iteration 7200 Training Loss: 2.552859306335449 Validation Loss: 2.9560186522347585\n",
      "Iteration 7300 Training Loss: 2.538024625778198 Validation Loss: 2.9837203196116855\n",
      "Iteration 7400 Training Loss: 2.5375968384742738 Validation Loss: 2.9889271770204817\n",
      "Iteration 7500 Training Loss: 2.5106743431091307 Validation Loss: 2.9459454161780223\n",
      "Iteration 7600 Training Loss: 2.521353716850281 Validation Loss: 3.001386659485953\n",
      "Iteration 7700 Training Loss: 2.495581967830658 Validation Loss: 2.9712452377591814\n",
      "Iteration 7800 Training Loss: 2.494172296524048 Validation Loss: 3.029913749013628\n",
      "Iteration 7900 Training Loss: 2.4835184454917907 Validation Loss: 2.937572189739772\n",
      "Iteration 8000 Training Loss: 2.4740228271484375 Validation Loss: 2.9227250133241927\n",
      "Iteration 8100 Training Loss: 2.481847867965698 Validation Loss: 2.9112187794276645\n",
      "Iteration 8200 Training Loss: 2.422059190273285 Validation Loss: 2.88399761063712\n",
      "Iteration 8300 Training Loss: 2.422113356590271 Validation Loss: 2.8717792204448154\n",
      "Iteration 8400 Training Loss: 2.449466872215271 Validation Loss: 2.8872082233428955\n",
      "Iteration 8500 Training Loss: 2.4244887232780457 Validation Loss: 2.8712247950690135\n",
      "Iteration 8600 Training Loss: 2.3911253428459167 Validation Loss: 2.936829583985465\n",
      "Iteration 8700 Training Loss: 2.414113850593567 Validation Loss: 2.8784224476133073\n",
      "Iteration 8800 Training Loss: 2.380569643974304 Validation Loss: 2.8920092412403653\n",
      "Iteration 8900 Training Loss: 2.382081079483032 Validation Loss: 2.980829187801906\n",
      "Iteration 9000 Training Loss: 2.394640202522278 Validation Loss: 2.906544770513262\n",
      "Iteration 9100 Training Loss: 2.338816428184509 Validation Loss: 2.9018259389059886\n",
      "Iteration 9200 Training Loss: 2.3654766631126405 Validation Loss: 2.912705489567348\n",
      "Iteration 9300 Training Loss: 2.3493257427215575 Validation Loss: 2.893407038279942\n",
      "Iteration 9400 Training Loss: 2.3169220685958862 Validation Loss: 2.9282727582114085\n",
      "Iteration 9500 Training Loss: 2.341605966091156 Validation Loss: 2.8637624297823225\n",
      "Iteration 9600 Training Loss: 2.3129995179176333 Validation Loss: 2.869928853852408\n",
      "Iteration 9700 Training Loss: 2.3395398664474487 Validation Loss: 2.8718867983136858\n",
      "Iteration 9800 Training Loss: 2.3155718207359315 Validation Loss: 2.900943194116865\n",
      "Iteration 9900 Training Loss: 2.2917000901699067 Validation Loss: 2.8546949114118303\n",
      "Iteration 10000 Training Loss: 2.2837922835350035 Validation Loss: 2.8293561594826833\n",
      "Iteration 10100 Training Loss: 2.2718810749053957 Validation Loss: 2.818730507578169\n",
      "Iteration 10200 Training Loss: 2.281844630241394 Validation Loss: 2.842173627444676\n",
      "Iteration 10300 Training Loss: 2.28664085149765 Validation Loss: 2.82650887966156\n",
      "Iteration 10400 Training Loss: 2.251278907060623 Validation Loss: 2.7731006826673235\n",
      "Iteration 10500 Training Loss: 2.2621150863170625 Validation Loss: 2.909914289202009\n",
      "Iteration 10600 Training Loss: 2.2388738894462588 Validation Loss: 2.7949586425508772\n",
      "Iteration 10700 Training Loss: 2.2629155683517457 Validation Loss: 2.826474802834647\n",
      "Iteration 10800 Training Loss: 2.2555684840679167 Validation Loss: 2.8221404211861745\n",
      "Iteration 10900 Training Loss: 2.2127696561813353 Validation Loss: 2.7837774583271573\n",
      "Iteration 11000 Training Loss: 2.2227425622940062 Validation Loss: 2.811241013663156\n",
      "Iteration 11100 Training Loss: 2.2156298851966856 Validation Loss: 2.7498369557516917\n",
      "Iteration 11200 Training Loss: 2.1986110723018646 Validation Loss: 2.838080814906529\n",
      "Iteration 11300 Training Loss: 2.2017039370536806 Validation Loss: 2.786996296473912\n",
      "Iteration 11400 Training Loss: 2.2030954706668853 Validation Loss: 2.7882863794054304\n",
      "Iteration 11500 Training Loss: 2.1827068173885347 Validation Loss: 2.8023012365613664\n",
      "Iteration 11600 Training Loss: 2.213490228652954 Validation Loss: 2.823285153933934\n",
      "Iteration 11700 Training Loss: 2.1793724691867826 Validation Loss: 2.766999500138419\n",
      "Iteration 11800 Training Loss: 2.1764027762413023 Validation Loss: 2.81482355935233\n",
      "Iteration 11900 Training Loss: 2.1992162919044493 Validation Loss: 2.7765913690839494\n",
      "Iteration 12000 Training Loss: 2.189405189752579 Validation Loss: 2.7590228489467075\n",
      "Iteration 12100 Training Loss: 2.1677948486804963 Validation Loss: 2.771863647869655\n",
      "Iteration 12200 Training Loss: 2.1841654419898986 Validation Loss: 2.7752687420163835\n",
      "Iteration 12300 Training Loss: 2.1829694175720213 Validation Loss: 2.8148185525621687\n",
      "Iteration 12400 Training Loss: 2.155565320253372 Validation Loss: 2.8074855974742343\n",
      "Iteration 12500 Training Loss: 2.157023924589157 Validation Loss: 2.750975625855582\n",
      "Iteration 12600 Training Loss: 2.1695812010765074 Validation Loss: 2.810640845979963\n",
      "Iteration 12700 Training Loss: 2.1622812283039092 Validation Loss: 2.8266554049083163\n",
      "Iteration 12800 Training Loss: 2.154675624370575 Validation Loss: 2.763991492135184\n",
      "Iteration 12900 Training Loss: 2.1654138731956483 Validation Loss: 2.8212782314845493\n",
      "Iteration 13000 Training Loss: 2.1426120376586915 Validation Loss: 2.716765488897051\n",
      "Iteration 13100 Training Loss: 2.159205058813095 Validation Loss: 2.801616362162999\n",
      "Iteration 13200 Training Loss: 2.148966201543808 Validation Loss: 2.8008382490703037\n",
      "Iteration 13300 Training Loss: 2.1373586094379426 Validation Loss: 2.754721164703369\n",
      "Iteration 13400 Training Loss: 2.1546865105628967 Validation Loss: 2.7637130362646922\n",
      "Iteration 13500 Training Loss: 2.121661065816879 Validation Loss: 2.7494628940309798\n",
      "Iteration 13600 Training Loss: 2.15453009724617 Validation Loss: 2.8360963378633772\n",
      "Iteration 13700 Training Loss: 2.1169460248947143 Validation Loss: 2.803018195288522\n",
      "Iteration 13800 Training Loss: 2.131352492570877 Validation Loss: 2.722903677395412\n",
      "Iteration 13900 Training Loss: 2.1480834317207336 Validation Loss: 2.808127301079886\n",
      "Iteration 14000 Training Loss: 2.1621626675128938 Validation Loss: 2.7797856671469554\n",
      "Iteration 14100 Training Loss: 2.1188216686248778 Validation Loss: 2.8456857204437256\n",
      "Iteration 14200 Training Loss: 2.1460546958446503 Validation Loss: 2.834080934524536\n",
      "Iteration 14300 Training Loss: 2.139097579717636 Validation Loss: 2.7162855012076244\n",
      "Iteration 14400 Training Loss: 2.1389025151729584 Validation Loss: 2.752189210483006\n",
      "Iteration 14500 Training Loss: 2.1243451619148255 Validation Loss: 2.7977316549846103\n",
      "Iteration 14600 Training Loss: 2.1257844400405883 Validation Loss: 2.7670906952449252\n",
      "Iteration 14700 Training Loss: 2.115929481983185 Validation Loss: 2.7589615072522844\n",
      "Iteration 14800 Training Loss: 2.12940416097641 Validation Loss: 2.7539590767451694\n",
      "Iteration 14900 Training Loss: 2.133685085773468 Validation Loss: 2.807044131415231\n",
      "Iteration 15000 Training Loss: 2.139973418712616 Validation Loss: 2.7334887300218855\n"
     ]
    }
   ],
   "source": [
    "### DEFINE TRAINING PARAMETERS ###\n",
    "iterations = 15000\n",
    "max_len = 100\n",
    "evaluate_interval = 100\n",
    "embedding_dim = 384\n",
    "depth = 4\n",
    "num_heads = 4\n",
    "lr = 0.0005\n",
    "batch_size = 64\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "### DEFINE MODEL AND OPTIMIZER ###\n",
    "model = RoBERTa(max_seq_len=max_len, \n",
    "                embed_dim=embedding_dim, \n",
    "                depth=depth, \n",
    "                num_heads=num_heads, \n",
    "                attn_p=0.1, \n",
    "                mlp_p=0.1, \n",
    "                proj_p=0.1, \n",
    "                pos_p=0.1)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "### DEFINE LOSS FUNCTION ###\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "### Build DataLoader ###\n",
    "dataset = MaskedLMLoader(tokenized_text, max_seq_len=max_len)\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [int(0.95*len(dataset)),int(len(dataset) - int(0.95*len(dataset)))])\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "### Define Scheduler ###\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer, \n",
    "                                            num_warmup_steps=1500, \n",
    "                                            num_training_steps=iterations)\n",
    "\n",
    "\n",
    "### Some Training Helper Stuff ###\n",
    "train = True\n",
    "completed_steps = 0\n",
    "all_training_losses, all_validation_losses = [], []\n",
    "training_losses, validation_losses = [], []\n",
    "progress_bar = tqdm(range(iterations))\n",
    "\n",
    "while train:\n",
    "\n",
    "    for batch in trainloader:\n",
    "\n",
    "        ### Grab Batch Information ###\n",
    "        inputs, labels, mask = batch[\"input_ids\"], batch[\"labels\"], batch[\"attention_mask\"]\n",
    "        inputs, labels, mask = inputs.to(DEVICE), labels.to(DEVICE), mask.to(DEVICE)\n",
    "\n",
    "        ### Pass Data through Model ###\n",
    "        prediction = model(inputs, mask)\n",
    "\n",
    "        ### Compute Loss (automatically on non -100 values)\n",
    "        prediction = prediction.reshape(-1, prediction.shape[-1]) # (B*S, E)\n",
    "        labels = labels.reshape(-1) # (B*S)\n",
    "        loss = loss_fn(prediction, labels)\n",
    "        training_losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        completed_steps += 1\n",
    "        progress_bar.update(1)\n",
    "            \n",
    "        if completed_steps % evaluate_interval == 0:\n",
    "            with torch.no_grad():\n",
    "                for batch in testloader:\n",
    "                    inputs, labels, mask = batch[\"input_ids\"], batch[\"labels\"], batch[\"attention_mask\"]\n",
    "                    inputs, labels, mask = inputs.to(DEVICE), labels.to(DEVICE), mask.to(DEVICE)\n",
    "                    prediction = model(inputs, mask)\n",
    "                    prediction = prediction.reshape(-1, prediction.shape[-1]) # (B*S, E)\n",
    "                    labels = labels.reshape(-1) # (B*S)\n",
    "                    loss = loss_fn(prediction, labels)\n",
    "                    validation_losses.append(loss.item())\n",
    "\n",
    "            avg_training_loss = np.mean(training_losses)\n",
    "            avg_eval_loss = np.mean(validation_losses)\n",
    "            \n",
    "            print(f\"Iteration {completed_steps} Training Loss:\", avg_training_loss, \"Validation Loss:\", avg_eval_loss)\n",
    "\n",
    "            all_training_losses.append(avg_training_loss)\n",
    "            all_validation_losses.append(avg_eval_loss)\n",
    "\n",
    "            training_losses = []\n",
    "            validation_losses = []\n",
    "\n",
    "        if completed_steps >= iterations:\n",
    "            train = False\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111b5e-d596-461c-be35-eb525f5428b2",
   "metadata": {},
   "source": [
    "### Lets take a look at our Training Curve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b32b266-9ac4-4a46-8600-0fef5ad878b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_training_losses)\n",
    "plt.plot(all_validation_losses)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca4ebf9-17b6-42a9-b2e8-cb24f09a7834",
   "metadata": {},
   "source": [
    "There is clear signs of overfitting, but the model did learn something! To match the original RoBERTa implementation we would need significantly larger datasets, batch sizes, and computational resources. But this should give a pretty good intuition about what is going on and how you can also implement this! But there is something we can try with the model, why don't we play fill in the blank?\n",
    "\n",
    "\n",
    "### Trying the Model Out!\n",
    "\n",
    "We will pass in a sentence with a mask, and then plot the top 5 things the model predicts to put there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fac10bd2-c0f7-4ff3-a8a5-96dcca0cafa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_the_gap(masked_sentence, k=3):\n",
    "\n",
    "    ### Encode Text and put on GPU ###\n",
    "    encoded_text = tokenizer.encode(masked_sentence)\n",
    "    input_tokens = torch.tensor(encoded_text).to(DEVICE)\n",
    "\n",
    "    ### Get which token is the mask token, we want to look at the predictions for that only! ###\n",
    "    masked_token_index = (input_tokens == special_token_idx[\"<mask>\"]).nonzero().item()\n",
    "\n",
    "    ### Pass through model, we dont have any attention mask because its just one sentence ###\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tokens.unsqueeze(0), attention_mask=None)\n",
    "\n",
    "    ### Grab the index of the masked token and apply softmax ###\n",
    "    predicted_output = output[0, masked_token_index]\n",
    "\n",
    "    ### Grab Top K predicted Words ###\n",
    "    top_k_predicted = torch.topk(predicted_output, k=k).indices\n",
    "\n",
    "    ### Decode and print out the predicted words! ###\n",
    "    predicted_words = tokenizer.decode(top_k_predicted.tolist()).strip().split(\" \")\n",
    "\n",
    "    return predicted_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d06bc0b6-993c-42d1-bd7f-dfc3ab3e310a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['community', 'world', 'Charm']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The Wizarding <mask> is a wonderful place to visit\"\n",
    "fill_in_the_gap(masked_sentence=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a88a3be3-6056-4a69-ae55-30f658d73ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['death', 'fear', 'birth']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Lord Voldemort is a dark wizard capable of <mask> and destruction\"\n",
    "fill_in_the_gap(masked_sentence=sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e59dfcd-e484-44b3-bcfd-6449492ccbe6",
   "metadata": {},
   "source": [
    "### Thats It!\n",
    "These seem like reasonable generations! There is a lot of improvement that can be made to this model, mostly in scale, but this is the most minimal implementation of a Masked Language Model that I could make!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
