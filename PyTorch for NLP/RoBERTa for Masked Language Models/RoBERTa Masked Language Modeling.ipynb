{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76d2a04e-9ef7-4bfa-a3d4-ae315e33f8fc",
   "metadata": {},
   "source": [
    "![banner](https://raw.githubusercontent.com/priyammaz/HAL-DL-From-Scratch/main/src/visuals/banner.png)\n",
    "\n",
    "# RoBERTa for Masked Language Modeling\n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/priyammaz/PyTorch-Adventures/main/src/visuals/masked_language_modeling_vis.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Today we will be implementing an updated variant of the original [BERT](https://arxiv.org/abs/1810.04805) model known as [RoBERTa](https://arxiv.org/abs/1907.11692), or Robustly Optimized BERT Approach. Before we keep going though, lets quickly review BERT!\n",
    "\n",
    "### Bidirectional Encoder Representations from Transformers (BERT)\n",
    "\n",
    "BERT was a model proposed in 2018, whoes whole purpose; was to learn contextual understanding from unlabeled text. There are two main differences between this model and GPT:\n",
    "- BERT is bidirectional, the model looks both forwards and backwards, where GPT is causal, so it only looks backwards\n",
    "- BERT uses Masked Language Modeling + Next Sentence Prediction with an Encoder Transformer instead of Causal Language Modeling with a Decoder Transformer.\n",
    "\n",
    "### Encoders (BERT, RoBERTa, ViT)\n",
    "Encoder transformers have the ability to look at the entire sequence and are bidirectional. This means when we compute attention, words can look forward and backwards, which is perfect for situations where we have the entire sequence given and we want to make some predictions about it (i.e. image classification, sentiment analysis, speech recognition, etc...)\n",
    "\n",
    "### Decoders (GPT)\n",
    "Decoder transformers only have the ability to look at the past, meaning any prediction we make for a specific time in the sequence is based only on times that came before it. This is important for tasks like time-series forecasting or language generation. When we want to generate a sequence, we can only look at the words that have come already as the future words aren't available. \n",
    "\n",
    "### Language Pre-Training\n",
    "There are typically two types of language pretraining. Encoders use **Masked Language Modeling**. This is the process of taking a sentence, randomly removing words, and then having the model predict the missing words by looking at existing words before and after it. Decoders use **Causal Language Modeling**, where we will train a model to predict the word at time $n+1$ give all the words 1 to $n$. Today we will be looking at Causal Language Models as that is what GPT uses for pretraining. \n",
    "\n",
    "\n",
    "This is what BERT looks like!\n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/priyammaz/PyTorch-Adventures/main/src/visuals/BERT_archtiecture.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "[Image Source](https://etc.cuit.columbia.edu/news/basics-language-modeling-transformers-bert)\n",
    "\n",
    "As you can see, BERT passes in two sentences of data, along with a learnable CLS token. The CLS token perform the Next Sentence Prediction task, basically does the second sentence provided actually follow the first? The rest of the model does Masked Language Modeling, to fill in the blanks in the masked sentence. \n",
    "\n",
    "### How Does RoBERTa Improve on BERT?\n",
    "\n",
    "- Train on a much larger dataset (didn't exist when BERT was made)\n",
    "- Remove the Next Sentence Prediction Task, which leads to slightly better performance\n",
    "- Dynamic Masking, in the dataloader, randomly mask in real time, rather than mask ahead of time and train on the same masks repeatedly. \n",
    "\n",
    "**Note**\n",
    "\n",
    "RoBERTA still includes the CLS token for downstream fine-tuning in the future, but because there is no loss attached to it, there is never any gradient updated during language pre-training.\n",
    "\n",
    "### Masked Language Model Attention\n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/priyammaz/PyTorch-Adventures/main/src/visuals/mlm_attention.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "Masked language models will be a full encoder based transformer, that means all the values in the attention mechanism above will be computed. We specifically care most about the spots highlighted, how is the Masked token related to all the other words, and also how are all the other words related to the masked token (therefore bidirectional).\n",
    "\n",
    "### How to Compute Attention\n",
    "\n",
    "So how do we actually perform attention masking? Lets first remind ourselves what attention is doing. Here is the equation for Attention as a reminder:\n",
    "\n",
    "$$\\text{Attention}(Q,K,V) = \\text{Softmax}(\\frac{QK^T}{\\sqrt{d_e}})V$$\n",
    "\n",
    "So the first step is the computing the $QK^T$, where Q and K both have the shape (Sequence Length x Embedding Dimension). The output of this computation will be sequence length x sequence length. This is what it looks like!\n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/priyammaz/PyTorch-Adventures/main/src/visuals/computing_attention.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "In the image above, I also applied the softmax (not shown for simplicity), so each row of the attention matrix adds up to 1 (like probabilities).\n",
    "\n",
    "Now we can multiply our output of $QK^T$ with our $V$. This is what a regular encoder (bidirectional) attention will look like. Remember, $V_1, ... V_4$ are the projection vectors (Values) of the data, and the attention matrix is a weighted average of all of these vectors. \n",
    "\n",
    "**Note**\n",
    "\n",
    "In transformers, our input $X$ goes through 3 linear projections to create $Q, K, \\text{and } V$ Initially, the each vector for each word in $X$ is the embedding vector representing that word. After the attention computation, each vector for each word isn't just the embedding of that word but rather a weighted average of all the vectors in the sequence and how they are related to the word of interest. \n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/priyammaz/PyTorch-Adventures/main/src/visuals/encoder_attention_vis.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Sequence Padding and Attention Masking\n",
    "\n",
    "One typical consideration for models like this is, when we pass in sentences of data, different sentences have different lengths. To create a matrix of tokens, we need to make sure all the sequences are the same, therefore we have to pad the shorter sequences to the longer ones. This is what the padding looks like!\n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/priyammaz/PyTorch-Adventures/main/src/visuals/sequence_padding.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "Now the pad tokens are just fillers, they don't add any information at all to the data. To deal with this, we need to make sure that when we compute attention, these pad tokens are ignored. This is very similar to the Causal Mask in [GPT](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20for%20NLP/GPT), and we will be doing exactly what we did there, fill in the masked locations with $-\\infty$ before softmax is computed. Lets take a quick look at this for one of the sentences above. \n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/priyammaz/PyTorch-Adventures/main/src/visuals/padding_attention_mask.png?raw=true\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "You can see that we zero out the columns of our attention mask that has padding tokens, but not the row. The reason for this is, the model should learn the relation of how the padding tokens are related to the words (this is most likely just positional information that padding is at the end), but it should not learn how other words are related to padding, as the padding adds no semantic meaning. This is why, when multiplying by our values, we have zeroed out the attention weights of the 4th column, as when computing weighted averages of how one word is related to all others, the padding token is not included. \n",
    "\n",
    "\n",
    "\n",
    "## Computing the Reweighted Padded Attention Mask\n",
    "\n",
    "Lets create some numbers so we can get a better idea of how this works. Let the tokens be $X = [10, 2, \\text{<pad>}]$, so the third token is a padding token. Lets then also pretend, we pass this to our RoBERTa model, and when we go to compute our attention $QK^T$, the raw output before the softmax is below.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "  7       & -8   & 6  \\\\\n",
    "  -3       & 2   & 4   \\\\\n",
    "  1       & 6  & -2   \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Remember, the equation for softmax is:\n",
    "\n",
    "$$\\text{Softmax}(\\vec{x}) = \\frac{e^{x_i}}{\\sum_{j=1}^N{e^{x_j}}}$$\n",
    "\n",
    "Then, if we ignore padding and everything right now, we can compute softmax for row of the matrix above:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Softmax}\n",
    "\\begin{bmatrix}\n",
    "  7       & -8   & 6  \\\\\n",
    "  -3       & 2   & 4   \\\\\n",
    "  1       & 6  & -2   \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "  \\frac{e^{7}}{e^{7}+e^{-8}+e^{6}}       & \\frac{e^{-8}}{e^{7}+e^{-8}+e^{6}}   & \\frac{e^{6}}{e^{7}+e^{-8}+e^{6}}  \\\\\n",
    "  \\frac{e^{-3}}{e^{-3}+e^{2}+e^{4}}       & \\frac{e^{2}}{e^{-3}+e^{2}+e^{4}}   & \\frac{e^{4}}{e^{-3}+e^{2}+e^{4}}  \\\\\n",
    "  \\frac{e^{1}}{e^{1}+e^{6}+e^{-2}}       & \\frac{e^{6}}{e^{1}+e^{6}+e^{-2}}   & \\frac{e^{-2}}{e^{1}+e^{6}+e^{-2}}  \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "  0.73       & 0.0000002   & 0.27   \\\\\n",
    "  0.0008       & 0.12   & 0.88 \\\\\n",
    "  0.007       & 0.99  & 0.003  \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "But what we need is to mask out all the tokens in this matrix related to padding. Just like we did in [GPT](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20for%20NLP/GPT), we will fill in the indexes of the that we want to mask with $-\\infty$. If only the last token was a padding token in our sequence, then the attention before the softmax should be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "  7       & -8   & -\\infty  \\\\\n",
    "  -3       & 2   & -\\infty   \\\\\n",
    "  1       & 6  & -\\infty  \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Taking the softmax of the rows of this matrix then gives:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Softmax}\n",
    "\\begin{bmatrix}\n",
    " 7       & -8   & -\\infty  \\\\\n",
    "  -3       & 2   & -\\infty   \\\\\n",
    "  1       & 6  & -\\infty  \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "  1       & 0   & 0  \\\\\n",
    "  0.0067  & 0.9933 & 0   \\\\\n",
    "  0.0067       & 0.9933  & 0  \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Which is exactly what we want! Therefore, when computing attention, we need to know which tokens are padding tokens in our data, and then go ahead and perform this computation on our attention output before multiplying with values.\n",
    "\n",
    "\n",
    "### Masked Language Modeling Logic\n",
    "\n",
    "Last piece! How do we do the masked language modeling? We will follow exactly what RoBERTa and BERT had done:\n",
    "- We will randomly mask out 15 percent of the words in our sentence. Of the 15 percent selected to be masked we will:\n",
    "    - 80% of the time we will replace it with a mask token\n",
    "    - 10% of the time it will be replaced with some random token (not mask token)\n",
    "    - 10% of the time it will be kept the same\n",
    " \n",
    "The reason for this logic is, in the real world, there are no mask tokens, just regular sentences. This way the model will have some experience understanding that a word is misplaced (a random word inserted) or that a word is correct. Regardless, the masked words will be predicted, where most of the times the input is the mask token, but sometimes it will just be exactly the word it was to begin with or some random word. \n",
    "\n",
    "\n",
    "### Tokenizer\n",
    "\n",
    "In our case, we will use the RoBERTa tokenizer found in Huggingface! RoBERTA tokenizer is exactly the same as the GPT2 Tokenizer, except it includes some of these special tokens like the Mask and Pad tokens that we need!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d8cb018-909f-4e80-8b42-b14c63966e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9225c9ab-123d-4c6f-bdd5-ac73310c08e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Tokens in Tokenizer\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n",
      "Special Token Index\n",
      "{'<s>': 0, '</s>': 2, '<unk>': 3, '<pad>': 1, '<mask>': 50264}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "print(\"Special Tokens in Tokenizer\")\n",
    "special_tokens = tokenizer.special_tokens_map\n",
    "print(special_tokens)\n",
    "\n",
    "special_token_idx = {}\n",
    "for token in special_tokens.values():\n",
    "    special_token_idx[token] = tokenizer.encode(token, add_special_tokens=False)[0]\n",
    "\n",
    "print(\"Special Token Index\")\n",
    "print(special_token_idx)\n",
    "\n",
    "\n",
    "### Get Indexes of All Types of Tokens ###\n",
    "all_tokens_idx = list(range(tokenizer.vocab_size))\n",
    "all_special_tokens_idx = sorted(list(special_token_idx.values()))\n",
    "all_non_special_tokens_idx = [tok for tok in all_tokens_idx if tok not in all_special_tokens_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73397168-70a9-4d5e-b5d4-afa87533ac6e",
   "metadata": {},
   "source": [
    "### Prepare Dataset\n",
    "\n",
    "Lets perpare our Harry Potter dataset again! In this case, we will split our data into sentences, and then grab 5 sentences per chunk for our model. Theres tons of ways to do this, but I will just split on the periods and then put them back together in chunks. This is definitely not perfect as we will split on periods that are not end of sentences (Mr., Mrs., ...) but this should be ok for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f1f03b2-bc6c-4ea5-b540-2d099e15728d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (601 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "path_to_data = \"../../data/harry_potter_txt/\"\n",
    "\n",
    "text_files = os.listdir(path_to_data)\n",
    "\n",
    "all_text = \"\"\n",
    "for book in text_files:\n",
    "    with open(os.path.join(path_to_data, book), \"r\") as f:\n",
    "        text = f.readlines() # Read in all lines\n",
    "        text = [line for line in text if \"Page\" not in line] # Remove lines with Page Numbers\n",
    "        text = \" \".join(text).replace(\"\\n\", \"\") # Remove all newline characters\n",
    "        text = [word for word in text.split(\" \") if len(word) > 0] # Remove all empty characters\n",
    "        text = \" \".join(text) # Combined lightly cleaned text\n",
    "        all_text += text\n",
    "\n",
    "### Split Data by \"sentences\" ###\n",
    "all_text = all_text.split(\".\")\n",
    "\n",
    "### Grab 5 Sentences at a time and put them together by the period ###\n",
    "all_text_chunked = [\".\".join(all_text[i:i+5]) for i in range(0, len(all_text), 5)]\n",
    "\n",
    "### Tokenize all the text! ###\n",
    "tokenized_text = [tokenizer.encode(text) for text in all_text_chunked]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af30908-89e4-42aa-86d0-9398a17d1331",
   "metadata": {},
   "source": [
    "### Write a Dataset Class\n",
    "\n",
    "Now that we have our tokenized data ready, we need to write a dataloader. This dataloader should perform a few things:\n",
    "\n",
    "- Padding to the longest sequence length\n",
    "- Masking with the ratios shown above (ensuring that we dont mask any special tokens or padding tokens)\n",
    "\n",
    "#### What are we predicting?\n",
    "\n",
    "Quick example: \n",
    "\n",
    "Lets pretend our input to the model is: [10, 5, 2, 6, 1, 4, 13, 1, 4, <pad>, <pad>]. \n",
    "\n",
    "After masking it can be something like [10, \"[mask]\", 2, 6, 1, \"[mask]\", 13, 1, 4, \"[pad]\", \"[pad]\"].\n",
    "\n",
    "Then our final prediction vector is [-100, 5, -100, -100, -100, 4, -100, -100, -100, -100, -100]\n",
    "\n",
    "Where did all the -100 come from? This is a classification problem in the end so we will be using CrossEntropyLoss. In the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), we see that there is a argument called *ignore_index=-100*. When we compute loss, we only want to do this for the masked tokens, and ignore the unmasked ones. This is why in our label, we will set the masked token to the index its supposed to predict, and everything else as -100!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263654e6-76d7-47d9-bf07-f8cb6648c4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,    73,  1941,  9963,   975, 13387,   226,  6372,  1691,   427,\n",
      "            4,     8,  3801,     4,   211, 50264,   607,     6,     9,   346,\n",
      "          237, 50264, 50264, 18068,  3936,     6,    58,  2602,     7,   224,\n",
      "           14,    51,    58,  6683,  2340,     6,  3392,    47, 50264,   203,\n",
      "            4,   252,    58,     5, 50264,    82,    47,    17,    27,   417,\n",
      "         1057,     7, 50264, 50264,    11,   932,  7782,    50, 12754,     6,\n",
      "          142,    51,    95, 50264,    17, 50264,    90,   946,    19,   215,\n",
      "        33736,     4,   427,     2])\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  4668,  -100,  -100,  -100,  -100,\n",
      "         -100,     6,  9522,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   182,  -100,\n",
      "         -100,  -100,  -100,  -100,    94,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,    28,   963,    11,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,   399,  -100,    27,  -100,  -100,  -100,  -100,\n",
      "        20175,  -100,  -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "class MaskedLMLoader(Dataset):\n",
    "    def __init__(self, tokenized_data, max_seq_len=100, masking_ratio=0.15):\n",
    "        self.data = tokenized_data\n",
    "        self.mask_ratio = masking_ratio\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _random_mask_text(self, tokens):\n",
    "\n",
    "        ### Create Random Uniform Sample Tensor ###\n",
    "        random_masking = torch.rand(*tokens.shape)\n",
    "\n",
    "        ### Set Value of Special Tokens to 1 so we DONT MASK THEM ###\n",
    "        special_tokens = torch.tensor(tokenizer.get_special_tokens_mask(tokens, already_has_special_tokens=True))\n",
    "        random_masking[special_tokens==1] = 1\n",
    "\n",
    "        ### Get Boolean of Words under Masking Threshold ###\n",
    "        random_masking = (random_masking < self.mask_ratio)\n",
    "\n",
    "        ### Create Labels ###\n",
    "        labels = torch.full((tokens.shape), -100)\n",
    "        labels[random_masking] = tokens[random_masking]\n",
    "\n",
    "        ### Get Indexes of True ###\n",
    "        random_selected_idx = random_masking.nonzero()\n",
    "\n",
    "        ### 80% Of the Time Replace with Mask Token ###\n",
    "        masking_flag = torch.rand(*random_selected_idx.shape)\n",
    "        masking_flag = (masking_flag<0.8)\n",
    "        selected_idx_for_masking = random_selected_idx[masking_flag]\n",
    "\n",
    "        ### Seperate out remaining indexes to be assigned ###\n",
    "        unselected_idx_for_masking = random_selected_idx[~masking_flag]\n",
    "\n",
    "        ### 10% of the time (or 50 percent of the remaining 20%) we fill with random token ###\n",
    "        ### The remaining times, leave the text as is ###\n",
    "        masking_flag = torch.rand(*unselected_idx_for_masking.shape)\n",
    "        masking_flag = (masking_flag<0.5)\n",
    "        selected_idx_for_random_filling = unselected_idx_for_masking[masking_flag]\n",
    "        selected_idx_to_be_left_alone = unselected_idx_for_masking[~masking_flag]\n",
    "        \n",
    "        ### Fill Mask Tokens ###\n",
    "        if len(selected_idx_for_masking) > 0:\n",
    "            tokens[selected_idx_for_masking] = special_token_idx[\"<mask>\"]\n",
    "        \n",
    "        ### Fill Random Tokens ###\n",
    "        if len(selected_idx_for_random_filling) > 0:\n",
    "            randomly_selected_tokens = torch.tensor(random.sample(all_non_special_tokens_idx, len(selected_idx_for_random_filling)))\n",
    "            tokens[selected_idx_for_random_filling] = randomly_selected_tokens\n",
    "        \n",
    "        \n",
    "        return tokens, labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.tensor(self.data[idx])\n",
    "\n",
    "        ### Make sure sequence is within context length ###\n",
    "        if len(data) > self.max_seq_len:\n",
    "            rand_start_idx = random.choice(list(range(len(data) - self.max_seq_len)))\n",
    "            end_idx = rand_start_idx + self.max_seq_len\n",
    "            data = data[rand_start_idx:end_idx]\n",
    "  \n",
    "        ### Uniform Random Masking ###\n",
    "        masked_tokens, label = self._random_mask_text(data)\n",
    "\n",
    "        return masked_tokens, label\n",
    "\n",
    "mlm = MaskedLMLoader(tokenized_text)\n",
    "\n",
    "for masked_tokens, labels in mlm:\n",
    "    print(masked_tokens)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63106ec4-61ab-472e-81a8-e1cb4b15680c",
   "metadata": {},
   "source": [
    "### Write a DataLoader Class\n",
    "\n",
    "Now that we have our masking and labels generation ready to go for our dataset, we now need to write a DataLoader! In this dataloader, needs some additional logic though:\n",
    "\n",
    "- Data collation to pad to longest sequence so we can stack our tensors together\n",
    "- Pad the labels with more -100 as well, because we dont predict on padding tokens \n",
    "- Padding mask which will be used as our attention mask later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73f393ce-d985-4756-b491-b73265b82462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,    73,  1941,  ...,     1,     1,     1],\n",
      "        [    0,   211,  4668,  ...,    49,  2979,    89],\n",
      "        [    0,    20, 50264,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,  1993,     9,  ...,     1,     1,     1],\n",
      "        [   91,    21,    11,  ...,    27,    90,   192],\n",
      "        [    0,    85,    21,  ...,   357,     9,    24]])\n",
      "tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100,  211,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]])\n",
      "tensor([[False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        ...,\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "\n",
    "    ### Grab Tokens and their Labels ###\n",
    "    token_samples = []\n",
    "    label_samples =[]\n",
    "\n",
    "    for token, label in batch:\n",
    "        token_samples.append(token)\n",
    "        label_samples.append(label)\n",
    "\n",
    "    ### Get Seq Len of Each Token along with Max Length ###\n",
    "    sequence_lengths = [len(tok) for tok in token_samples]\n",
    "    max_seq_len = max(sequence_lengths)\n",
    "\n",
    "    ### Append Padding Tokens to Token Samples, store padding masks and append more -100 for the labels ###\n",
    "    padding_masks = []\n",
    "    for idx in range(len(token_samples)):\n",
    "        sample = token_samples[idx]\n",
    "        seq_len = len(sample)\n",
    "        diff = max_seq_len - seq_len\n",
    "\n",
    "        if diff > 0:\n",
    "\n",
    "            ### Append Padding to Each Sequence ###\n",
    "            padding = torch.tensor([special_token_idx[\"<pad>\"] for _ in range(diff)])\n",
    "            sample = torch.concatenate((sample, padding))\n",
    "            token_samples[idx] = sample\n",
    "            \n",
    "            ### Append -100 to Labels (we dont predict padding masks) ###\n",
    "            label_padding = torch.tensor([-100 for _ in range(diff)])\n",
    "            label_samples[idx] = torch.concatenate((label_samples[idx], label_padding))\n",
    "\n",
    "            ### Store Padding Mask ###\n",
    "            padding_mask = (sample==special_token_idx[\"<pad>\"])\n",
    "            padding_masks.append(padding_mask)\n",
    "\n",
    "        else:\n",
    "\n",
    "            ### Padding Mask (with no padding) for longest sequence ###\n",
    "            padding_masks.append(torch.zeros(max_seq_len))\n",
    "\n",
    "    token_samples = torch.stack(token_samples)\n",
    "    label_samples = torch.stack(label_samples)\n",
    "    padding_masks = torch.stack(padding_masks)\n",
    "\n",
    "    assert token_samples.shape == label_samples.shape == padding_masks.shape\n",
    "    \n",
    "    batch = {\"input_ids\": token_samples, \n",
    "             \"labels\": label_samples, \n",
    "             \"attention_mask\": padding_masks.bool()}\n",
    "\n",
    "    return batch\n",
    "    \n",
    "### Test DataLoader ###\n",
    "dataloader = DataLoader(mlm, batch_size=16, collate_fn=collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch[\"input_ids\"])\n",
    "    print(batch[\"labels\"])\n",
    "    print(batch[\"attention_mask\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b1d0d5-f2bf-4500-a7b2-c18a4ebf3e02",
   "metadata": {},
   "source": [
    "### Attention Mechanism \n",
    "\n",
    "Now that we have completed all of our data prep, its time to move onto the attention mechanism implementation! This should be almost identical to the [Vision Transformer](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20for%20Computer%20Vision/Vision%20Transformer) implementation, with the additional logic for masking added in, which will be similar to our [GPT](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20for%20NLP/GPT) implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1573e7c4-e432-4bfd-8bc4-2f5f94d7235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "class SelfAttentionEncoder(nn.Module):\n",
    "  def __init__(self,\n",
    "               embed_dim=768,\n",
    "               num_heads=12, \n",
    "               attn_p=0,\n",
    "               proj_p=0):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "\n",
    "        embed_dim: What is the embedding dimension of each embedding vector\n",
    "        num_heads: How many heads of attention do we want?\n",
    "        attn_p: Dropout probability on Attention\n",
    "        proj_p: Dropout probability on projection matrix\n",
    "    \"\"\"\n",
    "    super(SelfAttentionEncoder, self).__init__()\n",
    "    assert embed_dim % num_heads == 0\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = int(embed_dim / num_heads)\n",
    "    self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    self.qkv = nn.Linear(embed_dim, embed_dim*3)\n",
    "    self.attn_p = attn_p\n",
    "    self.attn_drop = nn.Dropout(attn_p)\n",
    "    self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "    self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "  def forward(self, x, attention_mask=None):\n",
    "    batch_size, seq_len, embed_dim = x.shape\n",
    "    qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "    qkv = qkv.permute(2,0,3,1,4)\n",
    "    q,k,v = qkv.unbind(0)\n",
    "\n",
    "    # B x H x S x S\n",
    "    attn = (q @ k.transpose(-2,-1)) * self.scale\n",
    "\n",
    "    ####################################################################################\n",
    "    ### FILL ATTENTION MASK WITH -Infinity ###\n",
    "\n",
    "    if attention_mask is not None:\n",
    "        ### We Need to Unsqueeze Attention Mask to have placeholder dimensions for Num Heads and Seq Len ###\n",
    "\n",
    "        # B, S -> B, 1, 1, S\n",
    "        attention_mask = attention_mask.unsqueeze(1).unsqueeze(1)\n",
    "        attn = attn.masked_fill(attention_mask, float('-inf'))\n",
    "\n",
    "    ####################################################################################\n",
    "\n",
    "    attn = attn.softmax(dim=-1)\n",
    "    attn = self.attn_drop(attn)\n",
    "    x = attn @ v\n",
    "\n",
    "    x = x.transpose(1,2).reshape(batch_size, seq_len, embed_dim)\n",
    "    x = self.proj(x)\n",
    "    x = self.proj_drop(x)\n",
    "      \n",
    "    return x\n",
    "\n",
    "### Lets test Attention Mechanism ###\n",
    "rand_x = torch.randn(2,5,16)\n",
    "padding = torch.tensor([[False, False, False, True, True], \n",
    "                        [False, False, False, False, False]])\n",
    "\n",
    "a = SelfAttentionEncoder(embed_dim=16, num_heads=4)\n",
    "out = a(rand_x, padding)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c294ccbb-0a30-4d1c-a040-0fa61aa88de8",
   "metadata": {},
   "source": [
    "### Define the Rest of The Transformer\n",
    "\n",
    "All we have left is to implement the rest of the model! Again, this should be very similar to the [Vision Transformer](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20for%20Computer%20Vision/Vision%20Transformer) and [GPT](https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20for%20NLP/GPT)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bf554dc-97ce-4935-9910-c55dfb64fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "### DEFINE THE MULTILAYER PERCEPTRON ###\n",
    "########################################\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_features,\n",
    "                 hidden_features,\n",
    "                 out_features,\n",
    "                 act_layer=nn.GELU,\n",
    "                 mlp_p=0):\n",
    "\n",
    "\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.drop1 = nn.Dropout(mlp_p)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop2 = nn.Dropout(mlp_p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "\n",
    "################################################################\n",
    "### PUT ATTENTION AND MLP TOGETHER WITH RESIDUAL CONNECTIONS ###\n",
    "################################################################\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embed_dim=768, \n",
    "                 num_heads=12, \n",
    "                 mlp_ratio=4, \n",
    "                 proj_p=0., \n",
    "                 attn_p=0., \n",
    "                 mlp_p=0., \n",
    "                 act_layer=nn.GELU, \n",
    "                 norm_layer=nn.LayerNorm):\n",
    "\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(embed_dim, eps=1e-6)\n",
    "        self.attn = SelfAttentionEncoder(embed_dim=embed_dim,\n",
    "                                         num_heads=num_heads, \n",
    "                                         attn_p=attn_p,\n",
    "                                         proj_p=proj_p)\n",
    "\n",
    "\n",
    "        self.norm2 = norm_layer(embed_dim, eps=1e-6)\n",
    "        self.mlp = MLP(in_features=embed_dim,\n",
    "                       hidden_features=int(embed_dim*mlp_ratio),\n",
    "                       out_features=embed_dim,\n",
    "                       act_layer=act_layer,\n",
    "                       mlp_p=mlp_p)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        x = x + self.attn(self.norm1(x), attention_mask)\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac4746a-3225-48a8-9b6f-6f476f8177d8",
   "metadata": {},
   "source": [
    "### Write RoBERTa\n",
    "\n",
    "The final step! Lets put it all together to have our RoBERTa class! This isn't exactly RoBERTa, it is smaller, and I think some of the activations are different, but most of it should be pretty close! Here are the things we will be doing here:\n",
    "\n",
    "- Word Embeddings: We need to create an embedding matrix for our token embeddings\n",
    "- Positional Embeddings: We need a second embedding matrix for the positional encodings. We could have used Sin-Cosine embeddings like the original [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper, but we will used learned embeddings in this case. Theres lots of options though like [Rotary Embeddings](https://arxiv.org/abs/2104.09864) or [ALiBi](https://arxiv.org/abs/2108.12409) that are definitely better, but this is a start! **Also note, if our sequence length is 512, then our total number of positions is 513 due to the extra CLS token that also needs positional information. \n",
    "- CLS Token, although not used in our model for pretraining right now, we will leave it as it would be necessary for downstream finetuning. \n",
    "- Blocks: Stack of our transformer blocks\n",
    "- Prediction Head: For each embedding vector, we need to predict the next work, so this will predict our vocab size of the tokenizer.\n",
    "\n",
    "#### Weight Sharing\n",
    "Intuitively, the weight matrix that takes in indexes for tokens and converts to the embedding vectors and the prediction head that takes embedding vectors and predicts the token indexes are doing the same thing. A trick we can use is weight sharing, just make the weights of the word embeddings equal to the weights of the prediction heads, as these vectors have to serve the same purpose!\n",
    "\n",
    "#### Weight Initialization Strategy\n",
    "There is a lot of work in how different weight initialization effects transformers. We will just go with the typical one that I have seen of truncated normal initialization on all weight layers and biases set to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67d133c8-9e9d-4d10-a529-9da4b0c93954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 50265])\n"
     ]
    }
   ],
   "source": [
    "class RoBERTa(nn.Module):\n",
    "    def __init__(self, \n",
    "                 max_seq_len=512,\n",
    "                 vocab_size=tokenizer.vocab_size,\n",
    "                 embed_dim=768, \n",
    "                 depth=12, \n",
    "                 num_heads=12, \n",
    "                 mlp_ratio=4, \n",
    "                 attn_p=0., \n",
    "                 mlp_p=0., \n",
    "                 proj_p=0., \n",
    "                 pos_p=0., \n",
    "                 act_layer=nn.GELU, \n",
    "                 norm_layer=nn.LayerNorm):\n",
    "\n",
    "        super(RoBERTa, self).__init__()\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
    "        self.pos_embed = nn.Embedding(max_seq_len+1, embed_dim)\n",
    "        self.pos_drop = nn.Dropout(pos_p)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(embed_dim=embed_dim, \n",
    "                      num_heads=num_heads, \n",
    "                      mlp_ratio=mlp_ratio, \n",
    "                      proj_p=proj_p, \n",
    "                      attn_p=attn_p, \n",
    "                      mlp_p=mlp_p, \n",
    "                      act_layer=act_layer, \n",
    "                      norm_layer=norm_layer)\n",
    "\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        device = x.device\n",
    "\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        ### If we have too long of a sequence length, grab the last chunk ###\n",
    "        if seq_len > self.max_seq_len:\n",
    "            x = x[:, -self.max_seq_len:]\n",
    "\n",
    "        ### We only need the positional information upto the length of data we have plus CLS token ###\n",
    "        avail_idx = torch.arange(0, seq_len+1, dtype=torch.long, device=device)\n",
    "\n",
    "        ### Embed all the Tokens ###\n",
    "        tok_emb = self.embeddings(x)\n",
    "\n",
    "        ### Concatenate on the CLSL Token ###\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1) # (batch, 1, embed_dim)\n",
    "        tok_emb = torch.cat((cls_token, tok_emb), dim=1) \n",
    "\n",
    "        ### Add positional information ###\n",
    "        pos_emb = self.pos_embed(avail_idx)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        ### Slice Off CLS token ###\n",
    "        cls_token_final = x[:, 0] \n",
    "\n",
    "        ### Slice off Remaining Tokens ###\n",
    "        x = x[:, 1:]\n",
    "\n",
    "        ### MLM Prediction Head ###\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "        \n",
    "\n",
    "### Test Model ###\n",
    "rand_x = torch.randint(0,10, (2,5))\n",
    "padding = torch.tensor([[False, False, False, True, True], \n",
    "                        [False, False, False, False, False]])\n",
    "\n",
    "m = RoBERTa()\n",
    "out = m(rand_x, padding)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "438399f4-5d5b-4d9b-b90c-f18bb36e247d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31dc19d8ffb9419491284e088fc6c894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 Training Loss: 10.34432785987854 Validation Loss: 8.520377499716622\n",
      "Iteration 200 Training Loss: 7.274836959838868 Validation Loss: 6.715031385421753\n",
      "Iteration 300 Training Loss: 6.590062284469605 Validation Loss: 6.529433625085013\n",
      "Iteration 400 Training Loss: 6.435049891471863 Validation Loss: 6.390048095158169\n",
      "Iteration 500 Training Loss: 6.358944272994995 Validation Loss: 6.326058047158377\n",
      "Iteration 600 Training Loss: 6.1937928390502925 Validation Loss: 6.066775321960449\n",
      "Iteration 700 Training Loss: 5.879653730392456 Validation Loss: 5.68797162600926\n",
      "Iteration 800 Training Loss: 5.528706216812134 Validation Loss: 5.381109169551304\n",
      "Iteration 900 Training Loss: 5.21749849319458 Validation Loss: 5.1651431151798795\n",
      "Iteration 1000 Training Loss: 5.03770667552948 Validation Loss: 4.941173144749233\n",
      "Iteration 1100 Training Loss: 4.803663349151611 Validation Loss: 4.854219198226929\n",
      "Iteration 1200 Training Loss: 4.693519849777221 Validation Loss: 4.736985240663801\n",
      "Iteration 1300 Training Loss: 4.567406530380249 Validation Loss: 4.625240666525705\n",
      "Iteration 1400 Training Loss: 4.4078887176513675 Validation Loss: 4.466972827911377\n",
      "Iteration 1500 Training Loss: 4.310019483566284 Validation Loss: 4.324614524841309\n",
      "Iteration 1600 Training Loss: 4.215521287918091 Validation Loss: 4.261404480252947\n",
      "Iteration 1700 Training Loss: 4.133683545589447 Validation Loss: 4.169618793896267\n",
      "Iteration 1800 Training Loss: 4.039560732841491 Validation Loss: 4.052694797515869\n",
      "Iteration 1900 Training Loss: 3.9723936223983767 Validation Loss: 4.023970229285104\n",
      "Iteration 2000 Training Loss: 3.8973238563537596 Validation Loss: 3.971344096320016\n",
      "Iteration 2100 Training Loss: 3.8348073077201845 Validation Loss: 3.967359713145665\n",
      "Iteration 2200 Training Loss: 3.765213279724121 Validation Loss: 3.8494777509144376\n",
      "Iteration 2300 Training Loss: 3.7037471175193786 Validation Loss: 3.8329857758113315\n",
      "Iteration 2400 Training Loss: 3.6336718893051145 Validation Loss: 3.8156694003513882\n",
      "Iteration 2500 Training Loss: 3.5897860383987426 Validation Loss: 3.7894405126571655\n",
      "Iteration 2600 Training Loss: 3.574028491973877 Validation Loss: 3.7508210795266286\n",
      "Iteration 2700 Training Loss: 3.500345332622528 Validation Loss: 3.7380881309509277\n",
      "Iteration 2800 Training Loss: 3.4911122035980227 Validation Loss: 3.665018456322806\n",
      "Iteration 2900 Training Loss: 3.4705175256729124 Validation Loss: 3.632681097303118\n",
      "Iteration 3000 Training Loss: 3.3791495394706725 Validation Loss: 3.646083882876805\n",
      "Iteration 3100 Training Loss: 3.3504626750946045 Validation Loss: 3.581377148628235\n",
      "Iteration 3200 Training Loss: 3.3410040807723997 Validation Loss: 3.5173605510166714\n",
      "Iteration 3300 Training Loss: 3.270874674320221 Validation Loss: 3.432144045829773\n",
      "Iteration 3400 Training Loss: 3.25847403049469 Validation Loss: 3.4858591045652116\n",
      "Iteration 3500 Training Loss: 3.2275252389907836 Validation Loss: 3.445493595940726\n",
      "Iteration 3600 Training Loss: 3.2094538760185243 Validation Loss: 3.384581889425005\n",
      "Iteration 3700 Training Loss: 3.19661523103714 Validation Loss: 3.4814562797546387\n",
      "Iteration 3800 Training Loss: 3.0983346104621887 Validation Loss: 3.3901433093207225\n",
      "Iteration 3900 Training Loss: 3.1496882581710817 Validation Loss: 3.3893531220299855\n",
      "Iteration 4000 Training Loss: 3.1229434037208557 Validation Loss: 3.3808592557907104\n",
      "Iteration 4100 Training Loss: 3.062785813808441 Validation Loss: 3.340993744986398\n",
      "Iteration 4200 Training Loss: 3.0779599785804748 Validation Loss: 3.331388439450945\n",
      "Iteration 4300 Training Loss: 3.0447681760787964 Validation Loss: 3.315293346132551\n",
      "Iteration 4400 Training Loss: 3.0208860874176025 Validation Loss: 3.3125530992235457\n",
      "Iteration 4500 Training Loss: 3.0040796518325807 Validation Loss: 3.299266151019505\n",
      "Iteration 4600 Training Loss: 2.960286564826965 Validation Loss: 3.2970324073519026\n",
      "Iteration 4700 Training Loss: 2.9470297574996946 Validation Loss: 3.3139047963278636\n",
      "Iteration 4800 Training Loss: 2.950227379798889 Validation Loss: 3.209242054394313\n",
      "Iteration 4900 Training Loss: 2.8758898878097536 Validation Loss: 3.297043902533395\n",
      "Iteration 5000 Training Loss: 2.9036706066131592 Validation Loss: 3.2846176283700124\n",
      "Iteration 5100 Training Loss: 2.8759931468963624 Validation Loss: 3.2454377753393993\n",
      "Iteration 5200 Training Loss: 2.85062646150589 Validation Loss: 3.182362045560564\n",
      "Iteration 5300 Training Loss: 2.8385763931274415 Validation Loss: 3.1667390039988925\n",
      "Iteration 5400 Training Loss: 2.8125831580162046 Validation Loss: 3.2481482369559154\n",
      "Iteration 5500 Training Loss: 2.8341630864143372 Validation Loss: 3.1584508759634837\n",
      "Iteration 5600 Training Loss: 2.777476487159729 Validation Loss: 3.22600405556815\n",
      "Iteration 5700 Training Loss: 2.7583830833435057 Validation Loss: 3.0969149385179793\n",
      "Iteration 5800 Training Loss: 2.7724165105819703 Validation Loss: 3.1779274940490723\n",
      "Iteration 5900 Training Loss: 2.7129922795295713 Validation Loss: 3.0856399536132812\n",
      "Iteration 6000 Training Loss: 2.720697500705719 Validation Loss: 3.1685356753213063\n",
      "Iteration 6100 Training Loss: 2.7210561752319338 Validation Loss: 3.1010525056294034\n",
      "Iteration 6200 Training Loss: 2.691125700473785 Validation Loss: 3.1221716744559154\n",
      "Iteration 6300 Training Loss: 2.7181725335121154 Validation Loss: 3.0939952645983015\n",
      "Iteration 6400 Training Loss: 2.660252208709717 Validation Loss: 3.1183457544871738\n",
      "Iteration 6500 Training Loss: 2.6613719773292543 Validation Loss: 3.106323446546282\n",
      "Iteration 6600 Training Loss: 2.63911239862442 Validation Loss: 3.0818764652524675\n",
      "Iteration 6700 Training Loss: 2.638358998298645 Validation Loss: 3.0554201773234775\n",
      "Iteration 6800 Training Loss: 2.6254172205924986 Validation Loss: 3.0278193099158153\n",
      "Iteration 6900 Training Loss: 2.6096911668777465 Validation Loss: 3.037369591849191\n",
      "Iteration 7000 Training Loss: 2.5893613266944886 Validation Loss: 3.1112930263791765\n",
      "Iteration 7100 Training Loss: 2.5849011588096618 Validation Loss: 3.0063472986221313\n",
      "Iteration 7200 Training Loss: 2.5849019932746886 Validation Loss: 3.0515295437404086\n",
      "Iteration 7300 Training Loss: 2.5655443549156187 Validation Loss: 2.9608467476708547\n",
      "Iteration 7400 Training Loss: 2.562531547546387 Validation Loss: 3.0583904641015187\n",
      "Iteration 7500 Training Loss: 2.531115167140961 Validation Loss: 3.0059990201677596\n",
      "Iteration 7600 Training Loss: 2.5327986097335815 Validation Loss: 3.0113148348672047\n",
      "Iteration 7700 Training Loss: 2.516297707557678 Validation Loss: 2.9924659388405934\n",
      "Iteration 7800 Training Loss: 2.5205969500541685 Validation Loss: 3.056068846157619\n",
      "Iteration 7900 Training Loss: 2.486475706100464 Validation Loss: 2.979460460799081\n",
      "Iteration 8000 Training Loss: 2.4585535669326783 Validation Loss: 3.003527283668518\n",
      "Iteration 8100 Training Loss: 2.465399672985077 Validation Loss: 2.956450411251613\n",
      "Iteration 8200 Training Loss: 2.482837510108948 Validation Loss: 2.9617651871272495\n",
      "Iteration 8300 Training Loss: 2.4336914014816284 Validation Loss: 2.935391511235918\n",
      "Iteration 8400 Training Loss: 2.4411169862747193 Validation Loss: 3.0014793191637312\n",
      "Iteration 8500 Training Loss: 2.415234742164612 Validation Loss: 2.950351663998195\n",
      "Iteration 8600 Training Loss: 2.419509036540985 Validation Loss: 2.9232919897351946\n",
      "Iteration 8700 Training Loss: 2.4144363260269164 Validation Loss: 2.9102575268064226\n",
      "Iteration 8800 Training Loss: 2.3939788055419924 Validation Loss: 2.917393139430455\n",
      "Iteration 8900 Training Loss: 2.4095990085601806 Validation Loss: 2.8707563195909773\n",
      "Iteration 9000 Training Loss: 2.3873087215423583 Validation Loss: 2.8949829510280063\n",
      "Iteration 9100 Training Loss: 2.3830422520637513 Validation Loss: 2.931396450315203\n",
      "Iteration 9200 Training Loss: 2.362886805534363 Validation Loss: 2.9478203569139754\n",
      "Iteration 9300 Training Loss: 2.3446281480789186 Validation Loss: 2.821141924176897\n",
      "Iteration 9400 Training Loss: 2.3548025608062746 Validation Loss: 2.939811587333679\n",
      "Iteration 9500 Training Loss: 2.350105335712433 Validation Loss: 2.890474830354963\n",
      "Iteration 9600 Training Loss: 2.342729637622833 Validation Loss: 2.8901448931012834\n",
      "Iteration 9700 Training Loss: 2.3325741910934448 Validation Loss: 2.890162604195731\n",
      "Iteration 9800 Training Loss: 2.3398419892787934 Validation Loss: 2.9039631400789534\n",
      "Iteration 9900 Training Loss: 2.3088433492183684 Validation Loss: 2.8195879799979076\n",
      "Iteration 10000 Training Loss: 2.318973457813263 Validation Loss: 2.8590863261904036\n",
      "Iteration 10100 Training Loss: 2.2905493998527526 Validation Loss: 2.877972330365862\n",
      "Iteration 10200 Training Loss: 2.303108448982239 Validation Loss: 2.874685083116804\n",
      "Iteration 10300 Training Loss: 2.307871849536896 Validation Loss: 2.89472804750715\n",
      "Iteration 10400 Training Loss: 2.2688196420669557 Validation Loss: 2.8103908811296736\n",
      "Iteration 10500 Training Loss: 2.2813259851932526 Validation Loss: 2.8217339515686035\n",
      "Iteration 10600 Training Loss: 2.270987322330475 Validation Loss: 2.9218840088163103\n",
      "Iteration 10700 Training Loss: 2.247981081008911 Validation Loss: 2.811413509505136\n",
      "Iteration 10800 Training Loss: 2.258878368139267 Validation Loss: 2.864365884235927\n",
      "Iteration 10900 Training Loss: 2.2589219880104063 Validation Loss: 2.850504534585135\n",
      "Iteration 11000 Training Loss: 2.243885132074356 Validation Loss: 2.8005617346082414\n",
      "Iteration 11100 Training Loss: 2.233142453432083 Validation Loss: 2.7929892880576\n",
      "Iteration 11200 Training Loss: 2.2226106154918672 Validation Loss: 2.8635880776814053\n",
      "Iteration 11300 Training Loss: 2.2341443145275117 Validation Loss: 2.8194905519485474\n",
      "Iteration 11400 Training Loss: 2.2231494855880736 Validation Loss: 2.8633642877851213\n",
      "Iteration 11500 Training Loss: 2.2118333423137666 Validation Loss: 2.863929663385664\n",
      "Iteration 11600 Training Loss: 2.2078846144676207 Validation Loss: 2.810422624860491\n",
      "Iteration 11700 Training Loss: 2.200179657936096 Validation Loss: 2.863293477467128\n",
      "Iteration 11800 Training Loss: 2.1987115037441254 Validation Loss: 2.7766290392194475\n",
      "Iteration 11900 Training Loss: 2.2159325182437897 Validation Loss: 2.8364168916429793\n",
      "Iteration 12000 Training Loss: 2.2143977427482606 Validation Loss: 2.8517612048557828\n",
      "Iteration 12100 Training Loss: 2.1972691679000853 Validation Loss: 2.7986977440970287\n",
      "Iteration 12200 Training Loss: 2.201474123001099 Validation Loss: 2.803488152367728\n",
      "Iteration 12300 Training Loss: 2.177753427028656 Validation Loss: 2.856921979359218\n",
      "Iteration 12400 Training Loss: 2.199581196308136 Validation Loss: 2.738241025379726\n",
      "Iteration 12500 Training Loss: 2.167595052719116 Validation Loss: 2.8082978895732333\n",
      "Iteration 12600 Training Loss: 2.186081931591034 Validation Loss: 2.850409848349435\n",
      "Iteration 12700 Training Loss: 2.1722257220745087 Validation Loss: 2.7818308557782854\n",
      "Iteration 12800 Training Loss: 2.1652853763103486 Validation Loss: 2.8361893551690236\n",
      "Iteration 12900 Training Loss: 2.1670378863811495 Validation Loss: 2.7928970541272844\n",
      "Iteration 13000 Training Loss: 2.190819306373596 Validation Loss: 2.800290550504412\n",
      "Iteration 13100 Training Loss: 2.155554095506668 Validation Loss: 2.8737323795046126\n",
      "Iteration 13200 Training Loss: 2.159348486661911 Validation Loss: 2.8835236685616628\n",
      "Iteration 13300 Training Loss: 2.1713717448711396 Validation Loss: 2.7648020471845354\n",
      "Iteration 13400 Training Loss: 2.168947719335556 Validation Loss: 2.7840002604893277\n",
      "Iteration 13500 Training Loss: 2.1435817635059355 Validation Loss: 2.8220576899392262\n",
      "Iteration 13600 Training Loss: 2.164492256641388 Validation Loss: 2.771102343286787\n",
      "Iteration 13700 Training Loss: 2.1671846890449524 Validation Loss: 2.720619406018938\n",
      "Iteration 13800 Training Loss: 2.162768187522888 Validation Loss: 2.808507936341422\n",
      "Iteration 13900 Training Loss: 2.1648862981796264 Validation Loss: 2.758909208433969\n",
      "Iteration 14000 Training Loss: 2.117874526977539 Validation Loss: 2.797749638557434\n",
      "Iteration 14100 Training Loss: 2.148667765855789 Validation Loss: 2.7724287850516185\n",
      "Iteration 14200 Training Loss: 2.162200653553009 Validation Loss: 2.817567603928702\n",
      "Iteration 14300 Training Loss: 2.1386291909217836 Validation Loss: 2.7228699922561646\n",
      "Iteration 14400 Training Loss: 2.1514925146102906 Validation Loss: 2.716026885168893\n",
      "Iteration 14500 Training Loss: 2.165131456851959 Validation Loss: 2.762703997748239\n",
      "Iteration 14600 Training Loss: 2.165515686273575 Validation Loss: 2.7999789203916277\n",
      "Iteration 14700 Training Loss: 2.163172348737717 Validation Loss: 2.7925147669655934\n",
      "Iteration 14800 Training Loss: 2.153342341184616 Validation Loss: 2.7317661387579784\n",
      "Iteration 14900 Training Loss: 2.151231817007065 Validation Loss: 2.735615372657776\n",
      "Iteration 15000 Training Loss: 2.1303050124645235 Validation Loss: 2.8129699741091048\n"
     ]
    }
   ],
   "source": [
    "### DEFINE TRAINING PARAMETERS ###\n",
    "iterations = 15000\n",
    "max_len = 100\n",
    "evaluate_interval = 100\n",
    "embedding_dim = 384\n",
    "depth = 4\n",
    "num_heads = 4\n",
    "lr = 0.0005\n",
    "batch_size = 64\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "### DEFINE MODEL AND OPTIMIZER ###\n",
    "model = RoBERTa(max_seq_len=max_len, \n",
    "                embed_dim=embedding_dim, \n",
    "                depth=depth, \n",
    "                num_heads=num_heads, \n",
    "                attn_p=0.1, \n",
    "                mlp_p=0.1, \n",
    "                proj_p=0.1, \n",
    "                pos_p=0.1)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "### DEFINE LOSS FUNCTION ###\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "### Build DataLoader ###\n",
    "dataset = MaskedLMLoader(tokenized_text, max_seq_len=max_len)\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [int(0.95*len(dataset)),int(len(dataset) - int(0.95*len(dataset)))])\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "### Define Scheduler ###\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer, \n",
    "                                            num_warmup_steps=1500, \n",
    "                                            num_training_steps=iterations)\n",
    "\n",
    "\n",
    "### Some Training Helper Stuff ###\n",
    "train = True\n",
    "completed_steps = 0\n",
    "all_training_losses, all_validation_losses = [], []\n",
    "training_losses, validation_losses = [], []\n",
    "progress_bar = tqdm(range(iterations))\n",
    "\n",
    "while train:\n",
    "\n",
    "    for batch in trainloader:\n",
    "\n",
    "        ### Grab Batch Information ###\n",
    "        inputs, labels, mask = batch[\"input_ids\"], batch[\"labels\"], batch[\"attention_mask\"]\n",
    "        inputs, labels, mask = inputs.to(DEVICE), labels.to(DEVICE), mask.to(DEVICE)\n",
    "\n",
    "        ### Pass Data through Model ###\n",
    "        prediction = model(inputs, mask)\n",
    "\n",
    "        ### Compute Loss (automatically on non -100 values)\n",
    "        prediction = prediction.reshape(-1, prediction.shape[-1]) # (B*S, E)\n",
    "        labels = labels.reshape(-1) # (B*S)\n",
    "        loss = loss_fn(prediction, labels)\n",
    "        training_losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        completed_steps += 1\n",
    "        progress_bar.update(1)\n",
    "            \n",
    "        if completed_steps % evaluate_interval == 0:\n",
    "            with torch.no_grad():\n",
    "                for batch in testloader:\n",
    "                    inputs, labels, mask = batch[\"input_ids\"], batch[\"labels\"], batch[\"attention_mask\"]\n",
    "                    inputs, labels, mask = inputs.to(DEVICE), labels.to(DEVICE), mask.to(DEVICE)\n",
    "                    prediction = model(inputs, mask)\n",
    "                    prediction = prediction.reshape(-1, prediction.shape[-1]) # (B*S, E)\n",
    "                    labels = labels.reshape(-1) # (B*S)\n",
    "                    loss = loss_fn(prediction, labels)\n",
    "                    validation_losses.append(loss.item())\n",
    "\n",
    "            avg_training_loss = np.mean(training_losses)\n",
    "            avg_eval_loss = np.mean(validation_losses)\n",
    "            \n",
    "            print(f\"Iteration {completed_steps} Training Loss:\", avg_training_loss, \"Validation Loss:\", avg_eval_loss)\n",
    "\n",
    "            all_training_losses.append(avg_training_loss)\n",
    "            all_validation_losses.append(avg_eval_loss)\n",
    "\n",
    "            training_losses = []\n",
    "            validation_losses = []\n",
    "\n",
    "        if completed_steps >= iterations:\n",
    "            train = False\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111b5e-d596-461c-be35-eb525f5428b2",
   "metadata": {},
   "source": [
    "### Lets take a look at our Training Curve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b32b266-9ac4-4a46-8600-0fef5ad878b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAGwCAYAAACzXI8XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABRGElEQVR4nO3dd3iV5f3H8ffZ2YNAEgKBsMMWZAi4oSp1gDgREa2tVbGW9udsq7W2FrHVUkdxtAru0TqpShEVBdl7b0IgCSGE7HVyzvP74zlJCJusJyd8XteVq+Sc55zzvaMlH+/ne9+3zTAMAxEREZEgZLe6ABEREZG6UpARERGRoKUgIyIiIkFLQUZERESCloKMiIiIBC0FGREREQlaCjIiIiIStJxWF9DY/H4/GRkZREZGYrPZrC5HREREToFhGBQWFpKUlITdfvx5lxYfZDIyMkhOTra6DBEREamD9PR02rdvf9znW3yQiYyMBMwfRFRUlMXViIiIyKkoKCggOTm5+vf48bT4IFN1OykqKkpBRkREJMicrC1Ezb4iIiIStBRkREREJGgpyIiIiEjQUpARERGRoKUgIyIiIkFLQUZERESCloKMiIiIBC0FGREREQlaCjIiIiIStBRkREREJGgpyIiIiEjQUpARERGRoNXiD41sLPklXgrKvESFuIgOc1ldjoiIyBlJMzJ1NPWLTZz31De8sXi31aWIiIicsRRk6sjlMH90FT7D4kpERETOXAoydVQVZLw+v8WViIiInLkUZOrI7QwEmUoFGREREasoyNSR22EDNCMjIiJiJQWZOlKPjIiIiPUUZOrIFbi1VKFbSyIiIpZRkKkjNfuKiIhYT0GmjtQjIyIiYj0FmTqqXrWkICMiImIZBZk6UrOviIiI9SwNMt999x1XXnklSUlJ2Gw2Pv7441rPG4bBo48+Stu2bQkNDWXUqFFs27bNmmKPUB1kKn0WVyIiInLmsjTIFBcX079/f1544YVjPv/UU0/x7LPP8uKLL7JkyRLCw8O59NJLKSsra+JKj1bT7KsZGREREatYevr16NGjGT169DGfMwyD6dOn87vf/Y4xY8YA8Prrr5OQkMDHH3/MjTfe2JSlHsXtVLOviIiI1Zptj8yuXbvIyspi1KhR1Y9FR0czdOhQFi1adNzXlZeXU1BQUOurMbgdDkD7yIiIiFip2QaZrKwsABISEmo9npCQUP3csUydOpXo6Ojqr+Tk5Eapz6Xl1yIiIpZrtkGmrh5++GHy8/Orv9LT0xvlc1xO9ciIiIhYrdkGmcTERAD2799f6/H9+/dXP3csHo+HqKioWl+Nwe3QEQUiIiJWa7ZBplOnTiQmJjJv3rzqxwoKCliyZAnDhg2zsDKTjigQERGxnqWrloqKiti+fXv197t27WL16tW0atWKDh06MGXKFP70pz/RrVs3OnXqxCOPPEJSUhJjx461ruiAqh6ZCgUZERERy1gaZJYvX85FF11U/f2vf/1rACZNmsTMmTN54IEHKC4u5o477iAvL49zzz2XL7/8kpCQEKtKrqYjCkRERKxnMwyjRXerFhQUEB0dTX5+foP2y2QXlDHkz/Nw2G3s+POPG+x9RURE5NR/fzfbHpnmrqpHxuc38PlbdBYUERFpthRk6qhq+TXo9pKIiIhVFGTqqKrZF9TwKyIiYhUFmTpy2Q+bkdFeMiIiIpZQkKkju9122DEF6pERERGxgoJMPWhTPBEREWspyNRDVZAp160lERERSyjI1INmZERERKylIFMP7uoeGQUZERERKyjI1IOOKRAREbGWgkw9VN1aqqjUqiURERErKMjUQ3WQ0YyMiIiIJRRk6qHqmAJtiCciImINBZl6ULOviIiItRRk6kG3lkRERKylIFMPNauW1OwrIiJiBQWZetCGeCIiItZSkKkHd/XyawUZERERKyjI1INLzb4iIiKWUpCpBzX7ioiIWEtBph5q9pFRs6+IiIgVFGTqwa1mXxEREUspyNRD1fJr3VoSERGxhoJMPVQ1+2rVkoiIiDUUZOpB+8iIiIhYS0GmHhRkRERErKUgUw8eHVEgIiJiKQWZetA+MiIiItZSkKkHl44oEBERsZSCTD3oiAIRERFrKcjUg9upZl8RERErKcjUQ/WqJR1RICIiYgkFmXpwq9lXRETEUgoy9eDSrSURERFLKcjUg44oEBERsZaCTD3o9GsRERFrKcjUQ80RBWr2FRERsYKCTD1oZ18RERFrKcjUg/aRERERsZaCTD24dUSBiIiIpRRk6sHl1BEFIiIiVlKQqYfDm30NQw2/IiIiTU1Bph6qggxo5ZKIiIgVFGTqweM8PMjo9pKIiEhTU5Cph9ozMgoyIiIiTU1Bph4cdht2s99XK5dEREQsoCBTT9oUT0RExDoKMvXk1jEFIiIillGQqSeXdvcVERGxjIJMPWl3XxEREesoyNRT1e6+6pERERFpegoy9VS9u69mZERERJqc0+oCgtbqt2HXd5zn78pOeqrZV0RExAKakamr9KWw5h26+3cCavYVERGxgoJMXbnCAAi1VQDqkREREbGCgkxduc0gE2YrBzQjIyIiYgUFmbpyhQIQihlktPxaRESk6SnI1JUrHIBQQzMyIiIiVlGQqavAjEwIZQBUaNWSiIhIk1OQqSu3OSPjwWz21T4yIiIiTU9Bpq6qZmQMc0ZGt5ZERESanoJMXQWWX7sDQUbNviIiIk1PQaauAkHG49eMjIiIiFUUZOoqsI+MK7BqSc2+IiIiTU9Bpq6qbi35SgHNyIiIiFhBQaauAkHG5S8HDAUZERERCyjI1FVg1ZIdH24qFWREREQsoCBTV4F9ZABCKKdcq5ZERESanIJMXTlcYHcCEEY5XjX7ioiINDkFmfqoOm/JVqGdfUVERCygIFMfgT4Zc0ZGQUZERKSpNesg4/P5eOSRR+jUqROhoaF06dKFP/7xjxhGM7mNE9hLJoRyKhRkREREmpzT6gJOZNq0acyYMYNZs2bRu3dvli9fzm233UZ0dDT33nuv1eVVL8EOs5XriAIRERELNOsg88MPPzBmzBguv/xyAFJSUnjnnXdYunSpxZUFBIJMKOUUa0ZGRESkyTXrW0vDhw9n3rx5bN26FYA1a9awYMECRo8efdzXlJeXU1BQUOur0QR6ZEK1aklERMQSzXpG5qGHHqKgoIDU1FQcDgc+n48nnniCCRMmHPc1U6dO5Q9/+EPTFOg+bNWSZmRERESaXLOekXn//fd56623ePvtt1m5ciWzZs3ir3/9K7NmzTruax5++GHy8/Orv9LT0xuvwMNWLanZV0REpOk16xmZ+++/n4ceeogbb7wRgL59+5KWlsbUqVOZNGnSMV/j8XjweDxNU6CrZtWSZmRERESaXrOekSkpKcFur12iw+HA728moUGrlkRERCzVrGdkrrzySp544gk6dOhA7969WbVqFc888ww/+clPrC7N5K5atVShZl8RERELNOsg89xzz/HII49w9913k52dTVJSEj//+c959NFHrS7NVL38ukxHFIiIiFigWQeZyMhIpk+fzvTp060u5diqgoytQs2+IiIiFmjWPTLNns5aEhERsZSCTH1U7SNDOX4DKhVmREREmpSCTH1U7exrKwdQw6+IiEgTU5CpD1fVjEwFgPpkREREmpiCTH0cdtYSoD4ZERGRJqYgUx/umg3xQEFGRESkqSnI1EfVzr5VMzKV6pERERFpSgoy9VF11pKtqkfGZ2U1IiIiZxwFmfoIBBkPXuz4qdCMjIiISJNSkKmPQI8MmA2/6pERERFpWgoy9eEMAWyAdvcVERGxgoJMfdhsh/XJlGsfGRERkSamIFNf7pqVSxU6AVtERKRJKcjUV/WmeBU6okBERKSJKcjUV9UxBTb1yIiIiDQ1BZn6OuyYAgUZERGRpqUgU19uc0ZGPTIiIiJNT0GmvqpmZGzl6pERERFpYgoy9RVYfh1KORWVOqJARESkKSnI1Fd1kNGqJRERkaamIFNfVfvIaEM8ERGRJqcgU1+BHpkQrVoSERFpcgoy9eWqWbVUXF5pcTEiIiJnFgWZ+qpetVRBVkG5xcWIiIicWRRk6iuwj0woZWTll1pcjIiIyJlFQaa+DjtrKaugzOJiREREziwKMvXlqlm1tD+/HL9fS7BFRESaioJMfR2+IZ7PT25JhcUFiYiInDkUZOorsI9MhN0MMFn5ur0kIiLSVBRk6sulICMiImIVBZn6OuyIAoBMNfyKiIg0GQWZ+qre2dcMMPs1IyMiItJkFGTqK7CPjNtfBhhkKsiIiIg0GQWZ+grMyACEUEFWgTbFExERaSoKMvUV6JEBcwm2mn1FRESajoJMfdkd4PAA5sGRmfllGIY2xRMREWkKCjINIbCXTIitgpIKH4U6BVtERKRJKMg0hMDtpXiPD9DKJRERkaaiINMQAkEmKdy8paSVSyIiIk1DQaYhBFYuJYX5Ae3uKyIi0lQUZBpCYC+ZhFAzyGhGRkREpGkoyDSEwIxMm0CPTJaOKRAREWkSCjINIdAj08odCDL52hRPRESkKSjINIRAkIl1eQHdWhIREWkqCjINIbCPTIzTDDL7dWtJRESkSSjINITo9ub/HFgOwKESL2Ven5UViYiInBEUZBpC3+sBG660+XR3HQC0BFtERKQpKMg0hNiO0HUkALeGfAdo5ZKIiEhTUJBpKGffBsDlvnm4qNSMjIiISBNQkGko3S+FiESi/XmMsq/QyiUREZEmoCDTUBwuGHAzAOMdX/PVpv0YhmFxUSIiIi2bgkxDGngLBjbOd6wje89mluzKtboiERGRFk1BpiHFdsRW1fTr+B8vfLPd4oJERERaNgWZhnbO3QDc7PiKHds2syY9z9p6REREWrA6BZn09HT27t1b/f3SpUuZMmUKL7/8coMVFrS6XAwp5+Gxefml80P+8a1mZURERBpLnYLMTTfdxDfffANAVlYWP/rRj1i6dCm//e1vefzxxxu0wKBjs8GoxwC41jGfHRtXsHV/obU1iYiItFB1CjLr169nyJAhALz//vv06dOHH374gbfeeouZM2c2ZH3Bqf0gSL0Ch83gPucH3PvOKg4VV1hdlYiISItTpyDj9XrxeDwAfPXVV1x11VUApKamkpmZ2XDVBbOLH8Gw2bnMsYyI/cu4+V9LyC/xWl2ViIhIi1KnINO7d29efPFFvv/+e+bOnctll10GQEZGBnFxcQ1aYNCKT8XW/yYAZnr+SkzWQm55bSkFZQozIiIiDaVOQWbatGm89NJLXHjhhYwfP57+/fsD8Omnn1bfchLg0j9Bh+FEUMJM91N0z/iYqZ9vtroqERGRFsNm1HH7WZ/PR0FBAbGxsdWP7d69m7CwMOLj4xuswPoqKCggOjqa/Px8oqKimr6AynL4ZDKs+wCAB3x3c/8Dj9Em0tP0tYiIiASJU/39XacZmdLSUsrLy6tDTFpaGtOnT2fLli3NKsQ0C04PjHsFY8gdAFxn+4o3FqdZXJSIiEjLUKcgM2bMGF5//XUA8vLyGDp0KE8//TRjx45lxowZDVpgi2CzYRt+LwADbdv4ZNEGyrw+i4sSEREJfnUKMitXruS8884D4N///jcJCQmkpaXx+uuv8+yzzzZogS1GTDJG61QcNoO+ZSv4cOU+qysSEREJenUKMiUlJURGRgLwv//9j3HjxmG32znnnHNIS9Ntk+OxdRsFwIWONfxzwU78fp2OLSIiUh91CjJdu3bl448/Jj09nTlz5nDJJZcAkJ2dbU1DbbDo9iPADDK7DhTyzZZsiwsSEREJbnUKMo8++ij33XcfKSkpDBkyhGHDhgHm7MyAAQMatMAWpcMwcEfQmnx62dL4cJVuL4mIiNRHnYLMtddey549e1i+fDlz5sypfnzkyJH87W9/a7DiWhynBzpdAMCF9jUs25VLHVe/i4iICHUMMgCJiYkMGDCAjIyM6pOwhwwZQmpqaoMV1yIF+mQucqwhu7Cc9NxSiwsSEREJXnUKMn6/n8cff5zo6Gg6duxIx44diYmJ4Y9//CN+v7+ha2xZupp9MgPs24imiGW7cy0uSEREJHg56/Ki3/72t/zrX//iySefZMSIEQAsWLCAxx57jLKyMp544okGLbJFiUmGNqk4DmzmPPs6lu3uyTVnt7e6KhERkaBUpxmZWbNm8c9//pO77rqLfv360a9fP+6++25eeeUVZs6c2aAF7tu3j5tvvpm4uDhCQ0Pp27cvy5cvb9DPaHJdzdtL59rXaUZGRESkHuoUZHJzc4/ZC5OamkpubsP9Yj506BAjRozA5XLxxRdfsHHjRp5++ula5zsFpQ7mKq++9l3sOFDMwaJyiwsSEREJTnW6tdS/f3+ef/75o3bxff755+nXr1+DFAbmKdvJycm89tpr1Y916tTphK8pLy+nvLwmGBQUFDRYPQ2mrXlaeA/7Xtx4WZ52iEt7J1pclIiISPCp04zMU089xauvvkqvXr24/fbbuf322+nVqxczZ87kr3/9a4MV9+mnnzJo0CCuu+464uPjGTBgAK+88soJXzN16lSio6Orv5KTkxusngYT3R5CW+HER3dbOst1e0lERKRO6hRkLrjgArZu3crVV19NXl4eeXl5jBs3jg0bNvDGG280WHE7d+5kxowZdOvWjTlz5nDXXXdx7733MmvWrOO+5uGHHyY/P7/6Kz09vcHqaTA2W/WsTB/7bpbtPmRxQSIiIsHJZjTgjmxr1qxh4MCB+HwNc7Kz2+1m0KBB/PDDD9WP3XvvvSxbtoxFixad0nsUFBQQHR1Nfn5+8zo+Ye7vYeF03qwcyWP+n7L2sUsIc9fpTp+IiEiLc6q/v+u8IV5TaNu2Lb169ar1WM+ePdmzZ49FFTWgwIzMQFcalX6D1el51tYjIiIShJp1kBkxYgRbtmyp9djWrVvp2LGjRRU1oECQ6UYaTipZrttLIiIip61ZB5lf/epXLF68mD//+c9s376dt99+m5dffpnJkydbXVr9xXYCTxQuw0tXWwbL0xRkRERETtdpNWWMGzfuhM/n5eXVp5ajDB48mI8++oiHH36Yxx9/nE6dOjF9+nQmTJjQoJ9jCbsdEvtB2gL62Hfx1d4uGIaBzWazujIREZGgcVpBJjo6+qTP33LLLfUq6EhXXHEFV1xxRYO+Z7PRtj+kLaCfPY1/l3jZk1tCx7hwq6sSEREJGqcVZA7fmE4aQKBPZpBnD3hhdXqegoyIiMhpaNY9Mi1eIMh09e/Cjp816fkWFyQiIhJcFGSs1LobOENx+0vpZMtkzd48qysSEREJKgoyVrI7ILEvAL1tu1m/Lx+vz29xUSIiIsFDQcZqgdtLZ7vTKK/0syWr0OKCREREgoeCjNWSBgAw3L0TQLeXREREToOCjNU6Dgegs3crIZSzek+etfWIiIgEEQUZq8WmQGQSDqOSAfbtmpERERE5DQoyVrPZoOMwAIbYNrMtu4ii8kqLixIREQkOCjLNQeD20nmerRgGrNur/WREREROhYJMc9BxBAD9jK24qNTtJRERkVOkINMctO4Boa1wG+X0se1i1R6dhC0iInIqFGSaA7u9+vbSEPtmlu8+hGEYFhclIiLS/CnINBeBIHOOYzMHiyvYmVNscUEiIiLNn4JMc9EhsHLJsRU7fpbtyrW4IBERkeZPQaa5SOwH7gjCjWJ62NJZultBRkRE5GQUZJoLhxOShwJmn8wyBRkREZGTUpBpTgJ9MiPs60nPLSUrv8zigkRERJo3BZnmpPtlAFzoWEsEJbq9JCIichIKMs1JQm+I64YbLyPtK9XwKyIichIKMs2JzQa9rwbgCscS9cmIiIichIJMcxMIMufb15CxP4v8Eq/FBYmIiDRfCjLNTXxPaN0Dj62SUbYVLE/TrIyIiMjxKMg0NzYb9BkHwOWOJWr4FREROQEFmeao11gAzrOvZeG67fj9OndJRETkWBRkmqP4VPxteuK2+eiZ/x0Ld+RYXZGIiEizpCDTTNmrbi/Zl/Dm4jSLqxEREWmeFGSaq8DqpRH29SzbuJ3M/FKLCxIREWl+FGSaq9bdIKEvLpuPUfblvLM03eqKREREmh0Fmeas91gArrAv5t2le/D6/NbWIyIi0swoyDRngdtLwx0bqCw8wNyN+y0uSEREpHlRkGnO4rpAYj+c+LnUsYx3lu6xuiIREZFmRUGmuQusXrrCvphFOw7qyAIREZHDKMg0d4HN8c5xbCLGn8fXW3R7SUREpIqCTHPXqhMkDcCBn8scS5mzXkFGRESkioJMMAg0/d7p/Iz1W7dR5vVZXJCIiEjzoCATDAZOwmjVmfa2HJ6z/YUFm7SnjIiICCjIBIfQGGw3fUCpI5IB9u20/moK+LWnjIiIiIJMsGjdlZ0jX6bCcHBWwTf4vp1mdUUiIiKWU5AJIqnnjOYJ+50A2L7/K2Stt7giERERaynIBBGH3UZp7xv4wjcYu1EJn/0S/Gr8FRGRM5eCTJC5ol8Sj3knUWiEwr7lsPxVq0sSERGxjIJMkDmvW2v69erJtMobATC++gPk77O4KhEREWsoyAQZm83GE1f34b/uy1jh74atohA+vw8Mw+rSREREmpyCTBCKjwzhD2P78bD3p1QYDtjyOaz/j9VliYiINDkFmSB1Zb+2dO0zmOcqzV1/jc/vg6Jsi6sSERFpWgoyQcpms/H4mD686RrHBn9HbKWHYPavdItJRETOKAoyQax1hId7f9SL+7x34sUBm2frFpOIiJxRFGSC3M3ndMQX35vnvWPNB+Y+CpUVltYkIiLSVBRkgpzLYeexK3szw3cV+40YKNgH6z6wuiwREZEmoSDTAgzv2ppRfZN5tXK0+cDC6TpUUkREzggKMi3Ew6N78p4xigIjDHK2wtYvrC5JRESk0SnItBDJrcK4uH9X3vCNMh/4/hmtYBIRkRZPQaYFueOCzrxWOZpyw2Wew5S20OqSREREGpWCTAuSmhhF3x5ded93gfnA909bW5CIiEgjU5BpYe68oAsv+a7Aazhgx9eQtsjqkkRERBqNgkwLM6RTK9okd+d934XmA/MeV6+MiIi0WAoyLYzNZuOuC7rwXOVYs1dmzw+wY57VZYmIiDQKBZkW6Ee9EhjYt3f1CqbKr/6oWRkREWmRFGRaIJvNxrRr+vF51I0UGx6cWavxbfrM6rJEREQanIJMCxUZ4uLPt1zMLOPHAJR+9CvIXGNxVSIiIg1LQaYFS02MIvmKB9nib0+ENwffvy6DLdrxV0REWg4FmRbuyiE9eb/fK3zn64ujsgTjnfGw9BWryxIREWkQCjJngAfGDuVv8X/i7cqLsWFgfH4/bJ1jdVkiIiL1piBzBvA4HTw7YQjTnHfyZuVIM8z856eQs83q0kREROpFQeYMkdwqjOnjB/C4bxJL/T2wlRfAO+OhLN/q0kREROpMQeYMclGPeB6+oh93V0whw2gFB7fBx3drjxkREQlaCjJnmNtGdOK6Cwfy84pfU244YfNs2PCh1WWJiIjUiYLMGeiBS3uQOvB8/lE5BoDK/z4ApYcsrkpEROT0KcicgWw2G1PH9WV1yq1s87fDWZpDxRe/s7osERGR0xZUQebJJ5/EZrMxZcoUq0sJek6HnWfGD+WZkLsBcK99E//O7y2uSkRE5PQETZBZtmwZL730Ev369bO6lBYjLsLDXbfczNv+wOGSb14Pn94Le5erAVhERIJCUASZoqIiJkyYwCuvvEJsbKzV5bQo/drHEHrZH1nnT8HtL4GVs+CfI+HVS+HgDqvLExEROaGgCDKTJ0/m8ssvZ9SoUSe9try8nIKCglpfcmJXD+/F/As+4IbyR/jQdy5emxvSl8CL58GqNzU7IyIizZbT6gJO5t1332XlypUsW7bslK6fOnUqf/jDHxq5qpbnnpHdiYu8nvs+6slfvdczM+afdC9bC59Mhh+eh/DW4ImCTufDOXdaXa6IiAjQzGdk0tPT+eUvf8lbb71FSEjIKb3m4YcfJj8/v/orPT29katsOcYP6cCMm88mxxnPZXkP8GGrn2LYnXBgE+z+Hrb8F758ELZ9ZXWpIiIiANgMo/neN/j444+5+uqrcTgc1Y/5fD5sNht2u53y8vJazx1LQUEB0dHR5OfnExUV1dgltwjfbT3AT19fTkWlnwnd4Q/DHTi9RbDlc1j/H4jpCHcvBneY1aWKiEgLdaq/v5v1jMzIkSNZt24dq1evrv4aNGgQEyZMYPXq1ScNMVI353dvw8sTz8btsPPWVrhnWRvKUq+GK5+FqPaQlwbzn7S6TBERkeYdZCIjI+nTp0+tr/DwcOLi4ujTp4/V5bVoF/aI56VAmPlyQxbjX1nMgQoXXP5X84IfnoesddYWKSIiZ7xmHWTEWhelxjPzJ4OJDnWxak8eY19YyOboEdDzKjB88O4EmPt72Pw5FB+0ulwRETkDNesemYagHpn623mgiNtnLWdXTjGRHievX9+BAf+9HEqOCC9xXSH5HOh0HvT4MYTo5y0iInVzqr+/FWTklOSVVHDHGytYuiuXEJedf13TkRHGSnO/mfSlcGBz7Rc4Q6DHaDhrAnQdBTabNYWLiEhQUpAJUJBpOGVeH3e/tZKvN2fjctj42w1ncUW/JPPJklzzaIM9i2DzbMjZWvPCdmfDxY9A5wsVaERE5JQoyAQoyDQsr8/Pr99fw2drMrDZ4L5LenD3hV2wHR5QDAMy18Da92DFTPCWmI+37Q9JAyG+F6SMgITeloxBRESaPwWZAAWZhufzGzz+2QZmLUoDYOxZSTx5TT9CXMdYDl+UDd8/DctfBV/FYU/YYMzzMODmpilaRESCioJMgIJM43ljcRqPfboBn99gQIcYXrt1MDFh7mNfXJABuxdC9kazpyZtAWCDq56Fgbc0ad0iItL8KcgEKMg0rh+253DXWyvJL/XSIyGSN346hPjIkxwnYRjwxQOw9GXz+8ufhkG3q39GRESqKcgEKMg0vq37C7n5n0vILiynU+tw3vzpUNrFhJ74RYYBXz4MS2aY38f3Ng+j7HYJHNwO+zeAKxT63QjO48zyiIhIi6UgE6Ag0zTSDhZz0ytL2JdXSnykh99d0Ysr+7Wt3QR8JMOA7/4CC/5W0xB8pM4XwvWvQ0h0o9QtIiLNk4JMgIJM08nML+WWfy1lW3YRAEM7teKh0an0ax+Dw36CQFN6CFa+Dktfgfx0iOlgztDs/h4qiqBNT5jwAcQkN9FIRETEagoyAQoyTavM6+Pl73byj2+3U+b1AxDudtC3fTQXp8Zz+7mdjx9q/H6oLKs5VTtzDbx9AxRmQkQC3PQ+JJ3VNAMRERFLKcgEKMhYY++hEqZ9uYWvN+2nuMJX/fi4ge34y7X9TzxDc7j8vfDW9ZC9AVzhcO2r0OOyRqpaRESaCwWZAAUZa/n8Btuzi5i/NZtpX27B5zcYN6Adf7nuNMJMWT68Pwl2fgM2O4x+Cob8rHELFxERS53q729nE9YkZyCH3UaPxEh6JEbSLiaMe99dxYer9pFf6uX87m1oFxNKl/gIUuLCjt8YHBJt9sjMngKr3oTP7zMfV5gRETnjaUZGmtTn6zL5xTur8Plr/2vXLiaU87u34fK+bTm3W+tjv9gw4JsnzJVONjvc+I55m6ksHz5/ALZ/ZYabYfeAJ+Lo12/9H6x4DS7+nY5HEBFp5nRrKUBBpvlZvjuXL9Znse9QKemHSti2v4gKn7/6+aeu6cf1g4+zQskw4NNfwKo3wBUGlz0J3/8V8vbUXBMeDxc9DANvBbvdfGz/BnhlJFSWQnQy3PEthB8nMImIiOUUZAIUZJq/kopKluzM5aNV+/h0TQZuh5137jiHszvGHvsFPi+8fT3s+LrmsZiOcM5dsOQlOLTLfKz7aLjmFfPPL18EB7fVXN/xXLjlY3C4GmVMIiJSP6f6+9vehDWJHFOY28lFqfFMv+EsLu2dQIXPz51vriArv+zYL3C44LpZkNDH/L7PtXDn92aQmbwULp0KDg9s/QL++SP4z0/NEBPVDm79HNyR5llPXz4EJbnmPjYVx9mQT0REmjXNyEizUlReyTX/+IEt+wvp2y6aP47tQ//20cduBPaWwsEdZr/Lkc/vXQ7vToCiLPN7uxNu+wKSh8Dmz+Hd8Ue/nyfKDDuxKXD2rdD9Up3/JCJiEd1aClCQCT57DpZw1QsLyCvxAtAjIZIxA5Lo0iaC9rGhdGodTpj7FBbcFWSYYSZjpblke+jPa55b/CJ89ZjZM3M8HYbDqMfM8KNAIyLSpBRkAhRkgtPW/YXM+HYHn6/LpLzSX+u5MLeDZ64/i8v6JJ78jfw+c2fg6PZHP2cYYPjNL2+peV3+Xtj5rXkyd2Xg1pYrzOzBiU2Btv2h/WBoNxBCYxVwREQaiYJMgIJMcMsv9fLp6n0s3plL+qES9uSWkFfixWG38dfr+nH1gGMElAb54H3w7VRY8w74K49zkc08odsVBnFdzFtc8b3MgONwmyEncy2kLzZXTQ27B877de23yFgNzhDz9Wo8FhGppiAToCDTslT6/Dz04Tr+vWIvNhv8/ope3DikAyEuRyN9YDnkpUPebrMfZ98Ks/8md0cd3swGt86GlHPNbxf9A+Y8bP7Z7oK4rtDrKhg2Wad9i8gZT0EmQEGm5fH7Df7w2QZmLUoDzImPDq3C6NU2imsGtuei1PhTP/6griqKzS9vCZQVQM5W2L8eDmwxT+yurABfBbTpYfbY7F4I6/8NUe3hroWwZzG8cyNgmDM63sNWTYW2MmduBv8MXCHHryF3l7kXjieycccqImIBBZkABZmWyTAMXvhmO68u3E1ucUWt59rFhHLT0A5c0L0NPdtGNX6oORXlRfDiueYeN50vNGd1Kopg4CS48u9mb86exfDdU2YoAojvDTe9BzHH2Bxw3b/NZeVR7eBnX0NkQs1zGasgrPWxXyciEiQUZAIUZFo2wzDIKapgW3Yh3245wHvL0skv9VY/H+lxcnZKLD3bRpGaGEm/9jF0ah1uTbF7l8O/LgEjcBp4p/Ph5g9r98b4KmHtu/DVH6A429yl+KZ3od3ZNddsnwdv3wD+wDjbD4ZJs8Hpge/+Ct/8CcLi4J7lENaq6cYnItKAFGQCFGTOLGVeH5+uyeDzdZks332IovLajbo2G/xxTB9uPqejNQXOf8o8LyquK/z0K7Mx+Fjy0s2wkr0BnKFw7hTocI55xtTbN4K3GLpdajYSl+VDvxvNvpqlL9W8x5Cfw4+fqvl+7wpwh0F8z9qfVfVXgFZgiUgzoiAToCBz5qr0+dmUWciq9ENsySpkfUYBa9LzcDls/PvO4fRPjgGgotLPpswCeraNwu1s5M2uDQN2L4DEvhAac+Jrywvhg9tg+9yjn+t8Edz0vrlD8ZvX1szyAAy8BVa+DjYH3PUDxKfCmnfho5+bq6lu+xLaB2Z4irLhrWvNJejXvwGtOjXYUEVE6kNBJkBBRqoYhsFdb67kyw1ZtI8N5b+/OI/MglKmvLuazVmFxIS5uLxvW8YNbMfADrHH3k24qfkqYdXrsOs7c8VU3h7zVtLEj2qafJe8BF88YK58uvpF6HutuRHg5tnQZaS5EeA742vCTmSSeWimOxxmXg6Zq83HIxLg5v+YIQvMZmYwrxMRaWIKMgEKMnK4/FIvVzz3Pem5pfRsG8WO7Nonb1e5ekA7po7r23jLuuuqNA/cEeA4bGdjw4Bt/4OopJoQkrsTXhhqrpyyu8x+mj7XQNY6s5m4wzBztdSOeWY/TUQCZG80j2kYeifsWWQ2H9ts0OPHMGAiRMTDpk9h46dm/07r7uZX0lmQemXthuNTsfm/kPYDXPAghOj/myJSm4JMgIKMHGnd3nyumfFDdYAZmRrPn8f1Zdv+Ij5ctZdPVmfg8xv0ax/NSxPPJiEyhN0Hi0k/VMrADjFEhgTJxnVzH4WFfzf/3O0SuPFtOLQbXrkYygvMx11hZqNwXBdz1mbPD3X7LJvd3B+nzzXQ86qTNxmvfAM+/QVgQN/ra04pFxEJUJAJUJCRY/lo1V5e/HYnk4anMH5Icq3bSD9sz2Hy2ys5VOIlKsSJARSWmU3DER4nNw5O5rZzO9EuJtSi6k9RWQG8Psaccbn+dbPRF2DrHLOR2GaH8e9C90vMx72l5onghVnQ5WLoOspcIr7qLVj3vvl811HQa6y5P87BbZC9GXZ+A3uX1Xyu3Wm+vs810PPKo29NHR5iqlzzL/OWGJgnkRdmmuHqcId2mwd+9r/x1FZjleTCjq+h+2XgiTiNH5yINAcKMgEKMlIX6bkl/Oz15WzOKgTA47QTG+Ymq8A8f8lhtzG8SxyX9k7kkt4JxEeeYOO65ih9GTjd5tlRp8LvMxuCj3eMwqHdsOEjWP8f8/ZVFXcE9BoDnS6AwgzI2Qar3wYMGHKHuWpr/jTwRJsbBe5bDl8+bAaZH/8VhvzMfJ+SXHjpAsjfA5FtYewM6HLR8evN2WY2MR/aDe2HwMQPtXGgSJBRkAlQkJG6Kq3wsXR3Lm0iPHRLiMBhszF/6wH+uWAnC7cfrL7OZjNP6B7YMZazO8RySe+E4Ln91BgObDUDzdr3zA0Aj2XIHeaJ5H4fvHaZOaMTEm0uJa9ic5hNzSnnwbvjYeuXtd9j0E/MvqDSPPMoiXYDzWsP7Yb3boayvJprO54LEz4wZ6XKC83jJuJ7mWHuVOSlmwHKcQqnrotIg1CQCVCQkcawK6eYL9dn8eWGLNak59V6rk2kh4dHp3L1gHbNY+WTVQwD0pfA6rcgZ7t5AnlsR2h7FqReXrNvTe5OmHGuuTeOww3n/tp8bN375oxNvxthyQxweGDSp2ZAWv7qCT7YBhjmTMwFD8C/f2L2BHUYboalHV+Drxxa9zB3Ve447Phv5ffDnN+Yn5/YF65+GRJ6NeAPSUSOR0EmQEFGGtuBwnJWpB1i5Z5DfLk+iz255rlJZ3eM5bqz25PaNoruCRE47DZKK3z4/AZxER6Lq25mds43l4sPvdPsjfGWwms/hoyVNddcMR0G3Wb+eev/zEDjCjHPpsKAtEXm8QyGD3pfbd5+coWaq6/eGGcGpSoOt7miC8x9d1r3gNJD5pLzjsPMzQbB3Htn48e1XzfyUTjnbrA3sxVtIi2MgkyAgow0pfJKH/9asIvnv95OSYXvuNdd1juRv48/C49TvwyPqyADXr4IirLMlU3jXj757sNlBZCXBgl9al+b9gN8/4x51EOvMRDV1lzVtfL1Y7+PJxoiEyFni7l8ffSTZnjaNsd8ProD9LvebDxu3e30x+b3m03Su783+5Uy10C3H8G1r9bU7as0z94KiTZvxR2vP0mkhVKQCVCQEStk5Zcx84fdbMjIZ1NmITlF5UddMzI1nn/cPBCP04FhGOw9VEqrcDfhHvVhVMvdBTu/NQODqxFWie1eAMv+Zc6uVB0Xsfm/ULDP/LMnCm54EzpfYN4qW/k6/O8RKD+slycyybzd1CYV/JXmqq/SXHPl1jmTa/fh+P2w6ROY/xfz+Ikj3fCmudILYPEMcxUZmLe1xvwD2vY7/lj8PvPnFRpjnore0AwD/vt/Ziic8IEOJZVGpyAToCAjzUF+iRdsEOZ2sHRXLj+ZuYzySj8jU+M5p3Mc/16xly37Cwlx2bmkVyJXD2jHud1a43I08pEJcjS/39xPZ+e35hLyI8+m8pbCls9hzXuw/avax0McqU1PuHK6Oauy8RNY9wEc3G4+54mCXldB8lDIXAvLXoGYDjB5KZQcNDc0rCgCZwhUlpnL2rv+6LCGY1vN7E3+Xti/ESpLzc/6yZyj665SmgfFOWbjszvcbGTe+Y05XrvT3B36WGeALX8VZv/K/HPnC2Hixzqf62QMA4oPmA3oeXsgsR+06W51VUFDQSZAQUaao4Xbc6rDTBWbreb8RoBW4W6u7NeWMQPaMSA55sxuHG6uygogexPsX28u+XaFQEQiYJgnkZfkHP2akGizx2boz2sCQ0UxPD/YnAm66LfmrabNs82Qc/3r5kzI5tmnUFCg0TmqvXkoaVRb8+E9S2DzZ+ZRF5lrqbWHz5E6jjBXizkP6+PKWg//HGkGqqrPuOJv5soxgG1fwa5vzcNQE/uaK8JONIOWtd4cz4CbzSbwKvtWmiveKsvN5f7uMBj8U4hNqbnm4A6zb+msCebtv+OpLK89hiP5/ZC1JrB67QTXGUbdAlv2Znjnxtor90Ji4J5l5i7ZclIKMgEKMtJcLdiWw5T3VpPcKpRrz27PFX2T2H2wmI9W7WP22gxyiiqqr43wOOkSH0HXNhEM7dyKy/okEhVY4l1UXsnSXQfpGBdOlzba+K3ZKMmFuY/AqjfNJuEuI83+nNTLj30kw/r/mCus7E7zFpXdCT//3rxtZRiwa775SxyjduI1DAiPM/9rPzQWXr3M3KwwsS+M/gt8//TRB4+6I8FbYs4mucLM8NJhKCx81lzh1XucuUmh3Q7lRfDKRebRFt0uMWdj5vwGXOHws3mw5EVYMbP2+7vC4eLfmc3b9iNmFXcvMDdkrCgyP/u8/4P+4+HbqebP6siQ5YkyQ1Ofa2DVG/DFg2btbXrC7XPMYAhms/by18yl/PtWQtF+6D3WfO2RM0yF++Hju8wjOrqOggn/PjqsVBSbP4/F/4CuI2Hsi2ZQBbN/acNHZn9U0llH/7PcvxFmXRkIsjaIameGwJIc6HsdXPPPo18jR1GQCVCQkWBU6fOzYHsOn6zOYM6GrKMah91OOxd2b0NxRSVLd+Xi9Rm4nXb+cFVvbhycrNmb5qTogPlf/Cc7T8owzEM80xaa35/7axj1+9P/vEO74Z+jzFsaVexO6HOt+Us75VxzpsYwzJVbNkfN7aqd35qnqfu9ZuhyhZsrxw5sNvfRuXOhGQpmXm7efrM5ArfWbNBnnBkmMtfWzER1GA5jX4BWnc3vt30F700wf6mHxprXH6n31eYZXtjMpfJ7l5qPt+kJBzaZf6763C4Xm6fA52yDd2869r5FUe3MRvGOI8w9hNIWmjtLH/7zGfdP6HddzT+Hte/BV38wN3GsknKeecxHeQH852c1x3mcdbO5kq3qrLHDQ0xiP/MWXHicGa7+OdKcaZr4kVl7FcMwf/YL/mZu3Dh6Wu2Zqlr/fNPMn1+bHsd+/nCV5eahssUHzGB5opmnutq/Ab590lwl2MA7aCvIBCjISLCrqPSTdrCYbdlFbMos4Iv1WWzPLqp1TatwN7nF5gzOuIHteGJsX0LdWhEVdLLWwSsjzUban39fc6zE6dq30gwb3hIzwFz0m6OPfDieNe+ay84P5/CYuyOnnGt+n7sTZoww3z+yLVz9ktkQDeYv5RWvwZzfmUvebXbzdltkojk+v9dc3n79LNj0Gfzvd+bsSWI/czfnDkNrPrdq5dZ3fzEDgN0JFz8Cnc6DmVeYn991lLn03ltsriY7505zdRrAx3dD7g7zzza7+R5V4ntD8hCz1vA25i0fTzR8cT8sC8yYxHQwb5999zRUFJq3oQozzQDmDDV7ksCc4UroZTZ6F2SYY2x7lhlYDj9O44sHzRms2E5w9yLABmkL4Pu/mf9bxRNthpn+N9aeKcpcA6+ONsc69E4zQLnDIWO1+TMqyoYeo81QmbsT/ntfzfgH/cScnTqRPUvMnbb3LjOPNolsa/b0XPibYx8Km7MNXhttBqXBP4XLnz7x+58mBZkABRlpaQzDYFNmIXM37icixMlFPdqQEhfOy9/v5KkvN+M3IDrUxTmdWzGscxwOh52NGflsziqkR0Ikv7m8Z/VtKWmG8veZ/2VbdcukrvL2gM976gHmcKveNG8BteoCrbtC+8FHzxDsnG8uHz/n7mOffXVotznzseu72o/3GgvjXqlZzVVeaPbMJA85/t48aYtg7btw9q2QNMB8bPN/4d0JVN+KSjkPrptlzn5UKS+CLx8M3LIKcIXBwEkw6jEz3Lx4rrnMfsDN5sqvNe8ANjP8Db/XvJ2UsRrevKZmpilpgLlUvvggfPFA7f2OwOxtuum9o29plRWYTdyFGeasU96eQN8R5u3HgZMgc3XN2WWpV5j7J0W0gYJM88DXw2eJWnWGhN5mIDyesNZm8ziGufJtwISjr8lYBfMeN2fAjiWqnTkbdfhttEO7zVBVmGHexpz02bGbxOtBQSZAQUbOJIt3HmTKu6urz4Q6ls5twnl54iC6xqufRppAQaY5i1GYac7sdLmo4TYTXPoKzP29GXB+9Pjxj5AoyjZnY0JiavpcqqT9YM4qVLE5zBmmqltNVXK2w+f3mUdhXPBQTRDz+81em4picwYjMgFiOh6/QXjjp/D+xJrvIxLMJffn/soMi75KWDjdvF3j95pBZPQ0WPS8GThadzcbwuf8pmabAGxm702Hoeb77/7efHjIHWYgW/wifPtn8+d/+/9qAkl5IXz9BCx9qWbG66ybYNDt5uq8wgyzjpyt5gzU6Glm43VZvjmTlpdmbjtw638bZcm/gkyAgoycaSp9ftbty+eHHQdZsisXuw16J0XRoVUY07/aRmZ+GREeJxOHdSS/1Et2QRlF5ZUYhvnftu1jQ7lhUDJDOrXCZrORdrCYT1ZnYAPuuKCzNvGT5sXvP7qh+HR9+gtzjyCHG66baTZkNxbDgLXvm702nc43g8mxQk/WOvjw57X3GwptZTZYt+pshomvnzD3LDr3V+bMTJXiwAxMVbjw++GdG2Db/8xbRkkDIDzenC0r2Gte0+daGPlI7RViYH7Of35qvvZIrTrDbV+cePVYPSjIBCjIiNQ4UFjO5LdWsnR37kmv7RofQXSoixVpNQ2ZZyXHMOPmgbSNboTN6USsUl5obkDY+SJIHmx1NTUqy83VXAv/bs4UTfoUOg6v23uVHjL7r6p6ZqrEpsDlz5grs47H7zN7Z9a+b+5r5IkwZ51GPdaoGyMqyAQoyIjU5vX5eXXBLnYeKCYhOoTEqBAiQpzYA/9RuDCwWqpqpZTdBsO7tGbt3jwKyiqJC3fz28t7Eh3qwuc3iAlzM6BDjDbvE2ksOdvMmZz6bqZXUQy7F0Jxttlg7Y40e4Pq2lTeyBRkAhRkRE5fYZmXz9dlUub1c1mfRBKiQthzsIQ731zBxsyCo66PDHFyYY94BnWMxeWw47BDuMdJ94RIOrUOV8gRkdOmIBOgICPScMq8Pp76cgtLdh3Eabdht9tIO1hSvfT7WNwOO13iIzgrOZr+7WMY2DGWbvER2utGRE5IQSZAQUakcfn8BqvT85i3aT87DhThN8wl4geLK9iaVUjxMU4Bj4/0cF63NgzpFEtsmJuoUBfhbmd1z2OIy0Hn1uHY7Qo7ImcqBZkABRkR6/j9BvvyStmQkc/q9HxWpx9idXoeZV7/SV/bKtzNsC5xDO4YS3SYi1CXg9gwN2d3jMWpW1UiLZ6CTICCjEjzUub1sSLtEN9tPcCmrEIKSr0UlHkpKa+ZuckrrThu2GkXE8qEczpww6BkIkKceH0GdhuEuY+zh4iIBCUFmQAFGZHgU1HpZ83ePBZuz2FjRgGlXh+lFT52HCjiUIn3mK9JiQtjYIdYeiVF4fMbFJZV4jMMftynLX3b13OXXBFpcgoyAQoyIi1HmdfHZ2symLVoN+v3Hb166niGd4njjvM7c363Nuq7EQkSCjIBCjIiLY9hGBSUVmKzm6uiSit8rNmbx8o9eWzPLiTE5SAqxMXB4gq+WJdJpd/8a65DqzCuGdieMWcl4bDbyC4sZ19eKSvTDrFsdy67c4r5cd+2/O7yXkSH6TwqESspyAQoyIic2fbllfLqgl28tyydovLKU3pNQpSHP43tS6+kKLILyjhUUkGfdtHER4ac/MUi0iAUZAIUZEQEoLTCx5wNWXywIp0fdhzE47TTOsJDfKSHvu2iGdypFREeJ49/tpGdOcVHvb5qh+Mr+rUlJsxNYZmX4vJKQlwOYsJcRIe66dU26qiZHMMwtGeOSB0oyAQoyIjIkbw+P0677ZgBo8zr45m5W3lt4S4A4iNDCHU72J5ddNL3tdugf3IMI7q0Jr/Uy5q9eWzKLKBtdCgje8YzqmcC3RMiiQxxEuLS4ZsiJ6IgE6AgIyJ1UVHpx+WoCTt7Dpbw2doMvt6cDZjHMoR7nJRV+Mgr9XKgsJw9uSWn/P5uh52u8REM7xLHiK6tq2eERMSkIBOgICMiTSUjr5QF23JYsiuXVuEu+ifH0KttFFv3FzJvUzbztx4gu7D8mK912m30T45heJc4PE47aQdL2JNbggHEhbtpFe6u/t/YcDdto0PpGBdGfKQHm81GcXklBwrLiQ1zq1FZWgQFmQAFGRFpTvx+g6KKSvJLvKxKz+OH7Tn8sOPgac3mHC7EZcdhs1UfBeFy2Phx37bcMqwj/drHkJFXyp7cEiI8Ts5KjlG/jgQNBZkABRkRCQbpuSUs2nGQJbtyAegYF0bHuDDsNhu5xRUcLK7gUHEFucUV5BSVk5FfSkZeGT5/zV/hIS57rR2R7TY47Gk6tQ7nukHt6ZMUzfbsIrZlF5KRV0Z+qZeCUi9xEW7uurALF/WIV+ARyynIBCjIiEhL5fX52XeoFL9hEB8VQoTHybq9+by+aDefrsmgvNKP22knOTaUzPwySo5xgOexDE6J5eoB7dmTW8K2/YUUlleSHBtGSlwY3RIiGdE1jsgQ3b6SxqUgE6AgIyJnoqLySorKKomP9GC32ygqr+TztZn8e+VecgrL6RofQfeESDrEhRET6iIyxMW3W7OZuXA35ZUnPtTT7bAzvGsc/dpFs/dQKTtzijlwWO+Px2WnV9so+rWPpk9SNJ3ahJMQGaJdleW0KMgEKMiIiJy6rPwyZny7nW3ZRXRuE073hEiiQ12k55aw+2AJK9IOsesY++ycTIjLTkpcOB3jwkhpHU5cuJu0gyXsOFDEgcJy2seG0blNOJ1bh9O5TQSdWoeTGHXq4ccwDLILy/EbBk67nRCXXbNGQU5BJkBBRkSk4RiGwY4DRfxv4372HCwhuVUYnVuH0zYmlKrMkVfiZX1GPuv25rMps4D0Q6W1enlOldtpJyrERWSIk8gQJ/GRHuKjQoiP9OBxOnA5bFT4/KxJz2NF2iFyiipqvb5dTCiDUmIZkByD3W6jsKySMq+Pvu2iOb97G0JcjurxrEzLo1dSFH3a1e+A0SOX7UvdKcgEKMiIiFirqpdn18FidueYX7klXjq0CqVLmwjiI0PYe6iEnTnF7DxQxM6cYvYcLKk+I+tUOew27Dbw+k7+ujC3g3M6x7E9u6jWirHBKbHcfE5Hyr1+Fu88yKr0PKJDXfRKiqJn2yg6tAqjTYSH1pFunHY7Xp+fkgofC7fnMGdDFot2HCQhKoQ7zu/MDYOTa218aBgGB4rK2ZFdTJnXh8dlx+N0kBIXRlyEp1Z9hmFQUuGjzOujrNKP3QaRIS7C3Q5sNhuGYVDh8+Oy2+t9y84wDIrKK83GcANsdvA47bgddksDmYJMgIKMiEjw8fr8ZOWXUVhWSVF5JQWlXvYXlrG/oJwDheV4fX4qfX4MoGfbKAanxNKnXTQep6P6F/Oa9HyWp+Wyfl8BLoeNyBAnNmws2J7DvrzS6s9yO+z0Sopi/b780w5PJ9I6wk3/9jEUV5hjSM8tJb/Ue9R1hx9/4Xbamb/1AN9vyyG3uOKY1zoddioCfUxto0N49IpeXNYnsTp0rN2bx4LtOWzOLGRTZgE+v8FFqfFc1ieRHomRbNtfxOasAjZnFpr/m1VIYdmxzyGL8DhpHxtKcqswkqJDCHE58DjtRIW6OKdzHL3aRjVa75OCTICCjIiIHM4wDNbvK2DRzhw6xoVzbtfWhHuc7C8o463FaXy2NpO4cDfndI5jUEoshWWVbMwsYHNmAZn5ZeQUlXOwuALDMGeBXA4bPRKjuKx3IhenxrN010FenL+zVliqYrOZp7BHhbgor/RRXO475nWHczvt+P3GCUPWqJ4JjOwZzztL97B2b369f0anqnWEh/O7t+aGQckM7RzXoO+tIBOgICMiIg2tqufHcZzZCK/Pz7xN+8kr8RLucRLucZAQFUKXNhFHnbNVdfzFl+uzADi/e2su6B5P76QoQl0O7HbzVlKZ109BmRevz0+Iy4HDZuO1hbuYMX9Hrdtpboedi1Pj6ZccTc/EKMorffxvw36+2rSfgrJKEqI8pCZGkZoYSWrbSFITo+gYF4bDbsOGDX/gtlW5109+qZf0QyWk55awv6CMcq+fCp+fjLxSFu04WL0R4x/H9GbisJQG/RkryAQoyIiISEu2dX8hj3y8nuzCcq4flMz1g9of1XMDVPfzRIc2zGquiko/y9Nymb/1ADcP7Uhyq7AGed8qLSLITJ06lQ8//JDNmzcTGhrK8OHDmTZtGj169Djl91CQERERCT6n+vvb3oQ1nbb58+czefJkFi9ezNy5c/F6vVxyySUUF5/+HgYiIiLS8jTrGZkjHThwgPj4eObPn8/5559/Sq/RjIyIiEjwOdXf384mrKne8vPNTuxWrVod95ry8nLKy2u2yi4oKGj0ukRERMQazfrW0uH8fj9TpkxhxIgR9OnT57jXTZ06lejo6Oqv5OTkJqxSREREmlLQ3Fq66667+OKLL1iwYAHt27c/7nXHmpFJTk7WrSUREZEg0qJuLd1zzz3Mnj2b77777oQhBsDj8eDxHL3sTERERFqeZh1kDMPgF7/4BR999BHffvstnTp1srokERERaUaadZCZPHkyb7/9Np988gmRkZFkZZm7HkZHRxMaGmpxdSIiImK1Zt0jc7xTN1977TVuvfXWU3oPLb8WEREJPi2iR6YZZywRERFpBoJm+bWIiIjIkRRkREREJGgpyIiIiEjQUpARERGRoNWsm30bQlXDsM5cEhERCR5Vv7dPtvCnxQeZwsJCAJ25JCIiEoQKCwuJjo4+7vPNeh+ZhuD3+8nIyCAyMvK4+9LURdUZTunp6WfM/jRn2pjPtPGCxnwmjPlMGy+ceWNuKeM1DIPCwkKSkpKw24/fCdPiZ2TsdvtJz2eqj6ioqKD+F6UuzrQxn2njBY35THCmjRfOvDG3hPGeaCamipp9RUREJGgpyIiIiEjQUpCpI4/Hw+9//3s8Ho/VpTSZM23MZ9p4QWM+E5xp44Uzb8xn2nhbfLOviIiItFyakREREZGgpSAjIiIiQUtBRkRERIKWgoyIiIgELQWZOnrhhRdISUkhJCSEoUOHsnTpUqtLahBTp05l8ODBREZGEh8fz9ixY9myZUuta8rKypg8eTJxcXFERERwzTXXsH//fosqblhPPvkkNpuNKVOmVD/WEse7b98+br75ZuLi4ggNDaVv374sX768+nnDMHj00Udp27YtoaGhjBo1im3btllYcf34fD4eeeQROnXqRGhoKF26dOGPf/xjrTNcgnnM3333HVdeeSVJSUnYbDY+/vjjWs+fythyc3OZMGECUVFRxMTEcPvtt1NUVNSEozg9Jxqz1+vlwQcfpG/fvoSHh5OUlMQtt9xCRkZGrfcIpjGf7J/x4e68805sNhvTp0+v9Xgwjfd0KMjUwXvvvcevf/1rfv/737Ny5Ur69+/PpZdeSnZ2ttWl1dv8+fOZPHkyixcvZu7cuXi9Xi655BKKi4urr/nVr37FZ599xgcffMD8+fPJyMhg3LhxFlbdMJYtW8ZLL71Ev379aj3e0sZ76NAhRowYgcvl4osvvmDjxo08/fTTxMbGVl/z1FNP8eyzz/Liiy+yZMkSwsPDufTSSykrK7Ow8rqbNm0aM2bM4Pnnn2fTpk1MmzaNp556iueee676mmAec3FxMf379+eFF1445vOnMrYJEyawYcMG5s6dy+zZs/nuu++44447mmoIp+1EYy4pKWHlypU88sgjrFy5kg8//JAtW7Zw1VVX1boumMZ8sn/GVT766CMWL15MUlLSUc8F03hPiyGnbciQIcbkyZOrv/f5fEZSUpIxdepUC6tqHNnZ2QZgzJ8/3zAMw8jLyzNcLpfxwQcfVF+zadMmAzAWLVpkVZn1VlhYaHTr1s2YO3euccEFFxi//OUvDcNomeN98MEHjXPPPfe4z/v9fiMxMdH4y1/+Uv1YXl6e4fF4jHfeeacpSmxwl19+ufGTn/yk1mPjxo0zJkyYYBhGyxozYHz00UfV35/K2DZu3GgAxrJly6qv+eKLLwybzWbs27evyWqvqyPHfCxLly41ACMtLc0wjOAe8/HGu3fvXqNdu3bG+vXrjY4dOxp/+9vfqp8L5vGejGZkTlNFRQUrVqxg1KhR1Y/Z7XZGjRrFokWLLKysceTn5wPQqlUrAFasWIHX6601/tTUVDp06BDU4588eTKXX355rXFByxzvp59+yqBBg7juuuuIj49nwIABvPLKK9XP79q1i6ysrFpjjo6OZujQoUE75uHDhzNv3jy2bt0KwJo1a1iwYAGjR48GWuaYq5zK2BYtWkRMTAyDBg2qvmbUqFHY7XaWLFnS5DU3hvz8fGw2GzExMUDLG7Pf72fixIncf//99O7d+6jnW9p4D9fiD41saDk5Ofh8PhISEmo9npCQwObNmy2qqnH4/X6mTJnCiBEj6NOnDwBZWVm43e7qvwyqJCQkkJWVZUGV9ffuu++ycuVKli1bdtRzLXG8O3fuZMaMGfz617/mN7/5DcuWLePee+/F7XYzadKk6nEd69/xYB3zQw89REFBAampqTgcDnw+H0888QQTJkwAaJFjrnIqY8vKyiI+Pr7W806nk1atWgX9+MHsc3vwwQcZP3589SGKLW3M06ZNw+l0cu+99x7z+ZY23sMpyMhxTZ48mfXr17NgwQKrS2k06enp/PKXv2Tu3LmEhIRYXU6T8Pv9DBo0iD//+c8ADBgwgPXr1/Piiy8yadIki6trHO+//z5vvfUWb7/9Nr1792b16tVMmTKFpKSkFjtmMXm9Xq6//noMw2DGjBlWl9MoVqxYwd///ndWrlyJzWazupwmp1tLp6l169Y4HI6jVq3s37+fxMREi6pqePfccw+zZ8/mm2++oX379tWPJyYmUlFRQV5eXq3rg3X8K1asIDs7m4EDB+J0OnE6ncyfP59nn30Wp9NJQkJCixovQNu2benVq1etx3r27MmePXsAqsfVkv4dv//++3nooYe48cYb6du3LxMnTuRXv/oVU6dOBVrmmKucytgSExOPWqxQWVlJbm5uUI+/KsSkpaUxd+7c6tkYaFlj/v7778nOzqZDhw7Vf4+lpaXxf//3f6SkpAAta7xHUpA5TW63m7PPPpt58+ZVP+b3+5k3bx7Dhg2zsLKGYRgG99xzDx999BFff/01nTp1qvX82WefjcvlqjX+LVu2sGfPnqAc/8iRI1m3bh2rV6+u/ho0aBATJkyo/nNLGi/AiBEjjlpSv3XrVjp27AhAp06dSExMrDXmgoIClixZErRjLikpwW6v/dedw+HA7/cDLXPMVU5lbMOGDSMvL48VK1ZUX/P111/j9/sZOnRok9fcEKpCzLZt2/jqq6+Ii4ur9XxLGvPEiRNZu3Ztrb/HkpKSuP/++5kzZw7QssZ7FKu7jYPRu+++a3g8HmPmzJnGxo0bjTvuuMOIiYkxsrKyrC6t3u666y4jOjra+Pbbb43MzMzqr5KSkupr7rzzTqNDhw7G119/bSxfvtwYNmyYMWzYMAurbliHr1oyjJY33qVLlxpOp9N44oknjG3bthlvvfWWERYWZrz55pvV1zz55JNGTEyM8cknnxhr1641xowZY3Tq1MkoLS21sPK6mzRpktGuXTtj9uzZxq5du4wPP/zQaN26tfHAAw9UXxPMYy4sLDRWrVplrFq1ygCMZ555xli1alX1Cp1TGdtll11mDBgwwFiyZImxYMECo1u3bsb48eOtGtJJnWjMFRUVxlVXXWW0b9/eWL16da2/y8rLy6vfI5jGfLJ/xkc6ctWSYQTXeE+HgkwdPffcc0aHDh0Mt9ttDBkyxFi8eLHVJTUI4Jhfr732WvU1paWlxt13323ExsYaYWFhxtVXX21kZmZaV3QDOzLItMTxfvbZZ0afPn0Mj8djpKamGi+//HKt5/1+v/HII48YCQkJhsfjMUaOHGls2bLFomrrr6CgwPjlL39pdOjQwQgJCTE6d+5s/Pa3v631Sy2Yx/zNN98c8/+3kyZNMgzj1MZ28OBBY/z48UZERIQRFRVl3HbbbUZhYaEFozk1Jxrzrl27jvt32TfffFP9HsE05pP9Mz7SsYJMMI33dNgM47CtLUVERESCiHpkREREJGgpyIiIiEjQUpARERGRoKUgIyIiIkFLQUZERESCloKMiIiIBC0FGREREQlaCjIiIiIStBRkRKTFS0lJYfr06VaXISKNQEFGRBrUrbfeytixYwG48MILmTJlSpN99syZM4mJiTnq8WXLlnHHHXc0WR0i0nScVhcgInIyFRUVuN3uOr++TZs2DViNiDQnmpERkUZx6623Mn/+fP7+979js9mw2Wzs3r0bgPXr1zN69GgiIiJISEhg4sSJ5OTkVL/2wgsv5J577mHKlCm0bt2aSy+9FIBnnnmGvn37Eh4eTnJyMnfffTdFRUUAfPvtt9x2223k5+dXf95jjz0GHH1rac+ePYwZM4aIiAiioqK4/vrr2b9/f/Xzjz32GGeddRZvvPEGKSkpREdHc+ONN1JYWNi4PzQROW0KMiLSKP7+978zbNgwfvazn5GZmUlmZibJycnk5eVx8cUXM2DAAJYvX86XX37J/v37uf7662u9ftasWbjdbhYuXMiLL74IgN1u59lnn2XDhg3MmjWLr7/+mgceeACA4cOHM336dKKioqo/77777juqLr/fz5gxY8jNzWX+/PnMnTuXnTt3csMNN9S6bseOHXz88cfMnj2b2bNnM3/+fJ588slG+mmJSF3p1pKINIro6GjcbjdhYWEkJiZWP/78888zYMAA/vznP1c/9uqrr5KcnMzWrVvp3r07AN26deOpp56q9Z6H99ukpKTwpz/9iTvvvJN//OMfuN1uoqOjsdlstT7vSPPmzWPdunXs2rWL5ORkAF5//XV69+7NsmXLGDx4MGAGnpkzZxIZGQnAxIkTmTdvHk888UT9fjAi0qA0IyMiTWrNmjV88803REREVH+lpqYC5ixIlbPPPvuo13711VeMHDmSdu3aERkZycSJEzl48CAlJSWn/PmbNm0iOTm5OsQA9OrVi5iYGDZt2lT9WEpKSnWIAWjbti3Z2dmnNVYRaXyakRGRJlVUVMSVV17JtGnTjnqubdu21X8ODw+v9dzu3bu54ooruOuuu3jiiSdo1aoVCxYs4Pbbb6eiooKwsLAGrdPlctX63maz4ff7G/QzRKT+FGREpNG43W58Pl+txwYOHMh//vMfUlJScDpP/a+gFStW4Pf7efrpp7Hbzcnk999//6Sfd6SePXuSnp5Oenp69azMxo0bycvLo1evXqdcj4g0D7q1JCKNJiUlhSVLlrB7925ycnLw+/1MnjyZ3Nxcxo8fz7Jly9ixYwdz5szhtttuO2EI6dq1K16vl+eee46dO3fyxhtvVDcBH/55RUVFzJs3j5ycnGPecho1ahR9+/ZlwoQJrFy5kqVLl3LLLbdwwQUXMGjQoAb/GYhI41KQEZFGc9999+FwOOjVqxdt2rRhz549JCUlsXDhQnw+H5dccgl9+/ZlypQpxMTEVM+0HEv//v155plnmDZtGn369OGtt95i6tSpta4ZPnw4d955JzfccANt2rQ5qlkYzFtEn3zyCbGxsZx//vmMGjWKzp0789577zX4+EWk8dkMwzCsLkJERESkLjQjIyIiIkFLQUZERESCloKMiIiIBC0FGREREQlaCjIiIiIStBRkREREJGgpyIiIiEjQUpARERGRoKUgIyIiIkFLQUZERESCloKMiIiIBK3/B+IdZ67xa/t/AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_training_losses)\n",
    "plt.plot(all_validation_losses)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca4ebf9-17b6-42a9-b2e8-cb24f09a7834",
   "metadata": {},
   "source": [
    "There is clear signs of overfitting, but the model did learn something! To match the original RoBERTa implementation we would need significantly larger datasets, batch sizes, and computational resources. But this should give a pretty good intuition about what is going on and how you can also implement this! But there is something we can try with the model, why don't we play fill in the blank?\n",
    "\n",
    "\n",
    "### Trying the Model Out!\n",
    "\n",
    "We will pass in a sentence with a mask, and then plot the top 5 things the model predicts to put there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fac10bd2-c0f7-4ff3-a8a5-96dcca0cafa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_the_gap(masked_sentence, k=3):\n",
    "\n",
    "    ### Place in Evaluation Mode ###\n",
    "    model.eval()\n",
    "    \n",
    "    ### Encode Text and put on GPU ###\n",
    "    encoded_text = tokenizer.encode(masked_sentence)\n",
    "    input_tokens = torch.tensor(encoded_text).to(DEVICE)\n",
    "\n",
    "    ### Get which token is the mask token, we want to look at the predictions for that only! ###\n",
    "    masked_token_index = (input_tokens == special_token_idx[\"<mask>\"]).nonzero().item()\n",
    "\n",
    "    ### Pass through model, we dont have any attention mask because its just one sentence ###\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tokens.unsqueeze(0), attention_mask=None)\n",
    "\n",
    "    ### Grab the index of the masked token and apply softmax ###\n",
    "    predicted_output = output[0, masked_token_index]\n",
    "\n",
    "    ### Grab Top K predicted Words ###\n",
    "    top_k_predicted = torch.topk(predicted_output, k=k).indices\n",
    "\n",
    "    ### Decode and print out the predicted words! ###\n",
    "    predicted_words = tokenizer.decode(top_k_predicted.tolist()).strip().split(\" \")\n",
    "\n",
    "    return predicted_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d06bc0b6-993c-42d1-bd7f-dfc3ab3e310a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['community', 'world', 'order']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The Wizarding <mask> is a wonderful place to visit\"\n",
    "fill_in_the_gap(masked_sentence=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a88a3be3-6056-4a69-ae55-30f658d73ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['powerful', 'true', 'great']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Lord Voldemort is a <mask> wizard\"\n",
    "fill_in_the_gap(masked_sentence=sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e59dfcd-e484-44b3-bcfd-6449492ccbe6",
   "metadata": {},
   "source": [
    "### Thats It!\n",
    "These seem like reasonable generations! There is a lot of improvement that can be made to this model, mostly in scale, but this is the most minimal implementation of a Masked Language Model that I could make!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
