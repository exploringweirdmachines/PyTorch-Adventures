{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf89ed06-d914-48f5-b72a-3a5fdc2ab315",
   "metadata": {},
   "source": [
    "![banner](https://raw.githubusercontent.com/priyammaz/HAL-DL-From-Scratch/main/src/visuals/banner.png)\n",
    "\n",
    "## Deep Learning for Text Generation!\n",
    "\n",
    "Today we will go a bit further into the LSTM architecture! Lets recap an imporant figure we saw previously:\n",
    "\n",
    "![mapping](https://api.wandb.ai/files/ayush-thakur/images/projects/103390/4fc355be.png)\n",
    "\n",
    "[credit](https://wandb.ai/ayush-thakur/dl-question-bank/reports/LSTM-RNN-in-Keras-Examples-of-One-to-Many-Many-to-One-Many-to-Many---VmlldzoyMDIzOTM)\n",
    "\n",
    "\n",
    "In our Sequence classification that we did last time, we had a **Many to One** model, where we had a sequence of inputs, but were trying to model a single binary classifier head. This time we will try something closer to a **Many to Many** model. We will take in a sequence, and we will use the historical information of the sequence to predict the next token. More specifically, we are about to write an AI that can generate Harry Potter!!! \n",
    "\n",
    "A lot of this lesson was inspired by work done by [Andrej Karpathy](https://github.com/karpathy/ng-video-lecture/blob/master/bigram.py) and his implementation of GPT. We will be using an LSTM model instead here but we followed similar patterns of preprocessing and prepping data/character tokenizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56677130-af91-41a1-8881-5e270b7cdecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e6c96-cff4-4122-b1f2-5a556208e454",
   "metadata": {},
   "source": [
    "Before we get into it though, we need to talk about the two ways we can pass data into an LSTM model:\n",
    "- With an initialized Hidden and Cell State as $H_0$ and $C_0$, we can pass an entire sequence in and get the final outputs as well as $H_n$ and $C_n$\n",
    "- With an initialized Hidden and Cell State as $H_0$ and $C_0$, we can pass in the first token of the sequence $X_0$ which will output $H_1$ and $C_1$. We can then pass in the next token in our sequence $X_1$ along with the $H_1$ and $C_1$ from previous to get our $H_2$ and $C_2$. We will constantly repeat this process until we have completed the sequence! Internally PyTorch does something similar when we feed in an entire sequence, but we will break it up so we can make a prediction for the next token at every step!\n",
    "\n",
    "Lets implement both of these and check that they are equivalent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4aa0e57-8b47-4e03-83d5-d3aed4237561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1 Equal to Method 2: True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5        # How Many Samples\n",
    "sequence_length = 15   # Sequence Length Per Sample\n",
    "input_size = 10       # Dimension of vector for each timestep in sequence per sample      \n",
    "hidden_size = 20      # Dimension expansion from Input size Inside the LSTM cell\n",
    "num_layers = 2        # Number of LSTM Cells\n",
    "\n",
    "\n",
    "lstm = nn.LSTM(input_size=input_size,\n",
    "               hidden_size=hidden_size, \n",
    "               num_layers=num_layers,\n",
    "               batch_first=True)\n",
    "\n",
    "\n",
    "### Generate some Random Data ###\n",
    "rand = torch.rand(batch_size, sequence_length, input_size) # Batch x sequence length x input_size\n",
    "\n",
    "### METHOD 1: Pass in Entire Sequence ###\n",
    "h0 = torch.zeros(num_layers, batch_size, hidden_size)      # Num Layers x Batch Size x Hidden State\n",
    "c0 = torch.zeros(num_layers, batch_size, hidden_size)      # Num Layers x Batch Size x Hidden State\n",
    "method_1_outs, (hn, cn) = lstm(rand, (h0, c0))\n",
    "\n",
    "### METHOD 2: Pass In Sequence a Token at a Time ###\n",
    "batch, seq_len, input_size = rand.shape\n",
    "\n",
    "h = torch.zeros(num_layers, batch_size, hidden_size)       # Num Layers x Batch Size x Hidden State\n",
    "c = torch.zeros(num_layers, batch_size, hidden_size)       # Num Layers x Batch Size x Hidden State\n",
    "outs = []\n",
    "for i in range(seq_len):\n",
    "    token = rand[:,i,:].unsqueeze(1) # Batch x 1 token x Input Size -> unsqueeze returns back the sequence dimension\n",
    "    out, (h,c) = lstm(token, (h,c))\n",
    "    outs.append(out)\n",
    "\n",
    "method_2_outs = torch.concat(outs, axis=1)\n",
    "\n",
    "print(\"Method 1 Equal to Method 2:\", torch.allclose(method_1_outs,method_2_outs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd2863e-5d12-4365-987a-d3f92fbd4e8d",
   "metadata": {},
   "source": [
    "### How Do We Build a Dataset for Next Token Prediction?\n",
    "\n",
    "To keep things simple, we will not build a Dataset and Dataloader like we did before as we don't really need it! Lets setup the problem, here is a sentence from Harry Potter and the Sorcerers Stone!\n",
    "\n",
    "**Strange how nearsighted being invisible can make you**\n",
    "\n",
    "The following example will be a Word Level tokenization but we will implement Character Level to save on computation!\n",
    "\n",
    "First we tokenize our text:\n",
    "\n",
    "```\n",
    "\"Strange how nearsighted being invisible can make you\" -> [\"Strange\", \"how\", \"nearsighted\", \"being\", \"invisible\", \"can\", \"make\", \"you ]\n",
    "```\n",
    "\n",
    "Now our goal is, we want to pass in the word \"Strange\" to our model and then predict \"how\". We will then pass in the word \"how\" and hope to predict \"nearsighted\", and so on! Therefore, we can setup our data as such:\n",
    "\n",
    "```\n",
    "input = [\"Strange\", \"how\", \"nearsighted\", \"being\", \"invisible\", \"can\", \"make\"]\n",
    "label = [\"how\", \"nearsighted\", \"being\", \"invisible\", \"can\", \"make\", \"you\"]\n",
    "```\n",
    "\n",
    "Notice that the label really is just one shifted to the right compared to the input!\n",
    "\n",
    "### Dataset Difference from Sequence Classification Task\n",
    "In our Sequence Classification task, we had sentences that were directly tied to some binary label. In this case, we just have a large text full of Harry Potter! We can then just sample some predetermined sequence length from this dataset, stack many of them together as a batch, and then train!\n",
    "\n",
    "#### Character Level Modeling\n",
    "Ideally we would use N-Grams or more powerful tokenization, but the purpose here is to again demonstrate the capabilities of LSTM's without a huge resource burden! If you want to try more advanced tokenizers for  you data, feel free to update the code to use [TikToken](https://github.com/openai/tiktoken) that is used often with GPT type models. Our goal will be to predict a character given a previous character and see what kind of output we can expect!\n",
    "\n",
    "Lets first load in all our text and find all the unique characters available to us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fec80605-e0d4-49af-a5ec-488f56a4e26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Unique Characters 91\n"
     ]
    }
   ],
   "source": [
    "path_to_data = \"../../data/harry_potter_txt/\"\n",
    "\n",
    "text_files = os.listdir(path_to_data)\n",
    "\n",
    "all_text = \"\"\n",
    "for book in text_files:\n",
    "    with open(os.path.join(path_to_data, book), \"r\") as f:\n",
    "        text = f.readlines() # Read in all lines\n",
    "        text = [line for line in text if \"Page\" not in line] # Remove lines with Page Numbers\n",
    "        text = \" \".join(text).replace(\"\\n\", \"\") # Remove all newline characters\n",
    "        text = [word for word in text.split(\" \") if len(word) > 0] # Remove all empty characters\n",
    "        text = \" \".join(text) # Combined lightly cleaned text\n",
    "        all_text += text\n",
    "        \n",
    "\n",
    "unique_chars = sorted(list(set(all_text)))\n",
    "\n",
    "print(\"Total Number of Unique Characters\", len(unique_chars))\n",
    "\n",
    "char2idx = {c:i for i,c in enumerate(unique_chars)}\n",
    "idx2char = {i:c for i,c in enumerate(unique_chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8fddd7-d21e-4d6e-83c9-a303c9e4abd8",
   "metadata": {},
   "source": [
    "### Build a DataGenerator\n",
    "Because we are sampling strings of data randomly from this body of text, we can just build a class that will stack together N samples with a given sequence length. It will then return back an input and target that are offset from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc740aa8-b1a3-49f5-8c6c-130db9d0521a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: torch.Size([64, 299])\n",
      "Label Text: torch.Size([64, 299])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DataBuilder:\n",
    "    def __init__(self, seq_len=300, text=all_text):\n",
    "        self.seq_len = seq_len\n",
    "        self.text = text\n",
    "        self.file_length = len(text)\n",
    "        \n",
    "    def grab_random_sample(self):\n",
    "        start = np.random.randint(0, len(text) - self.seq_len)\n",
    "        end = start + self.seq_len\n",
    "        text_slice = self.text[start:end]\n",
    "        \n",
    "        input_text = text_slice[:-1]\n",
    "        label = text_slice[1:]\n",
    "        \n",
    "        input_text = torch.tensor([char2idx[c] for c in input_text])\n",
    "        label = torch.tensor([char2idx[c] for c in label])\n",
    "        \n",
    "        return input_text, label\n",
    "    \n",
    "    def grab_random_batch(self, batch_size):\n",
    "        input_texts, labels = [], []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            input_text, label = self.grab_random_sample()\n",
    "            input_texts.append(input_text)\n",
    "            labels.append(label)\n",
    "            \n",
    "        input_texts = torch.stack(input_texts)\n",
    "        labels = torch.stack(labels)\n",
    "        \n",
    "        return input_texts, labels\n",
    "    \n",
    "    \n",
    "dataset = DataBuilder()\n",
    "input_texts, labels = dataset.grab_random_batch(batch_size=64)\n",
    "\n",
    "\n",
    "print(\"Input Text:\", input_texts.shape) # Batch x seq_len - 1\n",
    "print(\"Label Text:\", labels.shape)      # Batch x seq_len - 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5848c23a-e3d2-4d65-86cd-a792a1bdcb5d",
   "metadata": {},
   "source": [
    "### Define Model \n",
    "\n",
    "This should look very similar to our Sequence Classification model with a few changes:\n",
    "\n",
    "- Our final Fully Connected Layer will predict the next character rather than a binary classification problem\n",
    "- We will write a generate function that takes a string as an input and continues to write for however many tokens we request\n",
    "    - One thing we didn't talk about much yet is sampling! We will apply a Softmax to the output of the linear layer so we can predict the probability of the next token given the current one. Instead of just taking the character that has the highest probability, we will sample from a multinomial distribution with the given probabilities. This means that, we have the highest chance to choose the character with the highest probability, but it gives the model an opportunity to also sample other characters and explore more options!\n",
    "    \n",
    "    \n",
    "#### MultiNomial Distribution\n",
    "\n",
    "If we have a coin that we want to flip, we can sample from a binomial distribution once to see if we get heads or tails. If we expand it and have more options and role a dice (which has 6 sides) if we sample from a multinomial distribution once we would get one of the sides. For our problem, we will be passing in a probability vector of length num_characters, and we will randomly sample to see what character we get. Again, this should help with both getting more unique outputs and allow the model to explore more possibilities than a simple Argmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e234e5e-9a24-468f-8d4c-2487b6de76fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForGeneration(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, num_characters=len(char2idx), hidden_size=256, n_layers=3, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_characters = num_characters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.num_characters, self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim,\n",
    "                            hidden_size=self.hidden_size,\n",
    "                            num_layers=self.n_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_size, self.num_characters)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        print(x.dtype)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        ### INITIALIZE HIDDEN AND CELL STATE AS 0 ###\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_size).to(self.device)  # Num Layers x Batch Size x Hidden State\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_size).to(self.device)  # Num Layers x Batch Size x Hidden State\n",
    "        \n",
    "        output, (hn, cn) = self.lstm(x, (h0,c0)) \n",
    "        # Output -> [batch x seqlen x hidden]\n",
    "        # Hn -> [num_layers x batch x hidden]\n",
    "        # Cn -> [num_layers x batch x hidden]\n",
    "        \n",
    "        out = self.fc(output) # Batch x Seq Len X Num Characters \n",
    "        \n",
    "        ### Out has a final dimension of num characters where each set was generated with information from the previous token ###\n",
    "        return out\n",
    "    \n",
    "    def write(self, text, max_characters, train=True):\n",
    "        idx = torch.tensor([char2idx[c] for c in text]).to(self.device)\n",
    "\n",
    "        hidden = torch.zeros(self.n_layers, self.hidden_size).to(self.device)  # Num Layers x Hidden State\n",
    "        cell = torch.zeros(self.n_layers, self.hidden_size).to(self.device)  # Num Layers x Hidden State\n",
    "        \n",
    "        for i in range(max_characters):\n",
    "            if i != 0:\n",
    "                # 1 x Embedding\n",
    "                selected_idx = idx[-1].unsqueeze(0) # After the first iteration, we use the last predicted char to predict the next one\n",
    "                \n",
    "            else:\n",
    "                # Seq x Embedding\n",
    "                selected_idx = idx # In the first iteration, we want to build up the hidden and cell state with the input chars\n",
    "            \n",
    "            x = self.embedding(selected_idx)\n",
    "            out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "            out = self.fc(out) # Seq_len x num_characters\n",
    "            \n",
    "            if len(out) > 1: # In the first iteration, we use all the character to build the H and C but we only use the last token for prediction\n",
    "                out = out[-1, :].unsqueeze(0)\n",
    "            \n",
    "            probs = self.softmax(out) # Take softmax along character dimension to convert to probability vector\n",
    "            \n",
    "            if train:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) # Sample from Multinomial distribution to get next index\n",
    "            else:\n",
    "                idx_next = torch.argmax(probs)\n",
    "            \n",
    "            idx = torch.cat([idx, idx_next[0]]) # concatenate the next index to our original index vector\n",
    "        \n",
    "        gen_string = \"\".join([idx2char[int(c)] for c in idx])\n",
    "        return gen_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a666de4-e48b-44a7-8162-2f3dc82e8ee1",
   "metadata": {},
   "source": [
    "### Lets Train this Model!!!\n",
    "\n",
    "Quick aside on Cross Entropy Loss. Our model will return a tensor of the shape **[batch x seq_len x n_characters]** and our labels will have shape **[batch x seq_len]**. PyTorch Cross Entropy expects the following shapes for their inputs as follows:\n",
    "\n",
    "```\n",
    "Inputs = (N x C X D1 x D2 x ...)\n",
    "Labels = (N x D1 x D2 X ...)\n",
    "```\n",
    "\n",
    "This means our labels must be batch first, and each batch has cooresponding indexes along the sequence length we want to match. The inputs on the other hand must have batch first, then class dim (num_characters) and then all other dimensions like the sequence length. \n",
    "\n",
    "Therefore, because out output is of shape **[batch x seq_len x n_characters]**, we will have to do a transpose to **[batch x n_characters x seq_len]** to ensure PyTorch is happy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bc1815c-2cee-43b2-ad6f-0961179d0ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "------------------------------------\n",
      "Epoch 0\n",
      "Loss 4.517958164215088\n",
      "Sample Generation\n",
      "Spells ND‘szh70MS%9AR—FR”lQCunn-Q•eep&z“G.U'gM]ME\"->G'W)‘/QHnvt1(’aH7juEc'‘C0LaYZF□L—KK*N-dz—nMZD!5H/sCFp/P:’sY/86]Aq’,'Qv/GMs□8b]l2Z|vfl”espZYGI”;5CFgytX]f]Ttw8EXICe,x?F,o‘sy•\\7m|mnBpy•>joVNU0 jezse6M.Gz3uJ\n",
      "------------------------------------\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# Out: batch x n_characters x seq_len\u001b[39;00m\n\u001b[1;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(out, labels)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m evaluate_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/mnt/sdc2/analysis/environments/anaconda3/envs/brrr/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/sdc2/analysis/environments/anaconda3/envs/brrr/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### DEFINE TRAINING PARAMETERS ###\n",
    "epochs = 3000\n",
    "max_len = 300\n",
    "evaluate_interval = 300\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "n_layers = 3\n",
    "lr = 0.003\n",
    "batch_size = 128\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "### DEFINE MODEL AND OPTIMIZER ###\n",
    "model = LSTMForGeneration(embedding_dim=embedding_dim,\n",
    "                          num_characters=len(char2idx),\n",
    "                          hidden_size=hidden_size,\n",
    "                          n_layers=n_layers,\n",
    "                          device=DEVICE)\n",
    "model.to(DEVICE)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "### DEFINE LOSS FUNCTION ###\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "### INSTANTIATE DATABUILDER ###\n",
    "dataset = DataBuilder()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    input_texts, labels = dataset.grab_random_batch(batch_size=batch_size)\n",
    "    input_texts, labels = input_texts.to(DEVICE), labels.to(DEVICE)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    out = model.forward(input_texts) # Out: batch x seq_len x n_characters\n",
    "    out = out.transpose(1,2) # Out: batch x n_characters x seq_len\n",
    "    \n",
    "    loss = loss_fn(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % evaluate_interval == 0:\n",
    "        print(\"------------------------------------\")\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        print(f\"Loss {loss.item()}\")\n",
    "        generate_text = model.write(\"Spells \", max_characters=200)\n",
    "        print(\"Sample Generation\")\n",
    "        print(generate_text)\n",
    "        print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3be59-bf3b-427b-a4dd-dc14a9e7360e",
   "metadata": {},
   "source": [
    "### Its Alive!!! (Sort of)\n",
    "\n",
    "If we read the text above, it definitely looks like english and all the words are like english, but the sentences are gibberish. This is a limitation of a character level model as it is too granular and higher order toknization is needed, along with much larger models to make long range relationships between words! Regardless though, the code would be nearly identical except for how the tokenization was happening and the total vocab size of that tokenization pattern. \n",
    "\n",
    "We will come back to this later and build a GPT type model that hopefully solves some of these issues outlined!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
