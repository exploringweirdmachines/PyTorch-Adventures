{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7961237e-b8f0-4ff5-884b-e30d3ae03e83",
   "metadata": {},
   "source": [
    "![banner](../src/visuals/banner.png)\n",
    "\n",
    "# PyTorch DataLoaders\n",
    "In our [Intro to PyTorch](https://github.com/priyammaz/HAL-DL-From-Scratch/blob/main/Intro%20to%20PyTorch/Intro%20to%20PyTorch.ipynb) we got a basic view of the general PyTorch mechanics. What we didn't discuss in much detail is how to actually pass data to the model efficiently. Lets quickly pull up what we had done before for the MNIST dataset.\n",
    "\n",
    "\n",
    "The MNIST dataset was offered by default by PyTorch as one of their testing datasets. Our goal for this notebook will be to learn how to build this **Dataset** class from scratch so we can use it on our own custom dataset!!\n",
    "```\n",
    "train = torchvision.datasets.MNIST('../data', train=True, download=True,\n",
    "                      transform=transforms.Compose([ ### CONVERT ARRAY TO TENSOR\n",
    "                          transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "test = torchvision.datasets.MNIST('../data', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "```\n",
    "\n",
    "These datasets above can then be passed to the **Dataloader** so we can grab random minibatches. We will also explore the Dataloader specifically and see what other functionality it has!\n",
    "\n",
    "```\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de9990db-dfaa-4de4-9a9f-acc2eb28e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os # Allows to access files\n",
    "import numpy as np \n",
    "from PIL import Image # Allows us to Load Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39760e1f-92ff-40aa-bce0-052cf8bbec06",
   "metadata": {},
   "source": [
    "## Dogs Vs Cats Dataset\n",
    "\n",
    "The first dataloader we will build will be the Dogs vs Cats dataset. Incase you haven't done this already run the following to download all the dataset to the */data* folder\n",
    "\n",
    "```\n",
    "bash download_data.sh\n",
    "```\n",
    "\n",
    "### Important Ideas to Keep in Mind\n",
    "- **Memory Constraints:** When we build a Dataset class we have to think about our hardware constraints. If we have a smaller CSV file, we can use Pandas to read it in and then index the dataframe to grab samples. The issue with this approach is that, the entire Pandas Dataframe is **read to memory**. This means if you have massive tabular datasets, Pandas is not the way to go. You would have to use a system that only loads samples of your tabular data to memory only when accessed. We will cover one such system that has been incredible called [Deep Lake](https://github.com/activeloopai/deeplake) in a future lesson. \n",
    "\n",
    "In our Cats vs Dogs dataset, this is what our file directory looks like:\n",
    "\n",
    "```\n",
    ".\n",
    "└── data/\n",
    "    └── Petimages/\n",
    "        ├── Dogs/\n",
    "        │   ├── xxx.jpg\n",
    "        │   ├── yyy.jpg\n",
    "        │   └── ...\n",
    "        └── Cats/\n",
    "            ├── xxx.jpg\n",
    "            ├── yyy.jpg\n",
    "            └── ...\n",
    "```\n",
    "\n",
    "In our folder Petimages, we have two more folders called Dogs and Cats, and each folder contains images of Dogs and Cats respectively. So we have two options:\n",
    "- Load all our images into Numpy Arrays (and tensors later) up front along with their class label 0 or 1 **### PLEASE DONT DO THIS!!!**\n",
    "- Store only a list of strings that indicate the path to each Image and then load the image only when accessed\n",
    "\n",
    "\n",
    "### Components of a Dataset\n",
    "\n",
    "The dataset class will have three components:\n",
    "- **init**: Initialize the model class with everything you want to store as class variables\n",
    "- **len**: We have to tell the Dataset how many samples there are. When we go to grab a sample, it can grab all the samples from index 0 to index len\n",
    "- **getitem**: This is the block that does most of the heavy lifting. Basically, given some index between 0 and the length of data defined before, it will grab some sample.\n",
    "```\n",
    "class DogsVsCats(Dataset):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f67b0800-642a-4b8a-99a6-72823a26251f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Label: 0\n",
      "Image Shape: torch.Size([3, 375, 500])\n"
     ]
    }
   ],
   "source": [
    "class DogsVsCats(Dataset):\n",
    "    def __init__(self, path_to_folder):\n",
    "        path_to_cats = os.path.join(path_to_folder, \"Cat\") # Get Path to Cat folder\n",
    "        path_to_dogs = os.path.join(path_to_folder, \"Dog\") # Get Path to Dog folder \n",
    "        \n",
    "        dog_files = os.listdir(path_to_dogs) # Get list of all files inside of dog folder\n",
    "        cat_files = os.listdir(path_to_cats) # Get list of all files inside cat folder\n",
    "        \n",
    "        path_to_dog_files = [os.path.join(path_to_dogs, file) for file in dog_files] # Get full path to each cat file\n",
    "        path_to_cat_files = [os.path.join(path_to_cats, file) for file in cat_files] # Get full path to each dog file\n",
    "        \n",
    "        self.training_files = path_to_dog_files + path_to_cat_files # Concatenate our list of paths to dog and cat files\n",
    "        self.dog_label, self.cat_label = 0, 1 # Store 0/1 Labels for Dogs and Cats\n",
    "        \n",
    "        self.transform = transforms.ToTensor() # Convert a PIL Image or ndarray to tensor and scale the values from [0,255] for 8Bit image to [0,1]\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.training_files) # The number of samples we have is just the number of training files we have\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Given some index from range [0, len(self.trainig_files) - 1] we want to load the sample\n",
    "        \"\"\"\n",
    "        path_to_image = self.training_files[idx]\n",
    "        \n",
    "        if \"Dog\" in path_to_image:\n",
    "            label = self.dog_label\n",
    "        \n",
    "        else:\n",
    "            label = self.cat_label\n",
    "            \n",
    "        image = Image.open(path_to_image) # Open Image with PIL to create a PIL Image\n",
    "        image = self.transform(image) # Convert image\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "catvdog = DogsVsCats(\"../data/PetImages/\")\n",
    "\n",
    "for image, label in catvdog:\n",
    "    print(\"Image Label:\",label)\n",
    "    print(\"Image Shape:\", image.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88577ba1-63dd-4d6c-9914-4b6dfafd0210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
