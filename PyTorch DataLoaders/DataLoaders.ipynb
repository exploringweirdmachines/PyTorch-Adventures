{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7961237e-b8f0-4ff5-884b-e30d3ae03e83",
   "metadata": {},
   "source": [
    "![banner](../src/visuals/banner.png)\n",
    "\n",
    "# PyTorch DataLoaders\n",
    "In our [Intro to PyTorch](https://github.com/priyammaz/HAL-DL-From-Scratch/blob/main/Intro%20to%20PyTorch/Intro%20to%20PyTorch.ipynb) we got a basic view of the general PyTorch mechanics. What we didn't discuss in much detail is how to actually pass data to the model efficiently. Lets quickly pull up what we had done before for the MNIST dataset.\n",
    "\n",
    "\n",
    "The MNIST dataset was offered by default by PyTorch as one of their testing datasets. Our goal for this notebook will be to learn how to build this **Dataset** class from scratch so we can use it on our own custom dataset!!\n",
    "```\n",
    "train = torchvision.datasets.MNIST('../data', train=True, download=True,\n",
    "                      transform=transforms.Compose([ ### CONVERT ARRAY TO TENSOR\n",
    "                          transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "test = torchvision.datasets.MNIST('../data', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "```\n",
    "\n",
    "These datasets above can then be passed to the **Dataloader** so we can grab random minibatches. We will also explore the Dataloader specifically and see what other functionality it has!\n",
    "\n",
    "```\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de9990db-dfaa-4de4-9a9f-acc2eb28e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os # Allows to access files\n",
    "import numpy as np \n",
    "from PIL import Image # Allows us to Load Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39760e1f-92ff-40aa-bce0-052cf8bbec06",
   "metadata": {},
   "source": [
    "## Dogs Vs Cats Dataset\n",
    "\n",
    "The first dataloader we will build will be the Dogs vs Cats dataset. Incase you haven't done this already run the following to download all the dataset to the */data* folder\n",
    "\n",
    "```\n",
    "bash download_data.sh\n",
    "```\n",
    "\n",
    "### Important Ideas to Keep in Mind\n",
    "- **Memory Constraints:** When we build a Dataset class we have to think about our hardware constraints. If we have a smaller CSV file, we can use Pandas to read it in and then index the dataframe to grab samples. The issue with this approach is that, the entire Pandas Dataframe is **read to memory**. This means if you have massive tabular datasets, Pandas is not the way to go. You would have to use a system that only loads samples of your tabular data to memory only when accessed. We will cover one such system that has been incredible called [Deep Lake](https://github.com/activeloopai/deeplake) in a future lesson. \n",
    "\n",
    "In our Cats vs Dogs dataset, this is what our file directory looks like:\n",
    "\n",
    "```\n",
    ".\n",
    "└── data/\n",
    "    └── Petimages/\n",
    "        ├── Dogs/\n",
    "        │   ├── xxx.jpg\n",
    "        │   ├── yyy.jpg\n",
    "        │   └── ...\n",
    "        └── Cats/\n",
    "            ├── xxx.jpg\n",
    "            ├── yyy.jpg\n",
    "            └── ...\n",
    "```\n",
    "\n",
    "In our folder Petimages, we have two more folders called Dogs and Cats, and each folder contains images of Dogs and Cats respectively. So we have two options:\n",
    "- Load all our images into Numpy Arrays (and tensors later) up front along with their class label 0 or 1 **### PLEASE DONT DO THIS!!!**\n",
    "- Store only a list of strings that indicate the path to each Image and then load the image only when accessed\n",
    "\n",
    "\n",
    "### Components of a Dataset\n",
    "\n",
    "The dataset class will have three components:\n",
    "- **init**: Initialize the model class with everything you want to store as class variables\n",
    "- **len**: We have to tell the Dataset how many samples there are. When we go to grab a sample, it can grab all the samples from index 0 to index len\n",
    "- **getitem**: This is the block that does most of the heavy lifting. Basically, given some index between 0 and the length of data defined before, it will grab some sample.\n",
    "```\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "```\n",
    "\n",
    "### Lets be a bit more specific about whats going on!\n",
    "\n",
    "```\n",
    ".\n",
    "└── data/\n",
    "    └── Petimages/\n",
    "        ├── Dogs/\n",
    "        │   ├── 111.jpg\n",
    "        │   ├── 222.jpg\n",
    "        └── Cats/\n",
    "            ├── 333.jpg\n",
    "            ├── 444.jpg\n",
    "```\n",
    "\n",
    "Lets say in our directories we have only 4 images in total (2 Cat and 2 Dog Images). Our **len** would then be 4 as we only have 4 images in total! The **getitem** will then use the indexes [0,1,2,3] to access these samples, so our goal in the getitem is to give the Dataset a way to load the image based on the sample number. \n",
    "\n",
    "The easiest way to do this is to store a list of filepaths to all of the images, and then when we access the filepath, we can load that image in! So we would first have to build this list of paths and store it to some class variable we can access later! \n",
    "\n",
    "**Note**: This function should not be in the **getitem**. You want it to happen when the Dataset is being Initialized, otherwise we will build the list every time we grab a sample. The code in **getitem** runs **EVERY TIME WE GRAB A SAMPLE**, but **init** will only run once.\n",
    "\n",
    "```\n",
    "path_to_dogs = [data/Petimages/Dogs/111.jpg, data/Petimages/Dogs/222.jpg] # Make a list of the path to all the Dog files\n",
    "path_to_cats = [data/Petimages/Cats/333.jpg, data/Petimages/Dogs/444.jpg] # Make a list of the path to all the Cat files\n",
    "\n",
    "training_files = path_to_dogs + path_to_cats = [data/Petimages/Dogs/111.jpg, data/Petimages/Dogs/222.jpg, \n",
    "                                                data/Petimages/Cats/333.jpg, data/Petimages/Dogs/444.jpg]\n",
    "                                                \n",
    "```\n",
    "\n",
    "Now that we have a list of files, we have to set the **len**, which in this case would just be the length of training_files, or 4.\n",
    "\n",
    "Lastly, when we start doing a forloop through our Dataset, the **getitem** function is run with an input of **idx**. More specifically, as we loop from 0 to 3 (our total samples of 4 as indicated from before), the idx returned for us to access in **getitem** can be utilized. For example, the first loop will return the index 0, and we can then take that and index our training files.\n",
    "\n",
    "- In the first iteration at the index 0 we have the filepath *data/Petimages/Dogs/111.jpg*.\n",
    "- In the second iteration at the index 1 we have the filepath *data/Petimages/Dogs/222.jpg*.\n",
    "- In the third iteration at the index 2 we have the filepath *data/Petimages/Cats/333.jpg*.\n",
    "- In the fourth iteration at the index 3 we have the filepath *data/Petimages/Cats/444.jpg*.\n",
    "\n",
    "Once we do the fourth iteration, we have done the entire length as indicated in **len** and the loop will end. Therefore in **getitem**, if we can index the filepath to each sample in this way, we can then load the image from the file and return the image as an array of numbers. Also, we can easily find the label of the image (Is it a Cat or Dog?) because the filepath includes \"Dogs\" and \"Cats\" in the name. Therefore we can also return from **getitem** some interger values like 0, if Dog is in the path, or 1 if Cat is in the path.\n",
    "\n",
    "#### Arrays vs Tensors\n",
    "When we load an image we will use the PIL Image module that we imported above. All this module does is take a filepath and loads the image in the PIL format. We can then convert this to a numpy array, but numpy arrays dont work with PyTorch, so we need to convert to a tensor. From PyTorch Torchvision module, we have imported transform which has a ton of cool image transformations we can do (and will look at a bit later). The one we need right now is the **ToTensor()** function that can accept a PIL image (or Numpy Array) and convert to a Tensor that PyTorch models can work with!\n",
    "\n",
    "\n",
    "#### 8Bit Images \n",
    "Most images are stored in what is known as an 8bit format. Essentially each pixel in the image can take integer values in the range of [0,255]. Now the problem with this is, Deep Learning tends to prefer numbers scaled between [0,1], so we just need to scale our 8bit images down. One way is to just divide everything by 255, but the **ToTensor()** function will already handle this for us in the PIL -> Tensor transformation.\n",
    "\n",
    "### Lets Put all the ideas Together!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f67b0800-642a-4b8a-99a6-72823a26251f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Samples 25002\n",
      "Image Label: 0\n",
      "Image Shape: torch.Size([3, 375, 500])\n"
     ]
    }
   ],
   "source": [
    "class DogsVsCats(Dataset):\n",
    "    def __init__(self, path_to_folder):\n",
    "        path_to_cats = os.path.join(path_to_folder, \"Cat\") # Get Path to Cat folder\n",
    "        path_to_dogs = os.path.join(path_to_folder, \"Dog\") # Get Path to Dog folder \n",
    "        \n",
    "        dog_files = os.listdir(path_to_dogs) # Get list of all files inside of dog folder\n",
    "        cat_files = os.listdir(path_to_cats) # Get list of all files inside cat folder\n",
    "        \n",
    "        path_to_dog_files = [os.path.join(path_to_dogs, file) for file in dog_files] # Get full path to each cat file\n",
    "        path_to_cat_files = [os.path.join(path_to_cats, file) for file in cat_files] # Get full path to each dog file\n",
    "        \n",
    "        self.training_files = path_to_dog_files + path_to_cat_files # Concatenate our list of paths to dog and cat files\n",
    "        self.dog_label, self.cat_label = 0, 1 # Store 0/1 Labels for Dogs and Cats\n",
    "        \n",
    "        self.transform = transforms.ToTensor() # Convert a PIL Image or ndarray to tensor and scale the values from [0,255] for 8Bit image to [0,1]\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.training_files) # The number of samples we have is just the number of training files we have\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Given some index from range [0, len(self.trainig_files) - 1] we want to load the sample\n",
    "        \"\"\"\n",
    "        path_to_image = self.training_files[idx] # Grab file path at the sampled index\n",
    "        \n",
    "        if \"Dog\" in path_to_image: # If the word \"Dog\" is in the filepath, then set the label to 0\n",
    "            label = self.dog_label\n",
    "        \n",
    "        else:\n",
    "            label = self.cat_label # Otherwise set the label to 1\n",
    "            \n",
    "        image = Image.open(path_to_image) # Open Image with PIL to create a PIL Image\n",
    "        image = self.transform(image) # Convert image to Tensor\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "dogvcat = DogsVsCats(\"../data/PetImages/\")\n",
    "\n",
    "print(\"Total Training Samples\", len(dogvcat))\n",
    "\n",
    "for image, label in dogvcat:\n",
    "    print(\"Image Label:\",label)\n",
    "    print(\"Image Shape:\", image.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f5827-d8bb-4b47-ad76-e7d1d141b00d",
   "metadata": {},
   "source": [
    "### We have a Dataset class built and now lets try loading it to the DataLoader!\n",
    "\n",
    "To reiterate, we have built a dataloader that can access a single image, but in Deep Learning, we want to sample minibatches, so we need to grab a **BATCH_SIZE** amount of images. We can do that pretty easily using the DataLoader!\n",
    "\n",
    "The Dataloader has some more functions we will look at later but the most basic things we need to include are:\n",
    "\n",
    "```\n",
    "DataLoader(dataset=DATASET, # We place here the dataset we have defined previously\n",
    "           batch_size=16,   # How many samples do you want to put together in each batch?\n",
    "           shuffle=True)    # Do you want to shuffle the data?\n",
    "```\n",
    "\n",
    "Lets then go ahead and instantiate the dataloder and try to do a for loop. Again we are expecting 16 images at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0ac84fe-60fc-4508-8ffe-b3feed1dc4ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 375, 500] at entry 0 and [3, 100, 133] at entry 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m dogsvcatsloader \u001b[38;5;241m=\u001b[39m DataLoader(dogvcat, \n\u001b[1;32m      2\u001b[0m                              batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, \n\u001b[1;32m      3\u001b[0m                              shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m dogsvcatsloader:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(labels)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:141\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    139\u001b[0m         storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    140\u001b[0m         out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr_\u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring_\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndarray\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 375, 500] at entry 0 and [3, 100, 133] at entry 4"
     ]
    }
   ],
   "source": [
    "dogsvcatsloader = DataLoader(dogvcat, \n",
    "                             batch_size=16, \n",
    "                             shuffle=True)\n",
    "\n",
    "for images, labels in dogsvcatsloader:\n",
    "    print(images.shape)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc08e40-b6ea-40d9-b855-c5f3efcb0bff",
   "metadata": {},
   "source": [
    "## Oh No! What Happened?\n",
    "\n",
    "Lets take a look at a key assumption of the dataloader before we keep going. Take a closer look at the error:\n",
    "\n",
    "```\n",
    "RuntimeError: stack expects each tensor to be equal size, but got [3, 371, 500] at entry 0 and [3, 416, 500] at entry 1\n",
    "```\n",
    "\n",
    "What this is trying to say is the first image had dimension [3, 371, 500] and the second image had dimension [3, 416, 500] and because the shapes are different it cannot stack them together. Therefore the only way we can stack these images together is to somehow resize them all into the same shape.\n",
    "\n",
    "\n",
    "## Torchvision Transforms\n",
    "We will be quickly taking a look at the different transformations available from [PyTorch Torchvision Transforms](https://pytorch.org/vision/stable/transforms.html). Now there are a ton of these and we can't talk about all of them but I want to cover some common ones we use. \n",
    "\n",
    "```\n",
    "ToTensor(): We already saw this one, but it converts a PIL Image or Numpy Array to a PyTorch Tensor.\n",
    "Resize(): This will resize an image to the wanted size and is what we need to handle the images in our dataset being different sizes\n",
    "Normalize(): Allows us to feed in the Means and Standard Deviations for each channel (RGB) and normalize our images.\n",
    "RandomHorizontalFlip(): Randomly flips an image horizontally with some probability (default 0.5)\n",
    "RandomVerticalFlip(): Randomly flips an image Vertically with some probability (default 0.5)\n",
    "\n",
    "Compose(): Allows us to stick together multiple image transformations in a single sequence!\n",
    "```\n",
    "\n",
    "**Note**: Why do random flipping? Well an image of a dog flipped around is still a picture of a dog? By including this, it will help further generalize the model and how it interprets different classes. We will also talk about some issues such as overfitting later and this is a great technique to help curtail that!\n",
    "\n",
    "\n",
    "## Lets put together a Composition of Transfomations and Add it to our Dataset Class!\n",
    "\n",
    "Compose expects a list of transformations we want to do in the sequence we want it in! Once it is done, lets try to For loop through it to see if we get any errors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b5e276e-93a1-451e-992b-87aa807cab35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224])\n",
      "tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "### Create a Composition of Transformations\n",
    "img_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224,224)), # Resize the image from whatever it is to a [3, 224, 224] image\n",
    "        transforms.RandomHorizontalFlip(p=0.5), # Do a random flip with 50% probability\n",
    "        transforms.ToTensor(), # Convert the image to a Tensor\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]) # Normalize the Image\n",
    "    ]\n",
    ")\n",
    "\n",
    "### Lets Add this to our Dataset Class\n",
    "class DogsVsCats(Dataset):\n",
    "    def __init__(self, path_to_folder, transforms): ### Our INIT Now Accepts a Transforms\n",
    "        \n",
    "        ### PREVIOUS CODE ###\n",
    "        path_to_cats = os.path.join(path_to_folder, \"Cat\")\n",
    "        path_to_dogs = os.path.join(path_to_folder, \"Dog\")\n",
    "        dog_files = os.listdir(path_to_dogs)\n",
    "        cat_files = os.listdir(path_to_cats) \n",
    "        path_to_dog_files = [os.path.join(path_to_dogs, file) for file in dog_files] \n",
    "        path_to_cat_files = [os.path.join(path_to_cats, file) for file in cat_files] \n",
    "        self.training_files = path_to_dog_files + path_to_cat_files \n",
    "        self.dog_label, self.cat_label = 0, 1 \n",
    "        \n",
    "        \n",
    "        ### NEW CODE ###\n",
    "        # self.transform = transforms.ToTensor() -> Notice how our transforms was just ToTensor before. It will be our Composition of Transforms now!\n",
    "        self.transform = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.training_files) # The number of samples we have is just the number of training files we have\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ### PREVIOUS CODE ###\n",
    "        path_to_image = self.training_files[idx] # Grab file path at the sampled index      \n",
    "        if \"Dog\" in path_to_image: # If the word \"Dog\" is in the filepath, then set the label to 0\n",
    "            label = self.dog_label\n",
    "        else:\n",
    "            label = self.cat_label # Otherwise set the label to 1\n",
    "        image = Image.open(path_to_image) # Open Image with PIL to create a PIL Image\n",
    "        \n",
    "        ### UPDATED CODE ###\n",
    "        image = self.transform(image) # Image now will go through series of transforms indicated in self.transform\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "### Instantiate Dataset With the Transforms ###\n",
    "dogvcat = DogsVsCats(path_to_folder=\"../data/PetImages/\",\n",
    "                     transforms=img_transforms)\n",
    "\n",
    "\n",
    "dogsvcatsloader = DataLoader(dogvcat, \n",
    "                             batch_size=16, \n",
    "                             shuffle=True)\n",
    "\n",
    "for images, labels in dogsvcatsloader:\n",
    "    print(images.shape)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf337b1-f088-4e14-9e9b-e1102e0a760d",
   "metadata": {},
   "source": [
    "## Voila! It Finally Works!!!\n",
    "We are now grabbing 16 samples of images, each image has 3 channels, and each channel is of size 224 x 224. We also are getting the 16 binary class labels of 0 or 1 depending on if its a Dog or Cat. This is the common Tensor format that most Image related Deep Learning tasks happen in:\n",
    "\n",
    "```\n",
    "[Batch Size x Num Channels x Image Height x Image Width]\n",
    "```\n",
    "\n",
    "We are mainly focusing on this notebook in how to use the DataLoader and not really train anything, but we will use this DataLoader in a future lesson!\n",
    "\n",
    "## The Next Problem: Deep Learning Consists of Highly Expressive Models\n",
    "Deep Learning models are extremly powerful because of their expressive nature and ability to model very complex, high dimensional relationships. This means that as we train on a model, how it is performing on the training data is deceptive. What I mean by this is, after training a model, you may get an astonishing 100% Accuracy! You think this model is incredible, but the second you start using it for real world prediction problems with examples your model has never seen, you see that it doesn't perform accurately at all.\n",
    "\n",
    "We breifly talked about this before in the Intro to PyTorch, the problem of **Overfitting** on your data. To have a fair comparison, we have to train our data on some of the labeled data we have, but keep a small part of it for validation. The model will never optimize on the Validation but just inference to see how well it performs on data it has never seen. We will get into that in a future lesson, but for now, how do we setup our DataLoader to be able to do this?\n",
    "\n",
    "\n",
    "### Random Split\n",
    "Luckily for us, PyTorch has a function in **torch.utils.data** called **random_split**. In this function we will essentially let PyTorch know how many samples we want in our training set and testing set, and it will randomly split the dataset for us! We saw previously that our datset has about 25000 samples, so what we want to do is give 90 percent of our samples for training and the remaining 10 percent for validation. We will then load each of these datasets into our dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c3ab5c1-c04a-4573-a071-60a573017162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Samples: 22437 Number of Test Samples 2494\n",
      "torch.Size([16, 3, 224, 224])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1])\n",
      "torch.Size([16, 3, 224, 224])\n",
      "tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "train_samples = int(0.9 * len(dogvcat))\n",
    "test_samples = len(dogvcat) - train_samples\n",
    "\n",
    "print(\"Number of Training Samples:\", train_samples, \"Number of Test Samples\", test_samples)\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dogvcat, lengths=[train_samples, test_samples]) # Split with the dataset by the lenghts\n",
    "\n",
    "### Load Datasets into two DataLoaders ###\n",
    "trainloader = DataLoader(train_dataset, \n",
    "                         batch_size=16, \n",
    "                         shuffle=True)\n",
    "\n",
    "testloader = DataLoader(test_dataset, \n",
    "                        batch_size=16, \n",
    "                        shuffle=True)\n",
    "\n",
    "\n",
    "### Test Loaders ###\n",
    "for images, labels in trainloader:\n",
    "    print(images.shape)\n",
    "    print(labels)\n",
    "    break\n",
    "    \n",
    "for images, labels in testloader:\n",
    "    print(images.shape)\n",
    "    print(labels)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
